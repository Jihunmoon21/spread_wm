defaults:
  - _self_
  - planner: mpc_cem
  - override hydra/launcher: submitit_slurm

hydra:
  run:
    dir: plan_outputs/${now:%Y%m%d%H%M%S}_${replace_slash:${model_name}}_gH${goal_H}
  sweep:
    dir: plan_outputs/${now:%Y%m%d%H%M%S}_${replace_slash:${model_name}}_gH${goal_H}
    subdir: ${hydra.job.num}
  launcher:
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    nodes: 1
    tasks_per_node: 1
    cpus_per_task: 16
    mem_gb: 256
    gres: "gpu:h100:1"
    qos: "explore"
    timeout_min: 720
    setup: ["export DEBUGVAR=$(scontrol show hostnames $SLURM_JOB_NODELIST)",
            export MASTER_ADDR="$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)",
            "export MASTER_PORT=$(for port in $(shuf -i 30000-65500 -n 20); do if [[ $(netstat -tupln 2>&1 | grep $port | wc -l) -eq 0 ]] ; then echo $port; break; fi; done;)",]

# model to load for planning
ckpt_base_path: /home/jihoonmun/spread # put absolute path here. Checkpoints will be loaded from ${ckpt_base_path}/outputs
model_name: null
model_epoch: latest # 임의 조정

seed: 99
n_evals: 10 # 25
goal_source: 'random_state' # 'random_state' or 'dset' or 'random_action' 
goal_H: 5 # specifies how far away the goal is if goal_source is 'dset'
n_plot_samples: 10

debug_dset_init: False

objective:
  _target_: planning.objectives.create_objective_fn
  alpha: 1
  base: 2 # coeff base for weighting all frames. Only applies when mode == 'all'
  mode: last


# 앙상블 기반 LoRA 설정 (lora.*로 통합)
lora:
  enabled: true
  online: true
  ensemble: true                  # ← 기존 ensemble_lora.enabled 대체
  lr: 0.0005                    # LoRA learning rate (기본값: 1e-4)
  visual_loss_weight: 1.0
  proprio_loss_weight: 0.3
  
  # 태스크 전환: loss 기반 전환 설정
  task_switch:
    enabled: true
    loss_threshold: 0.10          # 최근 윈도우 평균 손실이 이 값 이하로 수렴하면 전환
    max_planning_per_task: 300     # 태스크당 최대 플래닝 반복 횟수
    min_planning_per_task: 5      # 최소 플래닝 반복 후부터 전환 조건 평가
    loss_window_size: 3           # 전환 판단에 사용할 최근 loss 윈도우 크기
  
  # 하이브리드 적층 설정(기존 유지)
  hybrid_stacking:
    enabled: true
    task_based_stacking: false
    loss_based_stacking: false
    max_stacks_per_task: 3
    stack_type_tracking: true
    force_initial_stacking: true
  
  # 앙상블 상세 설정(구 ensemble_lora → lora.ensemble_cfg)
  ensemble_cfg:
    max_ensemble_size: 11
    evaluation_steps: 1
    cache_dir: "./lora_cache"
    max_memory_mb: 200
    inference:
      method: "weighted_average"
      detailed_evaluation: true
      evaluation_frequency: 1
      enable_planning_integration: false
      usage_strategy: "task_change_only"
      task_change_evaluation: true
      select_best_member: true
      stack_on_selected: true
      task_specific_evaluation: true
      evaluation_loss_threshold: 0.15
      task_change_evaluation_steps: 1
    consolidation_method: "best_only"
    consolidate_after_tasks: 5