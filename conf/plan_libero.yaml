defaults:
  - _self_
  - planner: mpc_cem
  - override hydra/launcher: submitit_slurm

hydra:
  run:
    dir: plan_outputs/${now:%Y%m%d%H%M%S}_${replace_slash:${model_name}}_gH${goal_H}
  sweep:
    dir: plan_outputs/${now:%Y%m%d%H%M%S}_${replace_slash:${model_name}}_gH${goal_H}
    subdir: ${hydra.job.num}
  launcher:
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    nodes: 1
    tasks_per_node: 1
    cpus_per_task: 16
    mem_gb: 256
    gres: "gpu:h100:1"
    qos: "explore"
    timeout_min: 720
    setup: ["export DEBUGVAR=$(scontrol show hostnames $SLURM_JOB_NODELIST)",
            export MASTER_ADDR="$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)",
            "export MASTER_PORT=$(for port in $(shuf -i 30000-65500 -n 20); do if [[ $(netstat -tupln 2>&1 | grep $port | wc -l) -eq 0 ]] ; then echo $port; break; fi; done;)",]

# model to load for planning
ckpt_base_path: /home/jihoonmun/spread # put absolute path here. Checkpoints will be loaded from ${ckpt_base_path}/outputs
model_name: libero_dummy
model_epoch: latest

seed: 99
n_evals: 10 # 25
goal_source: 'random_state' # 'random_state' or 'dset' or 'random_action'
goal_H: 5 # specifies how far away the goal is if goal_source is 'dset'
n_plot_samples: 10

debug_dset_init: False

objective:
  _target_: planning.objectives.create_objective_fn
  alpha: 1
  base: 2 # coeff base for weighting all frames. Only applies when mode == 'all'
  mode: last


# 앙상블 기반 LoRA 설정 (lora.*로 통합)
lora:
  enabled: true
  online: true
  ensemble: true                  # ← 기존 ensemble_lora.enabled 대체
  lr: 0.0005                    # LoRA learning rate (기본값: 1e-4)
  visual_loss_weight: 1.0
  proprio_loss_weight: 0.3

  # 태스크 전환: loss 기반 전환 설정
  task_switch:
    enabled: true
    loss_threshold: 0.10          # 최근 윈도우 평균 손실이 이 값 이하로 수렴하면 전환
    max_planning_per_task: 300     # 태스크당 최대 플래닝 반복 횟수
    min_planning_per_task: 5      # 최소 플래닝 반복 후부터 전환 조건 평가
    loss_window_size: 3           # 전환 판단에 사용할 최근 loss 윈도우 크기

  # 하이브리드 적층 설정(기존 유지)
  hybrid_stacking:
    enabled: true
    task_based_stacking: false
    loss_based_stacking: false
    max_stacks_per_task: 3
    stack_type_tracking: true
    force_initial_stacking: true

  # 앙상블 상세 설정(구 ensemble_lora → lora.ensemble_cfg)
  ensemble_cfg:
    max_ensemble_size: 11
    evaluation_steps: 1
    cache_dir: "./lora_cache"
    max_memory_mb: 200
    inference:
      method: "weighted_average"
      detailed_evaluation: true
      evaluation_frequency: 1
      enable_planning_integration: false
      usage_strategy: "task_change_only"
      task_change_evaluation: true
      select_best_member: true
      stack_on_selected: true
      task_specific_evaluation: true
      evaluation_loss_threshold: 0.15
      task_change_evaluation_steps: 1
    consolidation_method: "best_only"
    consolidate_after_tasks: 5


# --------- Libero Env Configuration ---------
env:
  # libero_wrapper.py 파일 경로('env/libero/libero_wrapper.py')를 반영
  _target_: env.libero.libero_wrapper.LiberoWrapper

  # [필수] LBP 프로젝트의 'assets/libero.pkl' 파일 절대 경로
  libero_pkl_path: /home/jihoonmun/spread/assets/libero.pkl

  # [필수] 실행할 태스크 스위트 (libero.pkl 내부 키와 일치해야 함)
  task_suite_name: libero_object  # 예: "libero_spatial" 또는 "libero_10"
  
  # [필수] 실행할 태스크의 전체 이름
  task_name: pick_up_the_bbq_sauce_and_place_it_in_the_basket         # 예: "libero_spatial.move_the_block_to_the_corner_of_the_table"

  # LBP/OpenVLA 표준을 따르기 위해 256으로 설정
  img_size: 256
  
  # libero_wrapper.py의 기본값
  camera_name: "agentview"

  # libero_wrapper.py의 DEFAULT_CONTROLLER_CONFIG 값
  controller_config:
    type: "OSC_POSE"
    input_max: 1
    input_min: -1
    output_max: [0.05, 0.05, 0.05, 0.5, 0.5, 0.5]
    output_min: [-0.05, -0.05, -0.05, -0.5, -0.5, -0.5]
    kp: 150
    damping: 1
    impedance_mode: "fixed"
    kp_limits: [0, 300]
    damping_limits: [0, 10]
    position_limits: null
    orientation_limits: null
    uncouple_pos_ori: true
    control_delta: true
    interpolation: null
    ramp_ratio: 0.2