defaults:
  - _self_
  - planner: mpc_cem
  - override hydra/launcher: submitit_slurm

hydra:
  run:
    dir: plan_outputs/${now:%Y%m%d%H%M%S}_${replace_slash:${model_name}}_gH${goal_H}
  sweep:
    dir: plan_outputs/${now:%Y%m%d%H%M%S}_${replace_slash:${model_name}}_gH${goal_H}
    subdir: ${hydra.job.num}
  launcher:
    submitit_folder: ${hydra.sweep.dir}/.submitit/%j
    nodes: 1
    tasks_per_node: 1
    cpus_per_task: 16
    mem_gb: 256
    gres: "gpu:h100:1"
    qos: "explore"
    timeout_min: 720
    setup: ["export DEBUGVAR=$(scontrol show hostnames $SLURM_JOB_NODELIST)",
            export MASTER_ADDR="$(scontrol show hostnames $SLURM_JOB_NODELIST | head -n 1)",
            "export MASTER_PORT=$(for port in $(shuf -i 30000-65500 -n 20); do if [[ $(netstat -tupln 2>&1 | grep $port | wc -l) -eq 0 ]] ; then echo $port; break; fi; done;)",]

# model to load for planning
ckpt_base_path: /home/jihoonmun/spread # put absolute path here. Checkpoints will be loaded from ${ckpt_base_path}/outputs
# [CHANGEME] ★★★ 학습된 LIBERO 월드 모델 이름으로 변경하세요 ★★★
model_name: CHANGEME_LIBERO_MODEL_NAME 
model_epoch: latest

seed: 99
n_evals: 10 # 25

# --- 수정된 부분: Goal Source 설정 ---
goal_source: 'trajectory' # 'random_state', 'dset', 'random_action' 대신 'trajectory' 사용
goal_H: 5 # 'dset'에서 사용되던 파라미터. 'trajectory' 모드에서는 사용되지 않을 수 있음

# --- 추가된 부분: Trajectory Goal 설정 ---
goal_traj_index: 0    # 플래닝에 사용할 LIBERO 데모 궤적의 인덱스
goal_frame_indices: [-1] # 목표로 사용할 프레임 인덱스 ([-1] = 마지막 프레임만 사용)
# ------------------------------------

n_plot_samples: 10
debug_dset_init: False

# --- 수정된 부분: Objective 함수 설정 ---
objective:
  # 새로 정의한 궤적 기반 목표 함수 (create_trajectory_objective_fn)
  _target_: planning.objectives.create_trajectory_objective_fn
  
  # 시각적 목표(visual)에 대한 가중치 (lora.visual_loss_weight와 맞춤)
  alpha: 1.0
  
  # 관절 상태(proprio) 목표에 대한 가중치 (lora.proprio_loss_weight와 맞춤)
  proprio_weight: 0.3
  
  # goal_frame_indices에서 정의한 값을 사용
  goal_indices: ${goal_frame_indices}
  
  # "last_predicted": 예측된 궤적의 *마지막* 상태와 목표 프레임(들)을 비교
  comparison_mode: "last_predicted" 
# -----------------------------------


# 앙상블 기반 LoRA 설정 (lora.*로 통합)
lora:
  enabled: true
  online: true
  ensemble: true                  # ← 기존 ensemble_lora.enabled 대체
  lr: 0.0005                    # LoRA learning rate (기본값: 1e-4)
  visual_loss_weight: 1.0
  proprio_loss_weight: 0.3

  # 태스크 전환: loss 기반 전환 설정
  task_switch:
    enabled: true
    loss_threshold: 0.10          # 최근 윈도우 평균 손실이 이 값 이하로 수렴하면 전환
    max_planning_per_task: 300     # 태스크당 최대 플래닝 반복 횟수
    min_planning_per_task: 5      # 최소 플래닝 반복 후부터 전환 조건 평가
    loss_window_size: 3           # 전환 판단에 사용할 최근 loss 윈도우 크기

  # 하이브리드 적층 설정(기존 유지)
  hybrid_stacking:
    enabled: true
    task_based_stacking: false
    loss_based_stacking: false
    max_stacks_per_task: 3
    stack_type_tracking: true
    force_initial_stacking: true

  # 앙상블 상세 설정(구 ensemble_lora → lora.ensemble_cfg)
  ensemble_cfg:
    max_ensemble_size: 11
    evaluation_steps: 1
    cache_dir: "./lora_cache"
    max_memory_mb: 200
    inference:
      method: "weighted_average"
      detailed_evaluation: true
      evaluation_frequency: 1
      enable_planning_integration: false
      usage_strategy: "task_change_only"
      task_change_evaluation: true
      select_best_member: true
      stack_on_selected: true
      task_specific_evaluation: true
      evaluation_loss_threshold: 0.15
      task_change_evaluation_steps: 1
    consolidation_method: "best_only"
    consolidate_after_tasks: 5


# --------- Libero Env Configuration ---------
env:
  # libero_wrapper.py 파일 경로('env/libero/libero_wrapper.py')를 반영
  _target_: env.libero.libero_wrapper.LiberoWrapper

  # [필수] LBP 프로젝트의 'assets/libero.pkl' 파일 절대 경로
  libero_pkl_path: /home/jihoonmun/spread/assets/libero.pkl

  # [필수] 실행할 태스크 스위트 (libero.pkl 내부 키와 일치해야 함)
  task_suite_name: libero_object  # 예: "libero_spatial" 또는 "libero_10"
  
  # [필수] 실행할 태스크의 전체 이름
  # ★★★ 중요: 이 task_name은 위에서 설정한 goal_traj_index (예: 0)에 
  # 해당하는 태스크와 일치해야 합니다. (plan.py에서 동적으로 로드하지 않는 경우)
  task_name: pick_up_the_bbq_sauce_and_place_it_in_the_basket

  # LBP/OpenVLA 표준을 따르기 위해 256으로 설정
  img_size: 256
  
  # libero_wrapper.py의 기본값
  camera_name: "agentview"

  # libero_wrapper.py의 DEFAULT_CONTROLLER_CONFIG 값
  controller_config:
    type: "OSC_POSE"
    input_max: 1
    input_min: -1
    output_max: [0.05, 0.05, 0.05, 0.5, 0.5, 0.5]
    output_min: [-0.05, -0.05, -0.05, -0.5, -0.5, -0.5]
    kp: 150
    damping: 1
    impedance_mode: "fixed"
    kp_limits: [0, 300]
    damping_limits: [0, 10]
    position_limits: null
    orientation_limits: null
    uncouple_pos_ori: true
    control_delta: true
    interpolation: null
    ramp_ratio: 0.2