import os
import gym
import json
import hydra
import random
import torch
import pickle
import wandb
import logging
import warnings
import time
import numpy as np
import submitit
from itertools import product
from pathlib import Path
from einops import rearrange
from omegaconf import OmegaConf, open_dict

from env.venv import SubprocVectorEnv
from custom_resolvers import replace_slash
from preprocessor import Preprocessor
from planning.evaluator import PlanEvaluator
from utils import cfg_to_dict, seed
from collections import deque

from models.vit import ViTPredictor
from models.lora import LoRA_ViT_spread
from planning.online import OnlineLora

warnings.filterwarnings("ignore")
log = logging.getLogger(__name__)

ALL_MODEL_KEYS = [
    "encoder",
    "predictor",
    "decoder",
    "proprio_encoder",
    "action_encoder",
]

def planning_main_in_dir(working_dir, cfg_dict):
    os.chdir(working_dir)
    return planning_main(cfg_dict=cfg_dict)

def launch_plan_jobs(
    epoch,
    cfg_dicts,
    plan_output_dir,
):
    with submitit.helpers.clean_env():
        jobs = []
        for cfg_dict in cfg_dicts:
            subdir_name = f"{cfg_dict['planner']['name']}_goal_source={cfg_dict['goal_source']}_goal_H={cfg_dict['goal_H']}_alpha={cfg_dict['objective']['alpha']}"
            subdir_path = os.path.join(plan_output_dir, subdir_name)
            executor = submitit.AutoExecutor(
                folder=subdir_path, slurm_max_num_timeout=20
            )
            executor.update_parameters(
                **{
                    k: v
                    for k, v in cfg_dict["hydra"]["launcher"].items()
                    if k != "submitit_folder"
                }
            )
            cfg_dict["saved_folder"] = subdir_path
            cfg_dict["wandb_logging"] = False  # don't init wandb
            job = executor.submit(planning_main_in_dir, subdir_path, cfg_dict)
            jobs.append((epoch, subdir_name, job))
            print(
                f"Submitted evaluation job for checkpoint: {subdir_path}, job id: {job.job_id}"
            )
        return jobs


def build_plan_cfg_dicts(
    plan_cfg_path="",
    ckpt_base_path="",
    model_name="",
    model_epoch="final",
    planner=["gd", "cem"],
    goal_source=["dset"],
    goal_H=[1, 5, 10],
    alpha=[0, 0.1, 1],
):
    """
    Return a list of plan overrides, for model_path, add a key in the dict {"model_path": model_path}.
    """
    config_path = os.path.dirname(plan_cfg_path)
    overrides = [
        {
            "planner": p,
            "goal_source": g_source,
            "goal_H": g_H,
            "ckpt_base_path": ckpt_base_path,
            "model_name": model_name,
            "model_epoch": model_epoch,
            "objective": {"alpha": a},
        }
        for p, g_source, g_H, a in product(planner, goal_source, goal_H, alpha)
    ]
    cfg = OmegaConf.load(plan_cfg_path)
    cfg_dicts = []
    for override_args in overrides:
        planner = override_args["planner"]
        planner_cfg = OmegaConf.load(
            os.path.join(config_path, f"planner/{planner}.yaml")
        )
        cfg["planner"] = OmegaConf.merge(cfg.get("planner", {}), planner_cfg)
        override_args.pop("planner")
        cfg = OmegaConf.merge(cfg, OmegaConf.create(override_args))
        cfg_dict = OmegaConf.to_container(cfg)
        cfg_dict["planner"]["horizon"] = cfg_dict["goal_H"]  # assume planning horizon equals to goal horizon
        cfg_dicts.append(cfg_dict)
    return cfg_dicts

# PlanWorkspace í´ë˜ìŠ¤ì˜ __init__ ë©”ì„œë“œ 
class PlanWorkspace:
    def __init__(
        self,
        cfg_dict: dict,
        wm: torch.nn.Module,
        dset,
        env: SubprocVectorEnv,
        env_name: str,
        frameskip: int,
        wandb_run: wandb.run,
    ):
        # --- 1. ê¸°ë³¸ ì†ì„± ì´ˆê¸°í™” ---
        self.cfg_dict = cfg_dict
        self.wm = wm
        self.dset = dset
        self.env = env
        self.env_name = env_name
        self.frameskip = frameskip
        self.wandb_run = wandb_run
        self.device = next(wm.parameters()).device
        
        self.eval_seed = [cfg_dict["seed"] * n + 1 for n in range(cfg_dict["n_evals"])]
        print("eval_seed: ", self.eval_seed)
        self.n_evals = cfg_dict["n_evals"]
        self.goal_source = cfg_dict["goal_source"]
        self.goal_H = cfg_dict["goal_H"]
        self.action_dim = self.dset.action_dim * self.frameskip
        self.debug_dset_init = cfg_dict["debug_dset_init"]

        objective_fn = hydra.utils.call(
            cfg_dict["objective"],
        )

        self.data_preprocessor = Preprocessor(
            action_mean=self.dset.action_mean,
            action_std=self.dset.action_std,
            state_mean=self.dset.state_mean,
            state_std=self.dset.state_std,
            proprio_mean=self.dset.proprio_mean,
            proprio_std=self.dset.proprio_std,
            transform=self.dset.transform,
        )

        if self.cfg_dict["goal_source"] == "file":
            self.prepare_targets_from_file(cfg_dict["goal_file_path"])
        else:
            self.prepare_targets()

        # --- LoRA í•™ìŠµ ê´€ë ¨ ì„¤ì • ë° ê°ì²´ ìƒì„± ---
        self.online_learner = None # OnlineLora ê°ì²´ë¥¼ ë‹´ì„ ë³€ìˆ˜
        self.is_lora_enabled = self.cfg_dict.get("lora", {}).get("enabled", False)
        self.is_online_lora = self.cfg_dict.get("lora", {}).get("online", False)

        if self.is_lora_enabled:
            print("INFO: LoRA training enabled. Initializing OnlineLora module.")
            # OnlineLora ê°ì²´ë¥¼ ìƒì„±í•˜ê³ , workspace ì „ì²´ë¥¼ ë„˜ê²¨ Evaluatorì—ì„œ ì ‘ê·¼í•  ìˆ˜ ìˆë„ë¡
            self.online_learner = OnlineLora(workspace=self)

        self.evaluator = PlanEvaluator(
            obs_0=self.obs_0,
            obs_g=self.obs_g,
            state_0=self.state_0,
            state_g=self.state_g,
            env=self.env,
            wm=self.wm,
            frameskip=self.frameskip,
            seed=self.eval_seed,
            preprocessor=self.data_preprocessor,
            n_plot_samples=self.cfg_dict["n_plot_samples"],
            workspace=self, # workspaceë¥¼ evaluatorì— ì „ë‹¬í•˜ëŠ” ê²ƒì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            is_lora_enabled=self.is_lora_enabled,
            is_online_lora=self.is_online_lora,
        )

        if self.wandb_run is None or isinstance(
            self.wandb_run, wandb.sdk.lib.disabled.RunDisabled
        ):
            self.wandb_run = DummyWandbRun()

        self.log_filename = "logs.json"  # planner and final eval logs are dumped here
        self.planner = hydra.utils.instantiate(
            self.cfg_dict["planner"],
            wm=self.wm,
            env=self.env,  # only for mpc
            action_dim=self.action_dim,
            objective_fn=objective_fn,
            preprocessor=self.data_preprocessor,
            evaluator=self.evaluator,
            wandb_run=self.wandb_run,
            log_filename=self.log_filename,
        )

        # optional: assume planning horizon equals to goal horizon
        from planning.mpc import MPCPlanner
        if isinstance(self.planner, MPCPlanner):
            self.planner.sub_planner.horizon = cfg_dict["goal_H"]
            self.planner.n_taken_actions = cfg_dict["goal_H"]
        else:
            self.planner.horizon = cfg_dict["goal_H"]
            
        self.dump_targets()

    def prepare_targets(self):
        states = []
        actions = []
        observations = []
        
        if self.goal_source == "random_state":
            # update env config from val trajs
            observations, states, actions, env_info = (
                self.sample_traj_segment_from_dset(traj_len=2)
            )
            self.env.update_env(env_info)

            # sample random states
            rand_init_state, rand_goal_state = self.env.sample_random_init_goal_states(
                self.eval_seed
            )
            if self.env_name == "deformable_env": # take rand init state from dset for deformable envs
                rand_init_state = np.array([x[0] for x in states])

            obs_0, state_0 = self.env.prepare(self.eval_seed, rand_init_state)
            obs_g, state_g = self.env.prepare(self.eval_seed, rand_goal_state)

            # add dim for t
            for k in obs_0.keys():
                obs_0[k] = np.expand_dims(obs_0[k], axis=1)
                obs_g[k] = np.expand_dims(obs_g[k], axis=1)

            self.obs_0 = obs_0
            self.obs_g = obs_g
            self.state_0 = rand_init_state  # (b, d)
            self.state_g = rand_goal_state
            self.gt_actions = None
        else:
            # update env config from val trajs
            observations, states, actions, env_info = (
                self.sample_traj_segment_from_dset(traj_len=self.frameskip * self.goal_H + 1)
            )
            self.env.update_env(env_info)

            # get states from val trajs
            init_state = [x[0] for x in states]
            init_state = np.array(init_state)
            actions = torch.stack(actions)
            if self.goal_source == "random_action":
                actions = torch.randn_like(actions)
            wm_actions = rearrange(actions, "b (t f) d -> b t (f d)", f=self.frameskip)
            exec_actions = self.data_preprocessor.denormalize_actions(actions)
            # replay actions in env to get gt obses
            rollout_obses, rollout_states = self.env.rollout(
                self.eval_seed, init_state, exec_actions.numpy()
            )
            self.obs_0 = {
                key: np.expand_dims(arr[:, 0], axis=1)
                for key, arr in rollout_obses.items()
            }
            self.obs_g = {
                key: np.expand_dims(arr[:, -1], axis=1)
                for key, arr in rollout_obses.items()
            }
            self.state_0 = init_state  # (b, d)
            self.state_g = rollout_states[:, -1]  # (b, d)
            self.gt_actions = wm_actions

    def sample_traj_segment_from_dset(self, traj_len):
        states = []
        actions = []
        observations = []
        env_info = []

        # Check if any trajectory is long enough
        valid_traj = [
            self.dset[i][0]["visual"].shape[0]
            for i in range(len(self.dset))
            if self.dset[i][0]["visual"].shape[0] >= traj_len
        ]
        if len(valid_traj) == 0:
            raise ValueError("No trajectory in the dataset is long enough.")

        # sample init_states from dset
        for i in range(self.n_evals):
            max_offset = -1
            while max_offset < 0:  # filter out traj that are not long enough
                traj_id = random.randint(0, len(self.dset) - 1)
                obs, act, state, e_info = self.dset[traj_id]
                max_offset = obs["visual"].shape[0] - traj_len
            state = state.numpy()
            offset = random.randint(0, max_offset)
            obs = {
                key: arr[offset : offset + traj_len]
                for key, arr in obs.items()
            }
            state = state[offset : offset + traj_len]
            act = act[offset : offset + self.frameskip * self.goal_H]
            actions.append(act)
            states.append(state)
            observations.append(obs)
            env_info.append(e_info)
        return observations, states, actions, env_info

    def prepare_targets_from_file(self, file_path):
        with open(file_path, "rb") as f:
            data = pickle.load(f)
        self.obs_0 = data["obs_0"]
        self.obs_g = data["obs_g"]
        self.state_0 = data["state_0"]
        self.state_g = data["state_g"]
        self.gt_actions = data["gt_actions"]
        self.goal_H = data["goal_H"]

    def dump_targets(self):
        with open("plan_targets.pkl", "wb") as f:
            pickle.dump(
                {
                    "obs_0": self.obs_0,
                    "obs_g": self.obs_g,
                    "state_0": self.state_0,
                    "state_g": self.state_g,
                    "gt_actions": self.gt_actions,
                    "goal_H": self.goal_H,
                },
                f,
            )
        file_path = os.path.abspath("plan_targets.pkl")
        print(f"Dumped plan targets to {file_path}")

    def perform_planning(self):
        if self.debug_dset_init:
            actions_init = self.gt_actions
        else:
            actions_init = None
        actions, action_len = self.planner.plan(
            obs_0=self.obs_0,
            obs_g=self.obs_g,
            actions=actions_init,
        )
        logs, successes, _, _ = self.evaluator.eval_actions(
            actions.detach(), action_len, save_video=True, filename="output_final"
        )
        logs = {f"final_eval/{k}": v for k, v in logs.items()}
        self.wandb_run.log(logs)
        logs_entry = {
            key: (
                value.item()
                if isinstance(value, (np.float32, np.int32, np.int64))
                else value
            )
            for key, value in logs.items()
        }
        with open(self.log_filename, "a") as file:
            file.write(json.dumps(logs_entry) + "\n")
        return logs


def load_ckpt(snapshot_path, device):
    with snapshot_path.open("rb") as f:
        payload = torch.load(f, map_location=device, weights_only=False)
    loaded_keys = []
    result = {}
    for k, v in payload.items():
        if k in ALL_MODEL_KEYS:
            loaded_keys.append(k)
            result[k] = v.to(device)
    result["epoch"] = payload["epoch"]
    return result

def load_model(model_ckpt, train_cfg, num_action_repeat, device):
    result = {}
    if model_ckpt.exists():
        result = load_ckpt(model_ckpt, device)
        print(f"Resuming from epoch {result['epoch']}: {model_ckpt}")

    if "encoder" not in result:
        result["encoder"] = hydra.utils.instantiate(
            train_cfg.encoder,
        )
    if "predictor" not in result:
        raise ValueError("Predictor not found in model checkpoint")

    if train_cfg.has_decoder and "decoder" not in result:
        base_path = os.path.dirname(os.path.abspath(__file__))
        if train_cfg.env.decoder_path is not None:
            decoder_path = os.path.join(base_path, train_cfg.env.decoder_path)
            ckpt = torch.load(decoder_path)
            if isinstance(ckpt, dict):
                result["decoder"] = ckpt["decoder"]
            else:
                result["decoder"] = torch.load(decoder_path)
        else:
            raise ValueError(
                "Decoder path not found in model checkpoint \
                                and is not provided in config"
            )
    elif not train_cfg.has_decoder:
        result["decoder"] = None

    model = hydra.utils.instantiate(
        train_cfg.model,
        encoder=result["encoder"],
        proprio_encoder=result["proprio_encoder"],
        action_encoder=result["action_encoder"],
        predictor=result["predictor"],
        decoder=result["decoder"],
        proprio_dim=train_cfg.proprio_emb_dim,
        action_dim=train_cfg.action_emb_dim,
        concat_dim=train_cfg.concat_dim,
        num_action_repeat=num_action_repeat,
        num_proprio_repeat=train_cfg.num_proprio_repeat,
    )
    model.to(device)
    return model


def measure_forgetting_on_past_tasks(model, past_task_envs, plan_workspace, current_task_id):
    """
    ê³¼ê±° íƒœìŠ¤í¬ë“¤ì— ëŒ€í•œ í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì—¬ íŒŒêµ­ì  ë§ê°ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    
    Args:
        model: í˜„ì¬ í•™ìŠµëœ ëª¨ë¸
        past_task_envs: ê³¼ê±° íƒœìŠ¤í¬ í™˜ê²½ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        plan_workspace: í˜„ì¬ í”Œë˜ë‹ ì›Œí¬ìŠ¤í˜ì´ìŠ¤
        current_task_id: í˜„ì¬ íƒœìŠ¤í¬ ID
    
    Returns:
        forgetting_results: ê° ê³¼ê±° íƒœìŠ¤í¬ì— ëŒ€í•œ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
    """
    forgetting_results = []
    
    for past_task in past_task_envs[:-1]:  # ë§ˆì§€ë§‰(í˜„ì¬) íƒœìŠ¤í¬ ì œì™¸
        past_task_id = past_task['task_id']
        past_env = past_task['env']
        past_env_config = past_task['env_config']
        
        print(f"   Testing on Task {past_task_id}: {past_env_config}")
        
        try:
            # ê³¼ê±° íƒœìŠ¤í¬ í™˜ê²½ì—ì„œ í”Œë˜ë‹ ìˆ˜í–‰
            past_plan_workspace = PlanWorkspace(
                cfg_dict=plan_workspace.cfg_dict.copy(),
                wm=model,  # í˜„ì¬ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
                dset=plan_workspace.dset,
                env=past_env,
                env_name=plan_workspace.env_name,
                frameskip=plan_workspace.frameskip,
                wandb_run=None,  # ë§ê° ì¸¡ì • ì‹œì—ëŠ” wandb ë¡œê¹… ë¹„í™œì„±í™”
            )
            
            # ê°„ë‹¨í•œ ì„±ëŠ¥ ì¸¡ì • (1íšŒ í”Œë˜ë‹)
            logs = past_plan_workspace.perform_planning()
            
            # ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ
            success_rate = logs.get('success_rate', 0.0)
            avg_loss = None
            
            # Loss ì¸¡ì • (ì˜¨ë¼ì¸ LoRAê°€ í™œì„±í™”ëœ ê²½ìš°)
            if (past_plan_workspace.is_online_lora and 
                hasattr(past_plan_workspace.online_learner, 'last_loss') and
                past_plan_workspace.online_learner.last_loss is not None):
                avg_loss = past_plan_workspace.online_learner.last_loss
            
            # Loss ê¸°ë°˜ retention rate ê³„ì‚° (0.1ê³¼ì˜ ê·¼ì ‘ë„)
            target_loss = 0.1
            if avg_loss is not None:
                # Lossê°€ 0.1ë³´ë‹¤ ì‘ìœ¼ë©´ ë¬´ì¡°ê±´ 100% retention
                if avg_loss <= target_loss:
                    retention_rate = 100.0
                else:
                    # Lossê°€ 0.1ë³´ë‹¤ í´ ë•Œë§Œ ê±°ë¦¬ ê¸°ë°˜ ê³„ì‚°
                    loss_distance = avg_loss - target_loss
                    # ì§€ìˆ˜ì  ê°ì†Œ: distanceê°€ í´ìˆ˜ë¡ retentionì´ ë¹ ë¥´ê²Œ ê°ì†Œ
                    retention_rate = max(0, 100 * (1.0 - loss_distance / target_loss))
            else:
                retention_rate = 0.0
            
            # ë§ê° ì¸¡ì • ê²°ê³¼ ì €ì¥
            result = {
                'past_task_id': past_task_id,
                'current_task_id': current_task_id,
                'env_config': past_env_config,
                'success_rate': success_rate,
                'avg_loss': avg_loss if avg_loss is not None else 0.0,
                'retention_rate': retention_rate,  # Loss ê¸°ë°˜ ìœ ì§€ìœ¨
                'target_loss': target_loss  # ëª©í‘œ Loss ì¶”ê°€
            }
            forgetting_results.append(result)
            
            avg_loss_str = f"{avg_loss:.6f}" if avg_loss is not None else "N/A"
            print(f"     â†’ Task {past_task_id} Success Rate: {success_rate:.3f}, Loss: {avg_loss_str} (Target: {target_loss}, Retention: {retention_rate:.1f}%)")
            
        except Exception as e:
            print(f"     â†’ Error measuring Task {past_task_id}: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ê¸°ë¡
            forgetting_results.append({
                'past_task_id': past_task_id,
                'current_task_id': current_task_id,
                'env_config': past_env_config,
                'success_rate': 0.0,
                'avg_loss': 0.0,
                'retention_rate': 0.0,
                'error': str(e)
            })
    
    return forgetting_results


class DummyWandbRun:
    def __init__(self):
        self.mode = "disabled"

    def log(self, *args, **kwargs):
        pass

    def watch(self, *args, **kwargs):
        pass

    def config(self, *args, **kwargs):
        pass

    def finish(self):
        pass

def planning_main(cfg_dict):
    output_dir = cfg_dict["saved_folder"]
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    if cfg_dict["wandb_logging"]:
        wandb_run = wandb.init(
            project=f"continual_plan_{cfg_dict['planner']['name']}", config=cfg_dict
        )
        wandb.run.name = "{}".format(output_dir.split("plan_outputs/")[-1])
    else:
        wandb_run = None

    ckpt_base_path = cfg_dict["ckpt_base_path"]
    model_path = f"{ckpt_base_path}/outputs/{cfg_dict['model_name']}/"
    with open(os.path.join(model_path, "hydra.yaml"), "r") as f:
        model_cfg = OmegaConf.load(f)

    seed(cfg_dict["seed"])
    _, dset = hydra.utils.call(
        model_cfg.env.dataset,
        num_hist=model_cfg.num_hist,
        num_pred=model_cfg.num_pred,
        frameskip=model_cfg.frameskip,
    )
    dset = dset["valid"]

    # --- â–¼ 1. ëª¨ë¸ ë¡œë”©ì„ ë£¨í”„ ë°–ìœ¼ë¡œ ì´ë™ â–¼ ---
    # ëª¨ë¸ì€ ëª¨ë“  íƒœìŠ¤í¬ì— ê±¸ì³ ìƒíƒœê°€ ìœ ì§€ë˜ì–´ì•¼ í•˜ë¯€ë¡œ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
    num_action_repeat = model_cfg.num_action_repeat
    model_ckpt = (
        Path(model_path) / "checkpoints" / f"model_{cfg_dict['model_epoch']}.pth"
    )
    model = load_model(model_ckpt, model_cfg, num_action_repeat, device=device)

    # --- â–¼ 2. ì—°ì† í•™ìŠµì„ ìœ„í•œ íƒœìŠ¤í¬ ì •ì˜ â–¼ ---
    # 11ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í™˜ê²½ ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.
    task_configs = [
        # Original (ê¸°ë³¸ ì„¤ì •)
        {'shape': 'T', 'color': 'LightSlateGray', 'background_color': 'White'},
        
        # ë¸”ë¡ ëª¨ì–‘ ë³€í™”
        {'shape': 'square', 'color': 'LightSlateGray', 'background_color': 'White'}, # Task 2: ì •ì‚¬ê°í˜•
        {'shape': 'small_tee', 'color': 'LightSlateGray', 'background_color': 'White'}, # Task 3: small_tee
        {'shape': 'L', 'color': 'LightSlateGray', 'background_color': 'White'},     # Task 4: L
        
        # ë¸”ë¡ ìƒ‰ìƒ ë³€í™”
        {'shape': 'T', 'color': 'Black', 'background_color': 'White'},              # Task 7: ë¸”ë¡ ê²€ì •
        {'shape': 'T', 'color': 'Yellow', 'background_color': 'White'},             # Task 8: ë¸”ë¡ ë…¸ë‘

        # ë°°ê²½ìƒ‰ ë³€í™”
        {'shape': 'T', 'color': 'LightSlateGray', 'background_color': 'Black'},      # Task 5: ë°°ê²½ ê²€ì •
        {'shape': 'T', 'color': 'LightSlateGray', 'background_color': 'Red'},       # Task 6: ë°°ê²½ ë¹¨ê°•
        
        # ë³µí•©ì  ë³€í™” (ë§¨ ë’¤ì— ë°°ì¹˜)
        {'shape': 'square', 'color': 'LightSlateGray', 'background_color': 'Black'}, # Task 9: ì •ì‚¬ê°í˜• + ë°°ê²½ ê²€ì •
        {'shape': 'L', 'color': 'Yellow', 'background_color': 'White'},             # Task 10: L + ë¸”ë¡ ë…¸ë‘
        {'shape': 'T', 'color': 'Black', 'background_color': 'Red'},              # Task 11: ë¸”ë¡ ê²€ì • + ë°°ê²½ ë¹¨ê°•
    ]
    num_tasks = len(task_configs)
    overall_logs = []
    
    # íƒœìŠ¤í¬ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    task_summary = []
    current_task_start_time = None
    
    # íŒŒêµ­ì  ë§ê° ì¸¡ì •ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    past_task_envs = []  # ê³¼ê±° íƒœìŠ¤í¬ í™˜ê²½ë“¤ ì €ì¥
    forgetting_metrics = []  # ê° íƒœìŠ¤í¬ì—ì„œì˜ ê³¼ê±° ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼

    # --- â–¼ 3. íƒœìŠ¤í¬ë¥¼ ìˆœíšŒí•˜ëŠ” ìµœìƒìœ„ ì œì–´ ë£¨í”„ ìƒì„± â–¼ ---
    for task_id, env_config in enumerate(task_configs):
        print(f"\n{'='*25} Starting Task {task_id + 1}/{num_tasks} {'='*25}")
        print(f"Environment Config: {env_config}")
        
        # íƒœìŠ¤í¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        current_task_start_time = time.time()
        task_planning_steps = 0
        task_lora_stacks = 0
        
        # íƒœìŠ¤í¬ ë³€ê²½ ì‹œ LoRA ì ì¸µ íŠ¸ë¦¬ê±° (ì²« ë²ˆì§¸ íƒœìŠ¤í¬ê°€ ì•„ë‹Œ ê²½ìš°)
        if task_id > 0:
            print(f"\nTask transition detected: Task {task_id} â†’ Task {task_id + 1}")
            print(f"Environment change: {task_configs[task_id-1]} â†’ {env_config}")

        # 3-A. í˜„ì¬ íƒœìŠ¤í¬ì— ë§ëŠ” í™˜ê²½ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        #      (pusht_env.pyê°€ ì´ ì¸ìë“¤ì„ ë°›ë„ë¡ ìˆ˜ì •ë˜ì—ˆë‹¤ê³  ê°€ì •)
        env = SubprocVectorEnv(
            [
                lambda: gym.make(
                    model_cfg.env.name,
                    *model_cfg.env.args,
                    **model_cfg.env.kwargs,
                    **env_config  # ì—¬ê¸°ì— shape, color ë“± ì¸ì ì „ë‹¬
                )
                for _ in range(cfg_dict["n_evals"])
            ]
        )

        task_cfg_dict = cfg_dict.copy()
        task_cfg_dict["planner"]["logging_prefix"] = f"task_{task_id+1:02d}_plan"
        task_cfg_dict["planner"]["sub_planner"]["logging_prefix"] = f"task_{task_id+1:02d}_plan"

        # 3-B. PlanWorkspaceë¥¼ ìƒì„±í•©ë‹ˆë‹¤. ëª¨ë¸(wm)ì€ ê³„ì† ìœ ì§€ë©ë‹ˆë‹¤.
        plan_workspace = PlanWorkspace(
            cfg_dict=task_cfg_dict,
            wm=model,
            dset=dset,
            env=env,
            env_name=model_cfg.env.name,
            frameskip=model_cfg.frameskip,
            wandb_run=wandb_run,
        )
        
        # LoRA ì ì¸µ ì½œë°± ì„¤ì • (íƒœìŠ¤í¬ ì¶”ì ì„ ìœ„í•´)
        if plan_workspace.is_online_lora and hasattr(plan_workspace.online_learner, 'on_lora_stack_callback'):
            def on_lora_stack(steps, loss):
                nonlocal task_lora_stacks
                task_lora_stacks += 1
                loss_str = f"{loss:.6f}" if loss is not None else "N/A"
                print(f"ğŸ”¥ LoRA Stack #{task_lora_stacks} at step {steps} with loss {loss_str}")
            plan_workspace.online_learner.on_lora_stack_callback = on_lora_stack
        
        # íƒœìŠ¤í¬ ë³€ê²½ ì‹œ LoRA ì ì¸µ íŠ¸ë¦¬ê±° (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì—ì„œë§Œ)
        if (hasattr(plan_workspace.online_learner, 'trigger_task_based_stacking') and
            hasattr(plan_workspace.online_learner, 'hybrid_enabled') and
            plan_workspace.online_learner.hybrid_enabled and
            hasattr(plan_workspace.online_learner, 'task_based_stacking') and
            plan_workspace.online_learner.task_based_stacking):
            print(f"\nğŸ¯ Triggering task-based LoRA stacking for Task {task_id + 1} (adds new LoRA layers to model)")
            success = plan_workspace.online_learner.trigger_task_based_stacking(task_id + 1, "task_change")
            if success:
                task_lora_stacks += 1
                print(f"âœ… Task-based LoRA stacking successful. Total stacks in task: {task_lora_stacks}")
            else:
                print(f"âš ï¸  Task-based LoRA stacking skipped or failed.")
        else:
            print(f"âš ï¸  Task-based LoRA stacking disabled (hybrid_enabled: {getattr(plan_workspace.online_learner, 'hybrid_enabled', False)}, task_based_stacking: {getattr(plan_workspace.online_learner, 'task_based_stacking', False)})")

        # 3-C. Loss ê¸°ë°˜ íƒœìŠ¤í¬ ì „í™˜ ë¡œì§
        task_switch_config = cfg_dict.get("lora", {}).get("task_switch", {})
        use_loss_based_switching = task_switch_config.get("enabled", False)
        
        if use_loss_based_switching:
            print("ğŸ“Š Using loss-based task transition logic (determines when to move to next task)")
            loss_threshold = task_switch_config.get("loss_threshold", )
            max_planning = task_switch_config.get("max_planning_per_task", )
            min_planning = task_switch_config.get("min_planning_per_task", )
            
            # Loss ìœˆë„ìš° ì´ˆê¸°í™” (íƒœìŠ¤í¬ ì „í™˜ìš©)
            loss_window_size = task_switch_config.get("loss_window_size", )
            loss_window = deque(maxlen=loss_window_size)
            planning_count = 0
            
            while planning_count < max_planning:
                planning_count += 1
                print(f"\n--- Task {task_id + 1}, Planning Step {planning_count} ---")
                
                # í”Œë˜ë‹ ìˆ˜í–‰
                logs = plan_workspace.perform_planning()
                task_planning_steps += 1
                
                # Loss ê°’ ì¶”ì¶œ (evaluatorì—ì„œ ì˜¨ë¼ì¸ í•™ìŠµì´ ìˆ˜í–‰ëœ ê²½ìš°)
                current_loss = None
                if plan_workspace.is_online_lora and hasattr(plan_workspace.online_learner, 'last_loss'):
                    current_loss = plan_workspace.online_learner.last_loss
                    print(f"Current loss: {current_loss:.6f} (threshold: {loss_threshold})")
                
                # ë¡œê·¸ ì €ì¥
                log_entry_with_task = {f"task_{task_id+1}/{k}": v for k, v in logs.items()}
                if current_loss is not None:
                    log_entry_with_task[f"task_{task_id+1}/current_loss"] = current_loss
                overall_logs.append(log_entry_with_task)
                
                if wandb_run:
                    wandb_run.log(log_entry_with_task)
                
                # Loss ê¸°ë°˜ ì „í™˜ ì¡°ê±´ í™•ì¸
                if current_loss is not None:
                    loss_window.append(current_loss)
                    
                    # ìµœì†Œ í”Œë˜ë‹ íšŸìˆ˜ í™•ì¸ í›„ Loss ì¡°ê±´ ì²´í¬
                    if (planning_count >= min_planning and 
                        len(loss_window) >= loss_window_size and
                        all(loss < loss_threshold for loss in list(loss_window)[-1:])):  # ìµœê·¼ 3ê°œê°€ ëª¨ë‘ ì„ê³„ê°’ ì´í•˜
                        print(f"âœ“ Loss converged below threshold {loss_threshold}. Moving to next task.")
                        break
                else:
                    # Loss ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° (ì˜¨ë¼ì¸ í•™ìŠµì´ ë¹„í™œì„±í™”ëœ ê²½ìš°) ê¸°ë³¸ íšŸìˆ˜ë¡œ ì§„í–‰
                    if planning_count >= min_planning:
                        print("No loss information available. Using default planning count.")
                        break
        else:
            # ê¸°ì¡´ ë°©ì‹: ê³ ì • íšŸìˆ˜
            num_planning_per_task = 1
            for i in range(num_planning_per_task):
                print(f"\n--- Task {task_id + 1}, Planning Step {i+1}/{num_planning_per_task} ---")
                logs = plan_workspace.perform_planning()
                log_entry_with_task = {f"task_{task_id+1}/{k}": v for k, v in logs.items()}
                overall_logs.append(log_entry_with_task)
                if wandb_run:
                    wandb_run.log(log_entry_with_task)

        # --- â–¼ 4. íƒœìŠ¤í¬ ì „í™˜ ì‹œ LoRA ì ì¸µ(Stacking) ë¡œì§ í˜¸ì¶œ â–¼ ---
        # Online-LoRA ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì–´ ìˆê³  ë§ˆì§€ë§‰ íƒœìŠ¤í¬ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        if (plan_workspace.is_online_lora and task_id < num_tasks - 1):
            print(f"\n--- End of Task {task_id + 1}. Adding new LoRA stack for the next task. ---")
            
            # ëª¨ë¸ì— LoRA ìŠ¤íƒ ì¶”ê°€ ë° ì˜µí‹°ë§ˆì´ì € ë¦¬ì…‹ì„ ìš”ì²­í•©ë‹ˆë‹¤.
            # ì´ ê¸°ëŠ¥ë“¤ì€ models/lora.py ì™€ planning/online.pyì— êµ¬í˜„ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
            if hasattr(model.predictor, 'add_new_lora_stack'):
                model.predictor.add_new_lora_stack()
            if hasattr(plan_workspace.online_learner, 'reset_optimizer'):
                plan_workspace.online_learner.reset_optimizer()

        # íƒœìŠ¤í¬ ì™„ë£Œ ìš”ì•½ ìƒì„±
        task_duration = time.time() - current_task_start_time
        
        # ì ì¸µ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì—ì„œ)
        stack_history = []
        if (plan_workspace.is_online_lora and 
            hasattr(plan_workspace.online_learner, 'stack_history')):
            stack_history = plan_workspace.online_learner.stack_history.copy()
        
        task_summary.append({
            'task_id': task_id + 1,
            'env_config': env_config,
            'planning_steps': task_planning_steps,
            'lora_stacks': task_lora_stacks,
            'duration_seconds': task_duration,
            'final_loss': current_loss if 'current_loss' in locals() else None,
            'stack_history': stack_history
        })
        
        print(f"\nğŸ“Š Task {task_id + 1} Summary:")
        print(f"   - Environment: {env_config}")
        print(f"   - Planning Steps: {task_planning_steps}")
        print(f"   - LoRA Stacks: {task_lora_stacks}")
        print(f"   - Duration: {task_duration:.2f}s")
        if 'current_loss' in locals() and current_loss is not None:
            print(f"   - Final Loss: {current_loss:.6f}")

        # í˜„ì¬ íƒœìŠ¤í¬ í™˜ê²½ì„ ê³¼ê±° íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        past_task_envs.append({
            'task_id': task_id + 1,
            'env_config': env_config,
            'env': env  # í™˜ê²½ ê°ì²´ ì €ì¥ (ì„±ëŠ¥ ì¸¡ì •ìš©)
        })
        
        # íŒŒêµ­ì  ë§ê° ì¸¡ì •: ê³¼ê±° íƒœìŠ¤í¬ë“¤ì— ëŒ€í•œ í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        if len(past_task_envs) > 1:  # ì²« ë²ˆì§¸ íƒœìŠ¤í¬ ì´í›„ë¶€í„° ì¸¡ì •
            print(f"\nğŸ§  Measuring Catastrophic Forgetting...")
            forgetting_results = measure_forgetting_on_past_tasks(
                model, past_task_envs, plan_workspace, task_id
            )
            forgetting_metrics.append(forgetting_results)
            
            # ë§ê° ì¸¡ì • ê²°ê³¼ ì¶œë ¥
            print(f"ğŸ“Š Forgetting Analysis for Task {task_id + 1}:")
            for result in forgetting_results:
                avg_loss_str = f"{result['avg_loss']:.6f}" if result['avg_loss'] is not None else "N/A"
                target_loss = result.get('target_loss', 0.1)
                print(f"   - Task {result['past_task_id']}: Success Rate {result['success_rate']:.3f}, "
                      f"Loss {avg_loss_str} (Target: {target_loss}, Retention: {result['retention_rate']:.1f}%)")

        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¥¼ ìœ„í•´ í˜„ì¬ íƒœìŠ¤í¬ì˜ í™˜ê²½ì„ ë‹«ìŠµë‹ˆë‹¤.
        # env.close()  # ê³¼ê±° íƒœìŠ¤í¬ ì¸¡ì •ì„ ìœ„í•´ í™˜ê²½ì„ ë‹«ì§€ ì•ŠìŒ

    print(f"\n{'='*25} Continual Learning Experiment Finished {'='*25}")
    
    # ì „ì²´ ì‹¤í—˜ ìš”ì•½ ë¦¬í¬íŠ¸ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ CONTINUAL LEARNING EXPERIMENT SUMMARY")
    print(f"{'='*60}")
    print(f"Total Tasks: {len(task_summary)}")
    print(f"Total Duration: {sum(task['duration_seconds'] for task in task_summary):.2f}s")
    print(f"Total LoRA Stacks: {sum(task['lora_stacks'] for task in task_summary)}")
    print(f"Total Planning Steps: {sum(task['planning_steps'] for task in task_summary)}")
    
    print(f"\nğŸ“‹ TASK-BY-TASK BREAKDOWN:")
    print(f"{'Task':<6} {'Environment':<40} {'Steps':<6} {'LoRA':<5} {'Duration':<8} {'Final Loss':<10}")
    print(f"{'-'*80}")
    
    for task in task_summary:
        env_str = f"{task['env_config'].get('shape', 'N/A')}-{task['env_config'].get('color', 'N/A')}-{task['env_config'].get('background_color', 'N/A')}"
        final_loss_str = f"{task['final_loss']:.6f}" if task['final_loss'] is not None else "N/A"
        print(f"{task['task_id']:<6} {env_str:<40} {task['planning_steps']:<6} {task['lora_stacks']:<5} {task['duration_seconds']:.2f}s{'':<4} {final_loss_str:<10}")
    
    print(f"\nğŸ¯ KEY INSIGHTS:")
    fastest_task = min(task_summary, key=lambda x: x['duration_seconds'])
    slowest_task = max(task_summary, key=lambda x: x['duration_seconds'])
    most_stacks = max(task_summary, key=lambda x: x['lora_stacks'])
    most_steps = max(task_summary, key=lambda x: x['planning_steps'])
    
    print(f"   - Fastest Task: Task {fastest_task['task_id']} ({fastest_task['duration_seconds']:.2f}s)")
    print(f"   - Slowest Task: Task {slowest_task['task_id']} ({slowest_task['duration_seconds']:.2f}s)")
    print(f"   - Most LoRA Stacks: Task {most_stacks['task_id']} ({most_stacks['lora_stacks']} stacks)")
    print(f"   - Most Planning Steps: Task {most_steps['task_id']} ({most_steps['planning_steps']} steps)")
    
    avg_steps = sum(task['planning_steps'] for task in task_summary) / len(task_summary)
    avg_stacks = sum(task['lora_stacks'] for task in task_summary) / len(task_summary)
    print(f"   - Average Planning Steps: {avg_steps:.1f}")
    print(f"   - Average LoRA Stacks: {avg_stacks:.1f}")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ì ì¸µ ë¶„ì„
    print(f"\nğŸ¯ HYBRID STACKING ANALYSIS")
    print(f"{'='*60}")
    
    total_task_based_stacks = 0
    total_loss_based_stacks = 0
    
    for task in task_summary:
        if 'stack_history' in task and task['stack_history']:
            task_based_count = sum(1 for stack in task['stack_history'] if stack['type'] == 'task_based')
            loss_based_count = sum(1 for stack in task['stack_history'] if stack['type'] == 'loss_based')
            total_task_based_stacks += task_based_count
            total_loss_based_stacks += loss_based_count
            
            print(f"Task {task['task_id']}: {task_based_count} task-based, {loss_based_count} loss-based stacks")
    
    print(f"\nğŸ“Š STACKING TYPE SUMMARY:")
    print(f"   - Total Task-based Stacks: {total_task_based_stacks}")
    print(f"   - Total Loss-based Stacks: {total_loss_based_stacks}")
    print(f"   - Total Stacks: {total_task_based_stacks + total_loss_based_stacks}")
    if total_task_based_stacks + total_loss_based_stacks > 0:
        task_based_ratio = total_task_based_stacks / (total_task_based_stacks + total_loss_based_stacks) * 100
        print(f"   - Task-based Ratio: {task_based_ratio:.1f}%")
    
    print(f"{'='*60}")
    
    # íŒŒêµ­ì  ë§ê° ë¶„ì„ ë¦¬í¬íŠ¸
    if forgetting_metrics:
        print(f"\nğŸ§  CATASTROPHIC FORGETTING ANALYSIS")
        print(f"{'='*60}")
        
        # ì „ì²´ ë§ê° í†µê³„
        all_retention_rates = []
        task_retention_summary = {}
        
        for task_idx, forgetting_results in enumerate(forgetting_metrics):
            current_task_id = task_idx + 2  # ì²« ë²ˆì§¸ íƒœìŠ¤í¬ ì´í›„ë¶€í„° ì¸¡ì •
            print(f"\nğŸ“Š Forgetting Analysis after Task {current_task_id}:")
            
            for result in forgetting_results:
                past_task_id = result['past_task_id']
                retention_rate = result['retention_rate']
                all_retention_rates.append(retention_rate)
                
                # ê° ê³¼ê±° íƒœìŠ¤í¬ë³„ í‰ê·  ìœ ì§€ìœ¨ ê³„ì‚°
                if past_task_id not in task_retention_summary:
                    task_retention_summary[past_task_id] = []
                task_retention_summary[past_task_id].append(retention_rate)
                
                env_str = f"{result['env_config'].get('shape', 'N/A')}-{result['env_config'].get('color', 'N/A')}-{result['env_config'].get('background_color', 'N/A')}"
                print(f"   Task {past_task_id} ({env_str}): {retention_rate:.1f}% retention")
        
        # ì „ì²´ ë§ê° ë¶„ì„
        avg_retention = sum(all_retention_rates) / len(all_retention_rates) if all_retention_rates else 0
        min_retention = min(all_retention_rates) if all_retention_rates else 0
        max_retention = max(all_retention_rates) if all_retention_rates else 0
        
        print(f"\nğŸ¯ OVERALL FORGETTING STATISTICS:")
        print(f"   - Average Retention Rate: {avg_retention:.1f}%")
        print(f"   - Minimum Retention Rate: {min_retention:.1f}%")
        print(f"   - Maximum Retention Rate: {max_retention:.1f}%")
        print(f"   - Total Measurements: {len(all_retention_rates)}")
        
        # ê° íƒœìŠ¤í¬ë³„ í‰ê·  ìœ ì§€ìœ¨
        print(f"\nğŸ“ˆ TASK-SPECIFIC RETENTION RATES:")
        for past_task_id in sorted(task_retention_summary.keys()):
            avg_retention_for_task = sum(task_retention_summary[past_task_id]) / len(task_retention_summary[past_task_id])
            print(f"   Task {past_task_id}: {avg_retention_for_task:.1f}% average retention")
        
        # ë§ê° ì‹¬ê°ë„ í‰ê°€
        print(f"\nâš ï¸  FORGETTING SEVERITY ASSESSMENT:")
        if avg_retention >= 80:
            print(f"   ğŸŸ¢ LOW FORGETTING: ({avg_retention:.1f}%)")
        elif avg_retention >= 60:
            print(f"   ğŸŸ¡ MODERATE FORGETTING: ({avg_retention:.1f}%)")
        elif avg_retention >= 40:
            print(f"   ğŸŸ  HIGH FORGETTING: ({avg_retention:.1f}%)")
        else:
            print(f"   ğŸ”´ SEVERE FORGETTING: ({avg_retention:.1f}%)")
        print(f"{'='*60}")
    
    # í™˜ê²½ ì •ë¦¬
    for past_task in past_task_envs:
        if 'env' in past_task and past_task['env']:
            past_task['env'].close()

    return overall_logs


@hydra.main(config_path="conf", config_name="plan")
def main(cfg: OmegaConf):
    with open_dict(cfg):
        cfg["saved_folder"] = os.getcwd()
        log.info(f"Planning result saved dir: {cfg['saved_folder']}")
    cfg_dict = cfg_to_dict(cfg)
    cfg_dict["wandb_logging"] = True
    planning_main(cfg_dict)


if __name__ == "__main__":
    main()