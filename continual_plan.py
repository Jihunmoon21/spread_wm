import os
import gym
import json
import hydra
import random
import torch
import pickle
import wandb
import logging
import warnings
import numpy as np
from utils import move_to_device
import time
import submitit
from itertools import product
from pathlib import Path
from einops import rearrange
from omegaconf import OmegaConf, open_dict
import sys

from env.venv import SubprocVectorEnv
from custom_resolvers import replace_slash
from preprocessor import Preprocessor
from planning.evaluator import PlanEvaluator
from utils import cfg_to_dict, seed
from collections import deque

from models.vit import ViTPredictor
from models.lora import LoRA_ViT_spread
from planning.online import OnlineLora
from planning.lora_ensemble import EnsembleOnlineLora

warnings.filterwarnings("ignore")
log = logging.getLogger(__name__)

ALL_MODEL_KEYS = [
    "encoder",
    "predictor",
    "decoder",
    "proprio_encoder",
    "action_encoder",
]

def planning_main_in_dir(working_dir, cfg_dict):
    os.chdir(working_dir)
    return planning_main(cfg_dict=cfg_dict)

def launch_plan_jobs(
    epoch,
    cfg_dicts,
    plan_output_dir,
):
    with submitit.helpers.clean_env():
        jobs = []
        for cfg_dict in cfg_dicts:
            subdir_name = f"{cfg_dict['planner']['name']}_goal_source={cfg_dict['goal_source']}_goal_H={cfg_dict['goal_H']}_alpha={cfg_dict['objective']['alpha']}"
            subdir_path = os.path.join(plan_output_dir, subdir_name)
            executor = submitit.AutoExecutor(
                folder=subdir_path, slurm_max_num_timeout=20
            )
            executor.update_parameters(
                **{
                    k: v
                    for k, v in cfg_dict["hydra"]["launcher"].items()
                    if k != "submitit_folder"
                }
            )
            cfg_dict["saved_folder"] = subdir_path
            cfg_dict["wandb_logging"] = False  # don't init wandb
            job = executor.submit(planning_main_in_dir, subdir_path, cfg_dict)
            jobs.append((epoch, subdir_name, job))
            print(
                f"Submitted evaluation job for checkpoint: {subdir_path}, job id: {job.job_id}"
            )
        return jobs


def build_plan_cfg_dicts(
    plan_cfg_path="",
    ckpt_base_path="",
    model_name="",
    model_epoch="final",
    planner=["gd", "cem"],
    goal_source=["dset"],
    goal_H=[1, 5, 10],
    alpha=[0, 0.1, 1],
):
    """
    Return a list of plan overrides, for model_path, add a key in the dict {"model_path": model_path}.
    """
    config_path = os.path.dirname(plan_cfg_path)
    overrides = [
        {
            "planner": p,
            "goal_source": g_source,
            "goal_H": g_H,
            "ckpt_base_path": ckpt_base_path,
            "model_name": model_name,
            "model_epoch": model_epoch,
            "objective": {"alpha": a},
        }
        for p, g_source, g_H, a in product(planner, goal_source, goal_H, alpha)
    ]
    cfg = OmegaConf.load(plan_cfg_path)
    cfg_dicts = []
    for override_args in overrides:
        planner = override_args["planner"]
        planner_cfg = OmegaConf.load(
            os.path.join(config_path, f"planner/{planner}.yaml")
        )
        cfg["planner"] = OmegaConf.merge(cfg.get("planner", {}), planner_cfg)
        override_args.pop("planner")
        cfg = OmegaConf.merge(cfg, OmegaConf.create(override_args))
        cfg_dict = OmegaConf.to_container(cfg)
        cfg_dict["planner"]["horizon"] = cfg_dict["goal_H"]  # assume planning horizon equals to goal horizon
        cfg_dicts.append(cfg_dict)
    return cfg_dicts

# PlanWorkspace í´ë˜ìŠ¤ì˜ __init__ ë©”ì„œë“œ 
class PlanWorkspace:
    def __init__(
        self,
        cfg_dict: dict,
        wm: torch.nn.Module,
        dset,
        env: SubprocVectorEnv,
        env_name: str,
        frameskip: int,
        wandb_run: wandb.run,
        current_task_id: int = None,  # í˜„ì¬ íƒœìŠ¤í¬ ID ì¶”ê°€
    ):
        # --- 1. ê¸°ë³¸ ì†ì„± ì´ˆê¸°í™” ---
        self.cfg_dict = cfg_dict
        self.wm = wm
        self.dset = dset
        self.env = env
        self.env_name = env_name
        self.frameskip = frameskip
        self.wandb_run = wandb_run
        self.current_task_id = current_task_id  # í˜„ì¬ íƒœìŠ¤í¬ ID ì €ì¥
        self.device = next(wm.parameters()).device
        
        # ğŸ”§ ê° í‰ê°€ë§ˆë‹¤ ë‹¤ë¥¸ ì‹œë“œ ì‚¬ìš© (ë‹¤ì–‘ì„± ë³´ì¥)
        self.eval_seed = [cfg_dict["seed"] * n + 1 for n in range(cfg_dict["n_evals"])]
        print("eval_seed: ", self.eval_seed)
        self.n_evals = cfg_dict["n_evals"]
        self.goal_source = cfg_dict["goal_source"]
        self.goal_H = cfg_dict["goal_H"]
        self.action_dim = self.dset.action_dim * self.frameskip
        self.debug_dset_init = cfg_dict["debug_dset_init"]

        objective_fn = hydra.utils.call(
            cfg_dict["objective"],
        )

        self.data_preprocessor = Preprocessor(
            action_mean=self.dset.action_mean,
            action_std=self.dset.action_std,
            state_mean=self.dset.state_mean,
            state_std=self.dset.state_std,
            proprio_mean=self.dset.proprio_mean,
            proprio_std=self.dset.proprio_std,
            transform=self.dset.transform,
        )

        if self.cfg_dict["goal_source"] == "file":
            self.prepare_targets_from_file(cfg_dict["goal_file_path"])
        else:
            self.prepare_targets()

        # --- LoRA í•™ìŠµ ê´€ë ¨ ì„¤ì • ë° ê°ì²´ ìƒì„± ---
        self.online_learner = None # OnlineLora ë˜ëŠ” EnsembleOnlineLora ê°ì²´ë¥¼ ë‹´ì„ ë³€ìˆ˜
        self.is_lora_enabled = self.cfg_dict.get("lora", {}).get("enabled", False)
        self.is_online_lora = self.cfg_dict.get("lora", {}).get("online", False)

        if self.is_lora_enabled:
            # ì•™ìƒë¸” LoRA ì‚¬ìš© ì—¬ë¶€ í™•ì¸
            # lora.ensembleë¡œ í”Œë˜ê·¸ ê²½ë¡œ ë³€ê²½
            use_ensemble_lora = self.cfg_dict.get("lora", {}).get("ensemble", False)
            
            if use_ensemble_lora:
                print("INFO: Ensemble LoRA training enabled. Initializing EnsembleOnlineLora module.")
                self.online_learner = EnsembleOnlineLora(workspace=self)
            else:
                print("INFO: Standard LoRA training enabled. Initializing OnlineLora module.")
                self.online_learner = OnlineLora(workspace=self)

        self.evaluator = PlanEvaluator(
            obs_0=self.obs_0,
            obs_g=self.obs_g,
            state_0=self.state_0,
            state_g=self.state_g,
            env=self.env,
            wm=self.wm,
            frameskip=self.frameskip,
            seed=self.eval_seed,
            preprocessor=self.data_preprocessor,
            n_plot_samples=self.cfg_dict["n_plot_samples"],
            workspace=self, # workspaceë¥¼ evaluatorì— ì „ë‹¬í•˜ëŠ” ê²ƒì€ ê·¸ëŒ€ë¡œ ìœ ì§€
            is_lora_enabled=self.is_lora_enabled,
            is_online_lora=self.is_online_lora,
        )

        if self.wandb_run is None or isinstance(
            self.wandb_run, wandb.sdk.lib.disabled.RunDisabled
        ):
            self.wandb_run = DummyWandbRun()

        self.log_filename = "logs.json"  # planner and final eval logs are dumped here
        # ğŸ”§ ì•™ìƒë¸” ë§¤ë‹ˆì €ë¥¼ í”Œë˜ë„ˆì— ì „ë‹¬
        planner_kwargs = {
            "wm": self.wm,
            "env": self.env,  # only for mpc
            "action_dim": self.action_dim,
            "objective_fn": objective_fn,
            "preprocessor": self.data_preprocessor,
            "evaluator": self.evaluator,
            "wandb_run": self.wandb_run,
            "log_filename": self.log_filename,
        }
        
        # ì•™ìƒë¸” LoRAê°€ í™œì„±í™”ëœ ê²½ìš° ì•™ìƒë¸” ë§¤ë‹ˆì € ì „ë‹¬
        if (self.is_online_lora and 
            hasattr(self.online_learner, 'ensemble_manager')):
            planner_kwargs["ensemble_manager"] = self.online_learner.ensemble_manager
            print(f"ğŸ”§ Passing ensemble manager to planner")
        
        self.planner = hydra.utils.instantiate(
            self.cfg_dict["planner"],
            **planner_kwargs
        )

        # optional: assume planning horizon equals to goal horizon
        from planning.mpc import MPCPlanner
        if isinstance(self.planner, MPCPlanner):
            self.planner.sub_planner.horizon = cfg_dict["goal_H"]
            self.planner.n_taken_actions = cfg_dict["goal_H"]
        else:
            self.planner.horizon = cfg_dict["goal_H"]
            
        self.dump_targets()

    def prepare_targets(self):
        states = []
        actions = []
        observations = []
        
        if self.goal_source == "random_state":
            # update env config from val trajs
            observations, states, actions, env_info = (
                self.sample_traj_segment_from_dset(traj_len=2)
            )
            self.env.update_env(env_info)

            # sample random states
            rand_init_state, rand_goal_state = self.env.sample_random_init_goal_states(
                self.eval_seed
            )
            if self.env_name == "deformable_env": # take rand init state from dset for deformable envs
                rand_init_state = np.array([x[0] for x in states])

            obs_0, state_0 = self.env.prepare(self.eval_seed, rand_init_state)
            obs_g, state_g = self.env.prepare(self.eval_seed, rand_goal_state)

            # add dim for t
            for k in obs_0.keys():
                obs_0[k] = np.expand_dims(obs_0[k], axis=1)
                obs_g[k] = np.expand_dims(obs_g[k], axis=1)

            self.obs_0 = obs_0
            self.obs_g = obs_g
            self.state_0 = rand_init_state  # (b, d)
            self.state_g = rand_goal_state
            self.gt_actions = None
        else:
            # update env config from val trajs
            observations, states, actions, env_info = (
                self.sample_traj_segment_from_dset(traj_len=self.frameskip * self.goal_H + 1)
            )
            self.env.update_env(env_info)

            # get states from val trajs
            init_state = [x[0] for x in states]
            init_state = np.array(init_state)
            actions = torch.stack(actions)
            if self.goal_source == "random_action":
                actions = torch.randn_like(actions)
            wm_actions = rearrange(actions, "b (t f) d -> b t (f d)", f=self.frameskip)
            exec_actions = self.data_preprocessor.denormalize_actions(actions)
            # replay actions in env to get gt obses
            rollout_obses, rollout_states = self.env.rollout(
                self.eval_seed, init_state, exec_actions.numpy()
            )
            self.obs_0 = {
                key: np.expand_dims(arr[:, 0], axis=1)
                for key, arr in rollout_obses.items()
            }
            self.obs_g = {
                key: np.expand_dims(arr[:, -1], axis=1)
                for key, arr in rollout_obses.items()
            }
            self.state_0 = init_state  # (b, d)
            self.state_g = rollout_states[:, -1]  # (b, d)
            self.gt_actions = wm_actions

    def sample_traj_segment_from_dset(self, traj_len):
        states = []
        actions = []
        observations = []
        env_info = []

        # Check if any trajectory is long enough
        valid_traj = [
            self.dset[i][0]["visual"].shape[0]
            for i in range(len(self.dset))
            if self.dset[i][0]["visual"].shape[0] >= traj_len
        ]
        if len(valid_traj) == 0:
            raise ValueError("No trajectory in the dataset is long enough.")

        # sample init_states from dset
        for i in range(self.n_evals):
            max_offset = -1
            while max_offset < 0:  # filter out traj that are not long enough
                traj_id = random.randint(0, len(self.dset) - 1)
                obs, act, state, e_info = self.dset[traj_id]
                max_offset = obs["visual"].shape[0] - traj_len
            state = state.numpy()
            offset = random.randint(0, max_offset)
            obs = {
                key: arr[offset : offset + traj_len]
                for key, arr in obs.items()
            }
            state = state[offset : offset + traj_len]
            act = act[offset : offset + self.frameskip * self.goal_H]
            actions.append(act)
            states.append(state)
            observations.append(obs)
            env_info.append(e_info)
        return observations, states, actions, env_info

    def prepare_targets_from_file(self, file_path):
        with open(file_path, "rb") as f:
            data = pickle.load(f)
        self.obs_0 = data["obs_0"]
        self.obs_g = data["obs_g"]
        self.state_0 = data["state_0"]
        self.state_g = data["state_g"]
        self.gt_actions = data["gt_actions"]
        self.goal_H = data["goal_H"]

    def dump_targets(self):
        with open("plan_targets.pkl", "wb") as f:
            pickle.dump(
                {
                    "obs_0": self.obs_0,
                    "obs_g": self.obs_g,
                    "state_0": self.state_0,
                    "state_g": self.state_g,
                    "gt_actions": self.gt_actions,
                    "goal_H": self.goal_H,
                },
                f,
            )
        file_path = os.path.abspath("plan_targets.pkl")
        print(f"Dumped plan targets to {file_path}")

    def perform_planning(self):
        if self.debug_dset_init:
            actions_init = self.gt_actions
        else:
            actions_init = None

        # ğŸ”§ íƒœìŠ¤í¬ ì „í™˜ ê°ì§€ ë° ì²˜ë¦¬ (ì¬í˜¸ì¶œ ì œê±°: ë°”ê¹¥ì—ì„œ ì´ë¯¸ ì„¤ì •ëœ í”Œë˜ê·¸ ì‚¬ìš©)
        if self.is_online_lora and hasattr(self.online_learner, 'task_changed'):
            # í˜„ì¬ íƒœìŠ¤í¬ IDëŠ” ì›Œí¬ìŠ¤í˜ì´ìŠ¤ ìƒì„± ì‹œ ì£¼ì…ë¨; ì—¬ê¸°ì„œ ì¬ê³„ì‚°/ì¬í˜¸ì¶œí•˜ì§€ ì•ŠìŒ
            task_changed = self.online_learner.task_changed
        
        # ğŸ”§ íƒœìŠ¤í¬ ì „í™˜ ì‹œì—ë§Œ ì•™ìƒë¸” ì¶”ë¡  ì‚¬ìš©
        # lora.ensemble_cfgë¡œ ê²½ë¡œ ë³€ê²½ (ì—†ìœ¼ë©´ ë¹ˆ dict)
        ensemble_cfg = self.cfg_dict.get("lora", {}).get("ensemble_cfg", {})
        inference_cfg = ensemble_cfg.get("inference", {})
        usage_strategy = inference_cfg.get("usage_strategy", "task_change_only")
        
        if (usage_strategy in ["task_change_only", "always"] and 
            self.is_online_lora and 
            hasattr(self.online_learner, 'ensemble_manager') and
            (len(self.online_learner.ensemble_manager.ensemble_members) > 0 or 
             getattr(self, 'current_task_id', 1) == 1) and  # ì²« ë²ˆì§¸ íƒœìŠ¤í¬ë„ í—ˆìš©
            (usage_strategy == "always" or 
             (hasattr(self.online_learner, 'task_changed') and self.online_learner.task_changed))):
            if usage_strategy == "always":
                print(f"ğŸ”„ Using ensemble for optimal member selection (always mode)...")
            else:
                print(f"ğŸ”„ Task changed! Using ensemble for optimal member selection...")
            
            # íƒœìŠ¤í¬ ì „í™˜ ì‹œ ì•™ìƒë¸” ê¸°ë°˜ ìµœì  ë©¤ë²„ ì„ íƒ
            self.online_learner.perform_task_change_ensemble_selection(self)
            
            # task_changed í”Œë˜ê·¸ ë¦¬ì…‹
            if hasattr(self.online_learner, 'reset_task_changed_flag'):
                self.online_learner.reset_task_changed_flag()
            
            # ì¼ë°˜ í”Œë˜ë‹ ìˆ˜í–‰ (ì„ íƒëœ ë©¤ë²„ ì‚¬ìš©)
            actions, action_len = self.planner.plan(
                obs_0=self.obs_0,
                obs_g=self.obs_g,
                actions=actions_init,
            )
        else:
            # ê¸°ì¡´ ë°©ì‹: ì¼ë°˜ í”Œë˜ë‹ ìˆ˜í–‰
            actions, action_len = self.planner.plan(
                obs_0=self.obs_0,
                obs_g=self.obs_g,
                actions=actions_init,
            )
        
        logs, successes, _, _ = self.evaluator.eval_actions(
            actions.detach(), action_len, save_video=True, filename="output_final"
        )
        logs = {f"final_eval/{k}": v for k, v in logs.items()}
        self.wandb_run.log(logs)
        logs_entry = {
            key: (
                value.item()
                if isinstance(value, (np.float32, np.int32, np.int64))
                else value
            )
            for key, value in logs.items()
        }
        with open(self.log_filename, "a") as file:
            file.write(json.dumps(logs_entry) + "\n")
        return logs
    
    


def load_ckpt(snapshot_path, device):
    with snapshot_path.open("rb") as f:
        payload = torch.load(f, map_location=device, weights_only=False)
    loaded_keys = []
    result = {}
    # ë””ë²„ê¹…: ì²´í¬í¬ì¸íŠ¸ì— ìˆëŠ” ëª¨ë“  í‚¤ ì¶œë ¥
    print(f"Checkpoint keys: {list(payload.keys())}")
    for k, v in payload.items():
        if k in ALL_MODEL_KEYS:
            loaded_keys.append(k)
            # None ì²´í¬ ì¶”ê°€ (dummy checkpoint ì§€ì›)
            if v is not None:
                result[k] = v.to(device)
            else:
                result[k] = None
    result["epoch"] = payload.get("epoch", 0)
    print(f"Loaded model keys: {loaded_keys}")
    print(f"Missing model keys: {set(ALL_MODEL_KEYS) - set(loaded_keys)}")
    return result

def load_model(model_ckpt, train_cfg, num_action_repeat, device):
    result = {}
    if model_ckpt.exists():
        result = load_ckpt(model_ckpt, device)
        print(f"Resuming from epoch {result['epoch']}: {model_ckpt}")
    else:
        raise FileNotFoundError(f"Model checkpoint not found: {model_ckpt}")

    if "encoder" not in result:
        result["encoder"] = hydra.utils.instantiate(
            train_cfg.encoder,
        )
    if "predictor" not in result:
        raise ValueError(
            f"Predictor not found in model checkpoint: {model_ckpt}\n"
            f"Available keys in checkpoint: {list(result.keys())}\n"
            f"Expected keys: {ALL_MODEL_KEYS}"
        )

    if train_cfg.has_decoder and "decoder" not in result:
        base_path = os.path.dirname(os.path.abspath(__file__))
        if train_cfg.env.decoder_path is not None:
            decoder_path = os.path.join(base_path, train_cfg.env.decoder_path)
            ckpt = torch.load(decoder_path)
            if isinstance(ckpt, dict):
                result["decoder"] = ckpt["decoder"]
            else:
                result["decoder"] = torch.load(decoder_path)
        else:
            raise ValueError(
                "Decoder path not found in model checkpoint \
                                and is not provided in config"
            )
    elif not train_cfg.has_decoder:
        result["decoder"] = None

    model = hydra.utils.instantiate(
        train_cfg.model,
        encoder=result["encoder"],
        proprio_encoder=result["proprio_encoder"],
        action_encoder=result["action_encoder"],
        predictor=result["predictor"],
        decoder=result["decoder"],
        proprio_dim=train_cfg.proprio_emb_dim,
        action_dim=train_cfg.action_emb_dim,
        concat_dim=train_cfg.concat_dim,
        num_action_repeat=num_action_repeat,
        num_proprio_repeat=train_cfg.num_proprio_repeat,
    )
    model.to(device)
    return model


def measure_forgetting_on_past_tasks(model, past_task_envs, plan_workspace, current_task_id, ensemble_cache_dir=None):
    """
    ê³¼ê±° íƒœìŠ¤í¬ë“¤ì— ëŒ€í•œ í˜„ì¬ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ì¸¡ì •í•˜ì—¬ íŒŒêµ­ì  ë§ê°ì„ ë¶„ì„í•©ë‹ˆë‹¤.
    granular_plan.pyì²˜ëŸ¼ í•´ë‹¹ íƒœìŠ¤í¬ì˜ ì•™ìƒë¸” ë©¤ë²„ë¥¼ ë¡œë“œí•˜ì—¬ í‰ê°€í•©ë‹ˆë‹¤.
    
    Args:
        model: í˜„ì¬ í•™ìŠµëœ ëª¨ë¸
        past_task_envs: ê³¼ê±° íƒœìŠ¤í¬ í™˜ê²½ë“¤ì˜ ë¦¬ìŠ¤íŠ¸
        plan_workspace: í˜„ì¬ í”Œë˜ë‹ ì›Œí¬ìŠ¤í˜ì´ìŠ¤
        current_task_id: í˜„ì¬ íƒœìŠ¤í¬ ID
        ensemble_cache_dir: ì•™ìƒë¸” ë©¤ë²„ ìºì‹œ ë””ë ‰í† ë¦¬ ê²½ë¡œ
    
    Returns:
        forgetting_results: ê° ê³¼ê±° íƒœìŠ¤í¬ì— ëŒ€í•œ ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼ (í˜„ì¬ ëª¨ë¸ + ì•™ìƒë¸” ë©¤ë²„)
    """
    forgetting_results = []
    device = next(model.parameters()).device
    
    # ì•™ìƒë¸” ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
    if ensemble_cache_dir is None:
        ensemble_cfg = plan_workspace.cfg_dict.get("lora", {}).get("ensemble_cfg", {})
        ensemble_cache_dir = os.path.abspath(ensemble_cfg.get("cache_dir", "./lora_cache"))
    
    for past_task in past_task_envs[:-1]:  # ë§ˆì§€ë§‰(í˜„ì¬) íƒœìŠ¤í¬ ì œì™¸
        past_task_id = past_task['task_id']
        past_env = past_task['env']
        past_env_config = past_task['env_config']
        
        print(f"   Testing on Task {past_task_id}: {past_env_config}")
        
        # ë‘ ê°€ì§€ ëª¨ë“œë¡œ ì¸¡ì •: í˜„ì¬ ëª¨ë¸ê³¼ ì•™ìƒë¸” ë©¤ë²„
        for mode_label, use_ensemble_member in [("current_model", False), ("ensemble_member", True)]:
            if use_ensemble_member and not plan_workspace.is_online_lora:
                continue  # ì•™ìƒë¸”ì´ ì—†ìœ¼ë©´ ê±´ë„ˆëœ€
            
            try:
                # ê³¼ê±° íƒœìŠ¤í¬ í™˜ê²½ì—ì„œ í”Œë˜ë‹ ìˆ˜í–‰
                past_plan_workspace = PlanWorkspace(
                    cfg_dict=plan_workspace.cfg_dict.copy(),
                    wm=model,  # í˜„ì¬ í•™ìŠµëœ ëª¨ë¸ ì‚¬ìš©
                    dset=plan_workspace.dset,
                    env=past_env,
                    env_name=plan_workspace.env_name,
                    frameskip=plan_workspace.frameskip,
                    wandb_run=None,  # ë§ê° ì¸¡ì • ì‹œì—ëŠ” wandb ë¡œê¹… ë¹„í™œì„±í™”
                    current_task_id=past_task_id,
                )
                
                # ì•™ìƒë¸” ë©¤ë²„ ë¡œë“œ (í•´ë‹¹ íƒœìŠ¤í¬ì˜ ë©¤ë²„)
                loaded_file = "N/A"
                if use_ensemble_member:
                    ensemble_member_lora_path = os.path.join(
                        ensemble_cache_dir, f"lora_task_{past_task_id}.pth"
                    )
                    if os.path.exists(ensemble_member_lora_path):
                        try:
                            lora_weights = torch.load(ensemble_member_lora_path, map_location=device)
                            learner = getattr(past_plan_workspace, "online_learner", None)
                            if (
                                past_plan_workspace.is_online_lora
                                and learner is not None
                                and hasattr(learner, "_apply_lora_weights")
                            ):
                                success = learner._apply_lora_weights(lora_weights)
                                if success and hasattr(learner, "last_selected_member_task_id"):
                                    setattr(learner, "last_selected_member_task_id", past_task_id)
                                loaded_file = os.path.basename(ensemble_member_lora_path)
                                print(f"     ğŸ“¥ Loaded ensemble member for Task {past_task_id} from {loaded_file}")
                            else:
                                print(f"     âš ï¸  Could not apply LoRA weights (is_online_lora={past_plan_workspace.is_online_lora})")
                        except Exception as e:
                            print(f"     âš ï¸  Failed to load ensemble member for Task {past_task_id}: {e}")
                            continue
                    else:
                        print(f"     âš ï¸  Ensemble member file not found: {ensemble_member_lora_path}")
                        continue
                else:
                    # í˜„ì¬ ëª¨ë¸ ëª¨ë“œ: í˜„ì¬ íƒœìŠ¤í¬ì˜ LoRA ì‚¬ìš© (ìˆëŠ” ê²½ìš°)
                    current_model_lora_path = os.path.join(
                        ensemble_cache_dir, f"lora_task_{current_task_id}.pth"
                    )
                    if os.path.exists(current_model_lora_path):
                        try:
                            lora_weights = torch.load(current_model_lora_path, map_location=device)
                            learner = getattr(past_plan_workspace, "online_learner", None)
                            if (
                                past_plan_workspace.is_online_lora
                                and learner is not None
                                and hasattr(learner, "_apply_lora_weights")
                            ):
                                learner._apply_lora_weights(lora_weights)
                                loaded_file = os.path.basename(current_model_lora_path)
                                print(f"     ğŸ“¥ Loaded current model LoRA from {loaded_file}")
                        except Exception as e:
                            print(f"     âš ï¸  Failed to load current model LoRA: {e}")
                
                # ê°„ë‹¨í•œ ì„±ëŠ¥ ì¸¡ì • (1íšŒ í”Œë˜ë‹)
                logs = past_plan_workspace.perform_planning()
                
                # ì„±ëŠ¥ ì§€í‘œ ì¶”ì¶œ
                success_rate = logs.get('success_rate', 0.0)
                avg_loss = None
                
                # Loss ì¸¡ì • (ì˜¨ë¼ì¸ LoRAê°€ í™œì„±í™”ëœ ê²½ìš°)
                if (past_plan_workspace.is_online_lora and 
                    hasattr(past_plan_workspace.online_learner, 'last_loss') and
                    past_plan_workspace.online_learner.last_loss is not None):
                    avg_loss = past_plan_workspace.online_learner.last_loss
                
                # ë§ê° ì¸¡ì • ê²°ê³¼ ì €ì¥
                result = {
                    'past_task_id': past_task_id,
                    'current_task_id': current_task_id,
                    'env_config': past_env_config,
                    'mode': mode_label,
                    'success_rate': success_rate,
                    'avg_loss': avg_loss if avg_loss is not None else 0.0,
                    'loaded_file': loaded_file
                }
                forgetting_results.append(result)
                
                avg_loss_str = f"{avg_loss:.6f}" if avg_loss is not None else "N/A"
                print(f"     â†’ Task {past_task_id} ({mode_label}): Success Rate: {success_rate:.3f}, Loss: {avg_loss_str} [loaded: {loaded_file}]")
                
            except Exception as e:
                print(f"     â†’ Error measuring Task {past_task_id} ({mode_label}): {e}")
                # ì˜¤ë¥˜ ë°œìƒ ì‹œ ê¸°ë³¸ê°’ìœ¼ë¡œ ê¸°ë¡
                forgetting_results.append({
                    'past_task_id': past_task_id,
                    'current_task_id': current_task_id,
                    'env_config': past_env_config,
                    'mode': mode_label,
                    'success_rate': 0.0,
                    'avg_loss': 0.0,
                    'error': str(e),
                    'loaded_file': loaded_file if 'loaded_file' in locals() else "N/A"
                })
    
    return forgetting_results


class DummyWandbRun:
    def __init__(self):
        self.mode = "disabled"

    def log(self, *args, **kwargs):
        pass

    def watch(self, *args, **kwargs):
        pass

    def config(self, *args, **kwargs):
        pass

    def finish(self):
        pass

def planning_main(cfg_dict):
    output_dir = cfg_dict["saved_folder"]
    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    if cfg_dict["wandb_logging"]:
        wandb_run = wandb.init(
            project=f"continual_plan_{cfg_dict['planner']['name']}", config=cfg_dict
        )
        wandb.run.name = "{}".format(output_dir.split("plan_outputs/")[-1])
    else:
        wandb_run = None

    ckpt_base_path = cfg_dict["ckpt_base_path"]
    model_path = f"{ckpt_base_path}/outputs/{cfg_dict['model_name']}/"
    with open(os.path.join(model_path, "hydra.yaml"), "r") as f:
        model_cfg = OmegaConf.load(f)

    seed(cfg_dict["seed"])
    _, dset = hydra.utils.call(
        model_cfg.env.dataset,
        num_hist=model_cfg.num_hist,
        num_pred=model_cfg.num_pred,
        frameskip=model_cfg.frameskip,
    )
    dset = dset["valid"]

    # --- â–¼ 1. ëª¨ë¸ ë¡œë”©ì„ ë£¨í”„ ë°–ìœ¼ë¡œ ì´ë™ â–¼ ---
    # ëª¨ë¸ì€ ëª¨ë“  íƒœìŠ¤í¬ì— ê±¸ì³ ìƒíƒœê°€ ìœ ì§€ë˜ì–´ì•¼ í•˜ë¯€ë¡œ í•œ ë²ˆë§Œ ë¡œë“œí•©ë‹ˆë‹¤.
    num_action_repeat = model_cfg.num_action_repeat
    model_ckpt = (
        Path(model_path) / "checkpoints" / f"model_{cfg_dict['model_epoch']}.pth"
    )
    model = load_model(model_ckpt, model_cfg, num_action_repeat, device=device)

    # --- â–¼ 2. ì—°ì† í•™ìŠµì„ ìœ„í•œ íƒœìŠ¤í¬ ì •ì˜ â–¼ ---
    # 11ê°œì˜ ì„œë¡œ ë‹¤ë¥¸ í™˜ê²½ ì„¤ì •ì„ ì •ì˜í•©ë‹ˆë‹¤.
    task_configs = [
        {'shape': 'T', 'color': 'Yellow', 'background_color': 'White'},             # Task 8: ë¸”ë¡ ë…¸ë‘


        {'shape': 'T', 'color': 'Black', 'background_color': 'White'},              # Task 7: ë¸”ë¡ ê²€ì •
        {'shape': 'T', 'color': 'Black', 'background_color': 'Red'},              # Task 11: ë¸”ë¡ ê²€ì • + ë°°ê²½ ë¹¨ê°•

        # ë³µí•©ì  ë³€í™” (ë§¨ ë’¤ì— ë°°ì¹˜)
        {'shape': 'square', 'color': 'LightSlateGray', 'background_color': 'Black'}, # Task 9: ì •ì‚¬ê°í˜• + ë°°ê²½ ê²€ì •
        # ë¸”ë¡ ëª¨ì–‘ ë³€í™”
        {'shape': 'T', 'color': 'LightSlateGray', 'background_color': 'Black'},      # Task 5: ë°°ê²½ ê²€ì •

        {'shape': 'T',       'color': 'LightSlateGray', 'background_color': 'White'},  # Task 1: A (baseline)

        {'shape': 'L', 'color': 'Yellow', 'background_color': 'White'},             # Task 10: L + ë¸”ë¡ ë…¸ë‘

        # ë¸”ë¡ ìƒ‰ìƒ ë³€í™”
        {'shape': 'L',       'color': 'LightSlateGray', 'background_color': 'White'},  # Task 1: A (baseline)
    ]
    num_tasks = len(task_configs)
    overall_logs = []
    
    # íƒœìŠ¤í¬ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    task_summary = []
    current_task_start_time = None
    
    # íŒŒêµ­ì  ë§ê° ì¸¡ì •ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    past_task_envs = []  # ê³¼ê±° íƒœìŠ¤í¬ í™˜ê²½ë“¤ ì €ì¥
    forgetting_metrics = []  # ê° íƒœìŠ¤í¬ì—ì„œì˜ ê³¼ê±° ì„±ëŠ¥ ì¸¡ì • ê²°ê³¼
    
    # ì•™ìƒë¸” ìºì‹œ ë””ë ‰í† ë¦¬ ì„¤ì •
    ensemble_cfg = cfg_dict.get("lora", {}).get("ensemble_cfg", {})
    ensemble_cache_dir = os.path.abspath(ensemble_cfg.get("cache_dir", "./lora_cache"))

    # --- â–¼ 3. íƒœìŠ¤í¬ë¥¼ ìˆœíšŒí•˜ëŠ” ìµœìƒìœ„ ì œì–´ ë£¨í”„ ìƒì„± â–¼ ---
    for task_id, env_config in enumerate(task_configs):
        print(f"\n{'='*25} Starting Task {task_id + 1}/{num_tasks} {'='*25}")
        print(f"Environment Config: {env_config}")
        
        # íƒœìŠ¤í¬ ì‹œì‘ ì‹œê°„ ê¸°ë¡
        current_task_start_time = time.time()
        task_planning_steps = 0
        task_lora_stacks = 0
        
        # íƒœìŠ¤í¬ ë³€ê²½ ì‹œ LoRA ì ì¸µ íŠ¸ë¦¬ê±° (ì²« ë²ˆì§¸ íƒœìŠ¤í¬ê°€ ì•„ë‹Œ ê²½ìš°)
        if task_id > 0:
            print(f"\nTask transition detected: Task {task_id} â†’ Task {task_id + 1}")
            print(f"Environment change: {task_configs[task_id-1]} â†’ {env_config}")

        # 3-A. í˜„ì¬ íƒœìŠ¤í¬ì— ë§ëŠ” í™˜ê²½ì„ ë™ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
        #      (pusht_env.pyê°€ ì´ ì¸ìë“¤ì„ ë°›ë„ë¡ ìˆ˜ì •ë˜ì—ˆë‹¤ê³  ê°€ì •)
        env = SubprocVectorEnv(
            [
                lambda: gym.make(
                    model_cfg.env.name,
                    *model_cfg.env.args,
                    **model_cfg.env.kwargs,
                    **env_config  # ì—¬ê¸°ì— shape, color ë“± ì¸ì ì „ë‹¬
                )
                for _ in range(cfg_dict["n_evals"])
            ]
        )

        task_cfg_dict = cfg_dict.copy()
        task_cfg_dict["planner"]["logging_prefix"] = f"task_{task_id+1:02d}_plan"
        task_cfg_dict["planner"]["sub_planner"]["logging_prefix"] = f"task_{task_id+1:02d}_plan"

        # ğŸ”§ ì•™ìƒë¸” ë©¤ë²„ ë°±ì—… (íƒœìŠ¤í¬ ì „í™˜ ì‹œ)
        ensemble_backup = None
        if task_id > 0 and 'plan_workspace' in locals() and plan_workspace and hasattr(plan_workspace.online_learner, 'ensemble_manager'):
            ensemble_backup = {
                'members': dict(plan_workspace.online_learner.ensemble_manager.ensemble_members),
                'memory_usage': plan_workspace.online_learner.ensemble_manager.memory_usage,
                'access_frequency': dict(plan_workspace.online_learner.ensemble_manager.access_frequency) if hasattr(plan_workspace.online_learner.ensemble_manager, 'access_frequency') else {}
            }
            print(f"ğŸ”§ Backed up {len(ensemble_backup['members'])} ensemble members from previous task")

        # 3-B. ìƒˆë¡œìš´ PlanWorkspace ìƒì„± (ê° íƒœìŠ¤í¬ë§ˆë‹¤)
        print(f"ğŸ”§ Creating new PlanWorkspace for Task {task_id + 1}")
        plan_workspace = PlanWorkspace(
            cfg_dict=task_cfg_dict,
            wm=model,
            dset=dset,
            env=env,
            env_name=model_cfg.env.name,
            frameskip=model_cfg.frameskip,
            wandb_run=wandb_run,
            current_task_id=task_id + 1,
        )
        
        # ğŸ”§ ì•™ìƒë¸” ë©¤ë²„ ë³µì›
        if ensemble_backup and hasattr(plan_workspace.online_learner, 'ensemble_manager'):
            plan_workspace.online_learner.ensemble_manager.ensemble_members = ensemble_backup['members']
            plan_workspace.online_learner.ensemble_manager.memory_usage = ensemble_backup['memory_usage']
            if hasattr(plan_workspace.online_learner.ensemble_manager, 'access_frequency'):
                plan_workspace.online_learner.ensemble_manager.access_frequency = ensemble_backup['access_frequency']
            print(f"ğŸ”§ Restored {len(ensemble_backup['members'])} ensemble members to new PlanWorkspace")
            print(f"   - Memory usage: {ensemble_backup['memory_usage']:.2f}MB")
        else:
            print(f"ğŸ”§ Starting with fresh ensemble (no previous members)")
        
        # ğŸ”§ íƒœìŠ¤í¬ ì „í™˜ í”Œë˜ê·¸ ì„¤ì •: íƒœìŠ¤í¬ ì „í™˜ ì‹œ ì•™ìƒë¸” í‰ê°€ ì¡°ê±´ ë§Œì¡±
        try:
            if (task_id >= 0 and hasattr(plan_workspace, 'online_learner') and
                hasattr(plan_workspace.online_learner, 'check_task_change')):
                changed = plan_workspace.online_learner.check_task_change(task_id + 1)
                print(f"ğŸ”„ Task change flag updated via OnlineLora.check_task_change: {changed}")
                # PlanWorkspaceì—ë„ í˜„ì¬ íƒœìŠ¤í¬ ID ë°˜ì˜ (perform_planning ë“±ì—ì„œ ì‚¬ìš©)
                plan_workspace.current_task_id = getattr(
                    plan_workspace.online_learner, 'current_task_id', task_id + 1
                )
        except Exception as e:
            print(f"âš ï¸  Failed to set task change flag: {e}")

        # ğŸ”§ ì•™ìƒë¸” ì „ìš© ëª¨ë“œì—ì„œ ìµœì´ˆ ì ì¸µ ê°•ì œ ìˆ˜í–‰ (task_based_stacking=falseì—¬ë„ 1íšŒ ì ì¸µ)
        try:
            if (hasattr(plan_workspace, 'online_learner') and
                hasattr(plan_workspace.online_learner, 'hybrid_enabled') and
                plan_workspace.online_learner.hybrid_enabled and
                hasattr(plan_workspace.online_learner, 'task_based_stacking') and
                not plan_workspace.online_learner.task_based_stacking and
                hasattr(plan_workspace.online_learner, 'ensemble_manager')):
                # cfg í”Œë˜ê·¸ í™•ì¸
                hybrid_cfg = cfg_dict.get("lora", {}).get("hybrid_stacking", {})
                force_initial = hybrid_cfg.get("force_initial_stacking", True)
                # í˜„ì¬ ì•™ìƒë¸” ë©¤ë²„ ìˆ˜ í™•ì¸ ë° ìµœì´ˆ ì ì¸µ ì—¬ë¶€ ê²°ì •
                num_members = len(plan_workspace.online_learner.ensemble_manager.ensemble_members)
                if force_initial and num_members == 0 and getattr(plan_workspace.online_learner, 'stacks_in_current_task', 0) == 0:
                    print(f"\nğŸ¯ Forcing initial LoRA stacking for Task {task_id + 1} (ensemble_initial)")
                    success = plan_workspace.online_learner.trigger_task_based_stacking(task_id + 1, "ensemble_initial")
                    if success:
                        task_lora_stacks += 1
                        print(f"âœ… Initial ensemble-based LoRA stacking completed. Total stacks in task: {task_lora_stacks}")
                    else:
                        print(f"âš ï¸  Initial ensemble-based LoRA stacking skipped or failed.")
        except Exception as e:
            print(f"âš ï¸  Failed to perform forced initial stacking: {e}")

        # LoRA ì ì¸µ ì½œë°± ì„¤ì • (íƒœìŠ¤í¬ ì¶”ì  + ì•™ìƒë¸” ì €ì¥ ë‘˜ ë‹¤ ìˆ˜í–‰)
        if plan_workspace.is_online_lora and hasattr(plan_workspace.online_learner, 'base_online_lora'):
            old_cb = getattr(plan_workspace.online_learner.base_online_lora, 'on_lora_stack_callback', None)

            def on_lora_stack(steps, loss, task_id, stack_type, reason):
                # 1) ê¸°ì¡´ ì½œë°± í˜¸ì¶œ (ì•™ìƒë¸” ë©¤ë²„ ì €ì¥ ë“±)
                if callable(old_cb):
                    try:
                        old_cb(steps, loss, task_id, stack_type, reason)
                    except Exception as e:
                        print(f"âš ï¸  Error in chained base callback: {e}")

                # 2) íƒœìŠ¤í¬ë³„ ìŠ¤íƒ ì¹´ìš´íŠ¸ ì—…ë°ì´íŠ¸
                nonlocal task_lora_stacks
                task_lora_stacks += 1
                loss_str = f"{loss:.6f}" if loss is not None else "N/A"
                # ì„ íƒëœ ì•™ìƒë¸” ë©¤ë²„ ID ì¶”ì  (ìˆìœ¼ë©´ í‘œì‹œ)
                selected_member_id = None
                try:
                    if hasattr(plan_workspace.online_learner, 'last_selected_member_task_id'):
                        selected_member_id = getattr(plan_workspace.online_learner, 'last_selected_member_task_id')
                except Exception:
                    selected_member_id = None
                if selected_member_id is not None:
                    print(f"ğŸ”¥ LoRA Stack #{task_lora_stacks} at step {steps} (task {task_id}, type {stack_type}, reason {reason}, on_member {selected_member_id}) loss {loss_str}")
                else:
                    print(f"ğŸ”¥ LoRA Stack #{task_lora_stacks} at step {steps} (task {task_id}, type {stack_type}, reason {reason}) loss {loss_str}")
                # ìµœê·¼ ìŠ¤íƒ íˆìŠ¤í† ë¦¬ì— ì„ íƒ ë©¤ë²„ ID ì£¼ì„ ì¶”ê°€
                try:
                    if hasattr(plan_workspace.online_learner, 'stack_history') and isinstance(plan_workspace.online_learner.stack_history, list):
                        if len(plan_workspace.online_learner.stack_history) > 0 and isinstance(plan_workspace.online_learner.stack_history[-1], dict):
                            plan_workspace.online_learner.stack_history[-1]['selected_member_task_id'] = selected_member_id
                except Exception:
                    pass

            # Base OnlineLoraê°€ í˜¸ì¶œí•˜ëŠ” ì½œë°±ì„ ë˜í•‘í•˜ì—¬ êµì²´
            plan_workspace.online_learner.base_online_lora.on_lora_stack_callback = on_lora_stack
        
        # íƒœìŠ¤í¬ ë³€ê²½ ì‹œ LoRA ì ì¸µ íŠ¸ë¦¬ê±° (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì—ì„œë§Œ)
        if (hasattr(plan_workspace.online_learner, 'trigger_task_based_stacking') and
            hasattr(plan_workspace.online_learner, 'hybrid_enabled') and
            plan_workspace.online_learner.hybrid_enabled and
            hasattr(plan_workspace.online_learner, 'task_based_stacking') and
            plan_workspace.online_learner.task_based_stacking):
            print(f"\nğŸ¯ Triggering task-based LoRA stacking for Task {task_id + 1} (adds new LoRA layers to model)")
            success = plan_workspace.online_learner.trigger_task_based_stacking(task_id + 1, "task_change")
            if success:
                task_lora_stacks += 1
                print(f"âœ… Task-based LoRA stacking successful. Total stacks in task: {task_lora_stacks}")
            else:
                print(f"Task-based LoRA stacking skipped or failed.")
        else:
            # ğŸ”§ ì•™ìƒë¸” LoRA ì‚¬ìš© ì—¬ë¶€ì— ë”°ë¥¸ ë©”ì‹œì§€ ì¶œë ¥
            if hasattr(plan_workspace.online_learner, 'ensemble_manager'):
                print(f"Task-based LoRA stacking disabled (hybrid_enabled: {getattr(plan_workspace.online_learner, 'hybrid_enabled', False)}, task_based_stacking: {getattr(plan_workspace.online_learner, 'task_based_stacking', False)})")
            else:
                print(f"âš ï¸  Standard OnlineLora mode - using default LoRA stacking behavior")

        # 3-C. Loss ê¸°ë°˜ íƒœìŠ¤í¬ ì „í™˜ ë¡œì§
        task_switch_config = cfg_dict.get("lora", {}).get("task_switch", {})
        use_loss_based_switching = task_switch_config.get("enabled", False)
        
        if use_loss_based_switching:
            print("ğŸ“Š Using loss-based task transition logic (determines when to move to next task)")
            loss_threshold = task_switch_config.get("loss_threshold", )
            max_planning = task_switch_config.get("max_planning_per_task", )
            min_planning = task_switch_config.get("min_planning_per_task", )
            
            # Loss ìœˆë„ìš° ì´ˆê¸°í™” (íƒœìŠ¤í¬ ì „í™˜ìš©)
            loss_window_size = task_switch_config.get("loss_window_size", )
            loss_window = deque(maxlen=loss_window_size)
            planning_count = 0
            
            while planning_count < max_planning:
                planning_count += 1
                print(f"\n--- Task {task_id + 1}, Planning Step {planning_count} ---")
                
                # í”Œë˜ë‹ ìˆ˜í–‰
                logs = plan_workspace.perform_planning()
                task_planning_steps += 1
                
                # Loss ê°’ ì¶”ì¶œ (evaluatorì—ì„œ ì˜¨ë¼ì¸ í•™ìŠµì´ ìˆ˜í–‰ëœ ê²½ìš°)
                current_loss = None
                if plan_workspace.is_online_lora and hasattr(plan_workspace.online_learner, 'last_loss'):
                    current_loss = plan_workspace.online_learner.last_loss
                    if current_loss is not None:
                        print(f"Current loss: {current_loss:.6f} (threshold: {loss_threshold})")
                    else:
                        print(f"Current loss: N/A (threshold: {loss_threshold})")
                
                # ë¡œê·¸ ì €ì¥
                log_entry_with_task = {f"task_{task_id+1}/{k}": v for k, v in logs.items()}
                if current_loss is not None:
                    log_entry_with_task[f"task_{task_id+1}/current_loss"] = current_loss
                overall_logs.append(log_entry_with_task)
                
                if wandb_run:
                    wandb_run.log(log_entry_with_task)
                
                # Loss ê¸°ë°˜ ì „í™˜ ì¡°ê±´ í™•ì¸
                if current_loss is not None:
                    loss_window.append(current_loss)
                    
                    # ìµœì†Œ í”Œë˜ë‹ íšŸìˆ˜ í™•ì¸ í›„ Loss ì¡°ê±´ ì²´í¬
                    if (planning_count >= min_planning and 
                        len(loss_window) >= loss_window_size and
                        all(loss < loss_threshold for loss in list(loss_window)[-1:])):  # ìµœê·¼ 3ê°œê°€ ëª¨ë‘ ì„ê³„ê°’ ì´í•˜
                        print(f"âœ“ Loss converged below threshold {loss_threshold}. Moving to next task.")
                        break
                else:
                    # Loss ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° (ì˜¨ë¼ì¸ í•™ìŠµì´ ë¹„í™œì„±í™”ëœ ê²½ìš°) ê¸°ë³¸ íšŸìˆ˜ë¡œ ì§„í–‰
                    if planning_count >= min_planning:
                        print("No loss information available. Using default planning count.")
                        break
        else:
            # ê¸°ì¡´ ë°©ì‹: ê³ ì • íšŸìˆ˜
            num_planning_per_task = 10
            for i in range(num_planning_per_task):
                print(f"\n--- Task {task_id + 1}, Planning Step {i+1}/{num_planning_per_task} ---")
                logs = plan_workspace.perform_planning()
                task_planning_steps += 1
                log_entry_with_task = {f"task_{task_id+1}/{k}": v for k, v in logs.items()}
                overall_logs.append(log_entry_with_task)
                if wandb_run:
                    wandb_run.log(log_entry_with_task)

        # --- â–¼ 4. íƒœìŠ¤í¬ ì „í™˜ ì‹œ LoRA ì ì¸µ(Stacking) ë¡œì§ í˜¸ì¶œ â–¼ ---
        # Online-LoRA ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì–´ ìˆê³  ë§ˆì§€ë§‰ íƒœìŠ¤í¬ê°€ ì•„ë‹Œ ê²½ìš°ì—ë§Œ ì‹¤í–‰í•©ë‹ˆë‹¤.
        if (plan_workspace.is_online_lora and task_id < num_tasks - 1):
            print(f"\n--- End of Task {task_id + 1}. Adding new LoRA stack for the next task. ---")
            
            # ëª¨ë¸ì— LoRA ìŠ¤íƒ ì¶”ê°€ ë° ì˜µí‹°ë§ˆì´ì € ë¦¬ì…‹ì„ ìš”ì²­í•©ë‹ˆë‹¤.
            # ì´ ê¸°ëŠ¥ë“¤ì€ models/lora.py ì™€ planning/online.pyì— êµ¬í˜„ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.
            if hasattr(model.predictor, 'add_new_lora_stack'):
                model.predictor.add_new_lora_stack()
            if hasattr(plan_workspace.online_learner, 'reset_optimizer'):
                plan_workspace.online_learner.reset_optimizer()

        # --- íƒœìŠ¤í¬ ì¢…ë£Œ ì‹œ ì•™ìƒë¸”ì— ìµœì¢… LoRA ë©¤ë²„ ì €ì¥ (ìµœì¢… í•™ìŠµ ìƒíƒœ ë°˜ì˜) ---
        try:
            if (plan_workspace.is_online_lora and hasattr(plan_workspace, 'online_learner') and
                hasattr(plan_workspace.online_learner, 'save_current_lora_member') and
                getattr(plan_workspace.online_learner, 'save_on_task_end', True)):
                
                # ğŸ”§ íƒœìŠ¤í¬ ì™„ë£Œ ì‹œ wì— wnew ëˆ„ì  (ëˆ„ë½ëœ ë¶€ë¶„!)
                if hasattr(plan_workspace.online_learner, 'update_and_reset_lora_parameters'):
                    plan_workspace.online_learner.update_and_reset_lora_parameters()
                    print(f"ğŸ”„ Updated w with wnew for Task {task_id + 1}")
                
                current_task_for_save = getattr(plan_workspace.online_learner, 'current_task_id', task_id + 1)
                print(f"ğŸ’¾ Saving finalized LoRA member for Task {current_task_for_save} at task end...")
                plan_workspace.online_learner.save_current_lora_member(task_id=current_task_for_save, reason="task_end")
                
                # ğŸ”§ ë””ìŠ¤í¬ì— ì €ì¥ (granular_plan.pyì™€ ë™ì¼í•œ ë¡œì§)
                if (hasattr(plan_workspace.online_learner, 'ensemble_manager') and
                    plan_workspace.online_learner.ensemble_manager is not None):
                    manager = plan_workspace.online_learner.ensemble_manager
                    if current_task_for_save in manager.ensemble_members:
                        try:
                            manager._save_member_to_disk(current_task_for_save, manager.ensemble_members[current_task_for_save])
                            print(f"ğŸ’¾ Saved LoRA member for Task {current_task_for_save} to disk: {os.path.join(manager.cache_dir, f'lora_task_{current_task_for_save}.pth')}")
                        except Exception as e:
                            print(f"âš ï¸  Failed to save LoRA member to disk for Task {current_task_for_save}: {e}")
        except Exception as e:
            print(f"âš ï¸  Failed to save finalized LoRA member at task end: {e}")

        # íƒœìŠ¤í¬ ì™„ë£Œ ìš”ì•½ ìƒì„±
        task_duration = time.time() - current_task_start_time
        
        # ì ì¸µ íˆìŠ¤í† ë¦¬ ìˆ˜ì§‘ (í•˜ì´ë¸Œë¦¬ë“œ ëª¨ë“œì—ì„œ)
        stack_history = []
        if (plan_workspace.is_online_lora and 
            hasattr(plan_workspace.online_learner, 'stack_history')):
            stack_history = plan_workspace.online_learner.stack_history.copy()
        
        task_summary.append({
            'task_id': task_id + 1,
            'env_config': env_config,
            'planning_steps': task_planning_steps,
            'lora_stacks': task_lora_stacks,
            'duration_seconds': task_duration,
            'final_loss': current_loss if 'current_loss' in locals() else None,
            'stack_history': stack_history
        })
        
        print(f"\nğŸ“Š Task {task_id + 1} Summary:")
        print(f"   - Environment: {env_config}")
        print(f"   - Planning Steps: {task_planning_steps}")
        print(f"   - LoRA Stacks: {task_lora_stacks}")
        print(f"   - Duration: {task_duration:.2f}s")
        if 'current_loss' in locals() and current_loss is not None:
            print(f"   - Final Loss: {current_loss:.6f}")

        # í˜„ì¬ íƒœìŠ¤í¬ í™˜ê²½ì„ ê³¼ê±° íƒœìŠ¤í¬ ë¦¬ìŠ¤íŠ¸ì— ì €ì¥
        past_task_envs.append({
            'task_id': task_id + 1,
            'env_config': env_config,
            'env': env  # í™˜ê²½ ê°ì²´ ì €ì¥ (ì„±ëŠ¥ ì¸¡ì •ìš©)
        })
        
        # íŒŒêµ­ì  ë§ê° ì¸¡ì •: ê³¼ê±° íƒœìŠ¤í¬ë“¤ì— ëŒ€í•œ í˜„ì¬ ëª¨ë¸ ì„±ëŠ¥ í‰ê°€
        if len(past_task_envs) > 1:  # ì²« ë²ˆì§¸ íƒœìŠ¤í¬ ì´í›„ë¶€í„° ì¸¡ì •
            print(f"\nğŸ§  Measuring Catastrophic Forgetting...")
            forgetting_results = measure_forgetting_on_past_tasks(
                model, past_task_envs, plan_workspace, task_id, ensemble_cache_dir
            )
            forgetting_metrics.append(forgetting_results)
            
            # ë§ê° ì¸¡ì • ê²°ê³¼ ì¶œë ¥
            print(f"ğŸ“Š Forgetting Analysis for Task {task_id + 1}:")
            for result in forgetting_results:
                avg_loss_str = f"{result['avg_loss']:.6f}" if result['avg_loss'] is not None else "N/A"
                mode_label = result.get('mode', 'unknown')
                loaded_file = result.get('loaded_file', 'N/A')
                print(f"   - Task {result['past_task_id']} ({mode_label}): Success Rate {result['success_rate']:.3f}, "
                      f"Loss {avg_loss_str} [loaded: {loaded_file}]")

        # ë¦¬ì†ŒìŠ¤ ì •ë¦¬ë¥¼ ìœ„í•´ í˜„ì¬ íƒœìŠ¤í¬ì˜ í™˜ê²½ì„ ë‹«ìŠµë‹ˆë‹¤.
        # env.close()  # ê³¼ê±° íƒœìŠ¤í¬ ì¸¡ì •ì„ ìœ„í•´ í™˜ê²½ì„ ë‹«ì§€ ì•ŠìŒ

    print(f"\n{'='*25} Continual Learning Experiment Finished {'='*25}")
    
    # ì „ì²´ ì‹¤í—˜ ìš”ì•½ ë¦¬í¬íŠ¸ ì¶œë ¥
    print(f"\n{'='*60}")
    print(f"ğŸ“ˆ CONTINUAL LEARNING EXPERIMENT SUMMARY")
    print(f"{'='*60}")
    print(f"Total Tasks: {len(task_summary)}")
    print(f"Total Duration: {sum(task['duration_seconds'] for task in task_summary):.2f}s")
    total_lora_stacks = sum(task['lora_stacks'] for task in task_summary)
    # ì•™ìƒë¸” ë©¤ë²„ ìˆ˜ëŠ” ì´ ìŠ¤íƒ ìˆ˜ì™€ ì¼ì¹˜í•´ì•¼ í•¨ (íƒœìŠ¤í¬ ì¢…ë£Œ ì‹œë§ˆë‹¤ ë©¤ë²„ ì €ì¥)
    ensemble_member_count = total_lora_stacks
    print(f"Total LoRA Stacks: {total_lora_stacks} (Ensemble Members: {ensemble_member_count})")
    print(f"Total Planning Steps: {sum(task['planning_steps'] for task in task_summary)}")
    
    print(f"\nğŸ“‹ TASK-BY-TASK BREAKDOWN:")
    print(f"{'Task':<6} {'Environment':<40} {'Steps':<6} {'LoRA':<5} {'Duration':<8} {'Final Loss':<10}")
    print(f"{'-'*80}")
    
    for task in task_summary:
        env_str = f"{task['env_config'].get('shape', 'N/A')}-{task['env_config'].get('color', 'N/A')}-{task['env_config'].get('background_color', 'N/A')}"
        final_loss_str = f"{task['final_loss']:.6f}" if task['final_loss'] is not None else "N/A"
        print(f"{task['task_id']:<6} {env_str:<40} {task['planning_steps']:<6} {task['lora_stacks']:<5} {task['duration_seconds']:.2f}s{'':<4} {final_loss_str:<10}")
    
    print(f"\nğŸ¯ KEY INSIGHTS:")
    fastest_task = min(task_summary, key=lambda x: x['duration_seconds'])
    slowest_task = max(task_summary, key=lambda x: x['duration_seconds'])
    most_stacks = max(task_summary, key=lambda x: x['lora_stacks'])
    most_steps = max(task_summary, key=lambda x: x['planning_steps'])
    
    print(f"   - Fastest Task: Task {fastest_task['task_id']} ({fastest_task['duration_seconds']:.2f}s)")
    print(f"   - Slowest Task: Task {slowest_task['task_id']} ({slowest_task['duration_seconds']:.2f}s)")
    print(f"   - Most LoRA Stacks: Task {most_stacks['task_id']} ({most_stacks['lora_stacks']} stacks)")
    print(f"   - Most Planning Steps: Task {most_steps['task_id']} ({most_steps['planning_steps']} steps)")
    
    avg_steps = sum(task['planning_steps'] for task in task_summary) / len(task_summary)
    avg_stacks = sum(task['lora_stacks'] for task in task_summary) / len(task_summary)
    print(f"   - Average Planning Steps: {avg_steps:.1f}")
    print(f"   - Average LoRA Stacks: {avg_stacks:.1f}")
    
    # í•˜ì´ë¸Œë¦¬ë“œ ì ì¸µ ë¶„ì„
    print(f"\nğŸ¯ HYBRID STACKING ANALYSIS")
    print(f"{'='*60}")
    
    total_task_based_stacks = 0
    total_loss_based_stacks = 0
    
    for task in task_summary:
        if 'stack_history' in task and task['stack_history']:
            task_based_count = sum(1 for stack in task['stack_history'] if stack['type'] == 'task_based')
            loss_based_count = sum(1 for stack in task['stack_history'] if stack['type'] == 'loss_based')
            total_task_based_stacks += task_based_count
            total_loss_based_stacks += loss_based_count
            
            print(f"Task {task['task_id']}: {task_based_count} task-based, {loss_based_count} loss-based stacks")
    
    print(f"\nğŸ“Š STACKING TYPE SUMMARY:")
    print(f"   - Total Task-based Stacks: {total_task_based_stacks}")
    print(f"   - Total Loss-based Stacks: {total_loss_based_stacks}")
    print(f"   - Total Stacks: {total_task_based_stacks + total_loss_based_stacks}")
    if total_task_based_stacks + total_loss_based_stacks > 0:
        task_based_ratio = total_task_based_stacks / (total_task_based_stacks + total_loss_based_stacks) * 100
        print(f"   - Task-based Ratio: {task_based_ratio:.1f}%")
    
    print(f"{'='*60}")
    
    # íŒŒêµ­ì  ë§ê° ë¶„ì„ ë¦¬í¬íŠ¸
    if forgetting_metrics:
        print(f"\nğŸ§  CATASTROPHIC FORGETTING ANALYSIS")
        print(f"{'='*60}")
        
        # ì „ì²´ ë§ê° í†µê³„
        all_losses = []
        task_loss_summary = {}
        
        for task_idx, forgetting_results in enumerate(forgetting_metrics):
            current_task_id = task_idx + 2  # ì²« ë²ˆì§¸ íƒœìŠ¤í¬ ì´í›„ë¶€í„° ì¸¡ì •
            print(f"\nğŸ“Š Forgetting Analysis after Task {current_task_id}:")
            
            for result in forgetting_results:
                past_task_id = result['past_task_id']
                avg_loss = result['avg_loss']
                mode_label = result.get('mode', 'unknown')
                loaded_file = result.get('loaded_file', 'N/A')
                if avg_loss is not None and avg_loss > 0:
                    all_losses.append(avg_loss)
                
                # ê° ê³¼ê±° íƒœìŠ¤í¬ë³„ í‰ê·  loss ê³„ì‚° (ëª¨ë“œë³„ë¡œ êµ¬ë¶„)
                key = f"{past_task_id}_{mode_label}"
                if key not in task_loss_summary:
                    task_loss_summary[key] = []
                task_loss_summary[key].append(avg_loss)
                
                env_str = f"{result['env_config'].get('shape', 'N/A')}-{result['env_config'].get('color', 'N/A')}-{result['env_config'].get('background_color', 'N/A')}"
                loss_str = f"{avg_loss:.6f}" if avg_loss is not None else "N/A"
                print(f"   Task {past_task_id} ({mode_label}, {env_str}): Loss {loss_str} [loaded: {loaded_file}]")
        
        # ì „ì²´ ë§ê° ë¶„ì„
        if all_losses:
            avg_loss = sum(all_losses) / len(all_losses)
            min_loss = min(all_losses)
            max_loss = max(all_losses)
            
            print(f"\nğŸ¯ OVERALL FORGETTING STATISTICS:")
            print(f"   - Average Loss: {avg_loss:.6f}")
            print(f"   - Minimum Loss: {min_loss:.6f}")
            print(f"   - Maximum Loss: {max_loss:.6f}")
            print(f"   - Total Measurements: {len(all_losses)}")
            
            # ê° íƒœìŠ¤í¬ë³„ í‰ê·  loss (ëª¨ë“œë³„ë¡œ êµ¬ë¶„)
            print(f"\nğŸ“ˆ TASK-SPECIFIC LOSS SUMMARY:")
            # íƒœìŠ¤í¬ IDë³„ë¡œ ê·¸ë£¹í™”
            task_grouped = {}
            for key, losses in task_loss_summary.items():
                parts = key.split('_', 1)
                if len(parts) == 2:
                    task_id, mode = parts
                    if task_id not in task_grouped:
                        task_grouped[task_id] = {}
                    task_grouped[task_id][mode] = losses
            
            for past_task_id in sorted(task_grouped.keys(), key=int):
                for mode in sorted(task_grouped[past_task_id].keys()):
                    valid_losses = [loss for loss in task_grouped[past_task_id][mode] if loss is not None and loss > 0]
                    if valid_losses:
                        avg_loss_for_task = sum(valid_losses) / len(valid_losses)
                        print(f"   Task {past_task_id} ({mode}): {avg_loss_for_task:.6f} average loss")
            
            # ë§ê° ì‹¬ê°ë„ í‰ê°€ (loss ê¸°ë°˜)
            print(f"\nâš ï¸  FORGETTING SEVERITY ASSESSMENT:")
            if avg_loss <= 0.1:
                print(f"   ğŸŸ¢ LOW FORGETTING: (Loss {avg_loss:.6f})")
            elif avg_loss <= 0.2:
                print(f"   ğŸŸ¡ MODERATE FORGETTING: (Loss {avg_loss:.6f})")
            elif avg_loss <= 0.3:
                print(f"   ğŸŸ  HIGH FORGETTING: (Loss {avg_loss:.6f})")
            else:
                print(f"   ğŸ”´ SEVERE FORGETTING: (Loss {avg_loss:.6f})")
        print(f"{'='*60}")
    
    # í™˜ê²½ ì •ë¦¬
    for past_task in past_task_envs:
        if 'env' in past_task and past_task['env']:
            past_task['env'].close()

    return overall_logs


@hydra.main(config_path="conf", config_name="plan")
def main(cfg: OmegaConf):
    with open_dict(cfg):
        cfg["saved_folder"] = os.getcwd()
        log.info(f"Planning result saved dir: {cfg['saved_folder']}")
    cfg_dict = cfg_to_dict(cfg)
    cfg_dict["wandb_logging"] = True
    planning_main(cfg_dict)


if __name__ == "__main__":
    main()