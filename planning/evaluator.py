import os
import torch
import imageio
import numpy as np
from einops import rearrange, repeat
from utils import (
    cfg_to_dict,
    seed,
    slice_trajdict_with_t,
    aggregate_dct,
    move_to_device,
    concat_trajdict,
)
from torchvision import utils


class PlanEvaluator:  # evaluator for planning
    def __init__(
        self,
        obs_0,
        obs_g,
        state_0,
        state_g,
        env,
        wm,
        frameskip,
        seed,
        preprocessor,
        n_plot_samples,
        # lora ê´€ë ¨ ì¸ì ì¶”ê°€ (ê¸°ë³¸ê°’ìœ¼ë¡œ í˜¸í™˜ì„± ìœ ì§€)
        is_lora_enabled=False,
        is_online_lora=False,
        workspace=None,
    ):
        self.obs_0 = obs_0
        self.obs_g = obs_g
        self.state_0 = state_0
        self.state_g = state_g
        self.env = env
        self.wm = wm
        self.frameskip = frameskip
        self.seed = seed
        self.preprocessor = preprocessor
        self.n_plot_samples = n_plot_samples
        self.device = next(wm.parameters()).device

        self.plot_full = False  # plot all frames or frames after frameskip
        self._initial_cd_measured = False  # Flag to track if initial CD has been measured

        # lora í•™ìŠµì„ ìœ„í•œ ê´€ë ¨ ì„¤ì • ì´ˆê¸°í™”
        self.workspace = workspace  # PlanWorkspace ì¸ìŠ¤í„´ìŠ¤ ì°¸ì¡°
        self.is_lora_enabled = self.workspace.is_lora_enabled if self.workspace is not None else is_lora_enabled
        self.is_online_lora = self.workspace.is_online_lora if self.workspace is not None else is_online_lora
        # workspaceë¡œë¶€í„° optimizerì™€ loss_fn ê°€ì ¸ì˜¤ê¸°
        if self.is_lora_enabled:
            # ì•™ìƒë¸” ì‚¬ìš© ì—¬ë¶€ í™•ì¸ (online_learnerê°€ ìˆê³  ensemble_managerê°€ ìˆìœ¼ë©´ True)
            try:
                lora_ensemble_enabled = (
                    hasattr(self.workspace, 'online_learner') and
                    hasattr(self.workspace.online_learner, 'ensemble_manager') and
                    self.workspace.online_learner.ensemble_manager is not None
                )
            except Exception:
                lora_ensemble_enabled = False
            print(
                f"LoRA enabled: {self.is_lora_enabled}, Online LoRA: {self.is_online_lora}, LoRA Ensemble: {lora_ensemble_enabled}"
            )
            
            # LoRA íŒŒë¼ë¯¸í„° ìƒíƒœ í™•ì¸
            if hasattr(self.wm, 'predictor') and hasattr(self.wm.predictor, 'lora_vit'):
                total_params = sum(p.numel() for p in self.wm.parameters())
                trainable_params = sum(p.numel() for p in self.wm.parameters() if p.requires_grad)
                print(f"Model parameters - Total: {total_params:,}, Trainable: {trainable_params:,} ({(trainable_params / total_params) * 100 if total_params > 0 else 0:.4f}%)")
                
            else:
                print("Warning: LoRA predictor not found in world model")
            
        #     # LoRA íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ ì¶”ì ì„ ìœ„í•œ ë³€ìˆ˜ ì´ˆê¸°í™”
        #     self._prev_lora_params = None

    def assign_init_cond(self, obs_0, state_0):
        self.obs_0 = obs_0
        self.state_0 = state_0

    def assign_goal_cond(self, obs_g, state_g):
        self.obs_g = obs_g
        self.state_g = state_g

    def get_init_cond(self):
        return self.obs_0, self.state_0

    def _get_trajdict_last(self, dct, length):
        new_dct = {}
        for key, value in dct.items():
            new_dct[key] = self._get_traj_last(value, length)
        return new_dct

    def _get_traj_last(self, traj_data, length):
        last_index = np.where(length == np.inf, -1, length - 1)
        last_index = last_index.astype(int)
        
        # ğŸ”§ last_indexë¥¼ ì‹¤ì œ trajectory ê¸¸ì´ë¡œ í´ë¨í•‘
        if isinstance(traj_data, torch.Tensor):
            max_index = traj_data.shape[1] - 1
            last_index = np.clip(last_index, 0, max_index)
            traj_data = traj_data[np.arange(traj_data.shape[0]), last_index].unsqueeze(
                1
            )
        else:
            max_index = traj_data.shape[1] - 1
            last_index = np.clip(last_index, 0, max_index)
            traj_data = np.expand_dims(
                traj_data[np.arange(traj_data.shape[0]), last_index], axis=1
            )
        return traj_data

    def _mask_traj(self, data, length):
        """
        Zero out everything after specified indices for each trajectory in the tensor.
        data: tensor
        """
        result = data.clone()  # Clone to preserve the original tensor
        for i in range(data.shape[0]):
            if length[i] != np.inf:
                result[i, int(length[i]) :] = 0
        return result

    def eval_actions(
        self,
        actions,
        action_len=None,
        filename="output",
        save_video=False,
        learning_enabled=True,
    ):
        """
        actions: detached torch tensors on cuda
        Returns
            metrics, and feedback from env
        """
        n_evals = actions.shape[0]
        if action_len is None:
            action_len = np.full(n_evals, np.inf)
        # rollout in wm
        trans_obs_0 = move_to_device(
            self.preprocessor.transform_obs(self.obs_0), self.device
        )
        trans_obs_g = move_to_device(
            self.preprocessor.transform_obs(self.obs_g), self.device
        )
        with torch.no_grad():
            i_z_obses, _ = self.wm.rollout(
                obs_0=trans_obs_0,
                act=actions,
            )
        i_final_z_obs = self._get_trajdict_last(i_z_obses, action_len + 1)

        # rollout in env
        exec_actions = rearrange(
            actions.cpu(), "b t (f d) -> b (t f) d", f=self.frameskip
        )
        exec_actions = self.preprocessor.denormalize_actions(exec_actions).numpy()

        # ğŸ”§ final output ê³„ì‚° ì‹œ ì¬ì„¼í„°ë§ ê°•ì œ ì ìš© (íƒœìŠ¤í¬ë³„ íŒŒì¼ëª… í¬í•¨, failure í¬í•¨)
        if filename.startswith("output_final"):
            print("[EVAL] Setting force_recenter flag for final output - will recenter in reset() and set_states()")
            # ğŸ”§ force_recenter_for_next_rollout()ë¥¼ í˜¸ì¶œí•˜ì—¬ ì¹´ìš´í„° ë¦¬ì…‹ + _force_recenter_unlimited í”Œë˜ê·¸ ì„¤ì •
            self.force_recenter_for_next_rollout()

        # Set flag before rollout to measure Initial CD after first set_states in prepare()
        if not hasattr(self, '_initial_cd_measured') or not self._initial_cd_measured:
            # Set flags in environment to measure CD after set_states
            if hasattr(self.env, 'envs') and len(self.env.envs) > 0:
                # For vectorized environment, set flag in first worker
                first_env = self.env.envs[0].env
                if hasattr(first_env, 'set_measure_initial_cd'):
                    first_env.set_measure_initial_cd(True, self.state_g)
            elif hasattr(self.env, 'env'):
                # For single environment
                if hasattr(self.env.env, 'set_measure_initial_cd'):
                    self.env.env.set_measure_initial_cd(True, self.state_g)
            self._initial_cd_measured = True

        # ğŸ”§ final outputì¸ ê²½ìš° í”Œë˜ê·¸ë¥¼ rollout ì§ì „ì— ë‹¤ì‹œ í™•ì¸ ë° ì„¤ì • (íƒœìŠ¤í¬ë³„ íŒŒì¼ëª… í¬í•¨)
        force_recenter_for_rollout = False
        if filename.startswith("output_final"):
            print("[EVAL] Re-checking force_recenter flag before rollout")
            force_recenter_for_rollout = True
            if hasattr(self.env, 'envs') and len(self.env.envs) > 0:
                for i, env_wrapper in enumerate(self.env.envs):
                    # FlexEnvWrapperì— ì§ì ‘ ì„¤ì •
                    env_wrapper._force_recenter_after_set_states = True
                    # í™•ì¸ìš© ë¡œê·¸
                    flag_after = getattr(env_wrapper, '_force_recenter_after_set_states', None)
                    print(f"[EVAL] Re-set _force_recenter_after_set_states=True for env[{i}], verified={flag_after}")
        
        # ğŸ”§ final outputì¸ ê²½ìš° force_recenter íŒŒë¼ë¯¸í„°ë¥¼ ì§ì ‘ ì „ë‹¬
        e_obses, e_states = self.env.rollout(
            self.seed,
            self.state_0,
            exec_actions,
            force_recenter=force_recenter_for_rollout,
        )
        # # ======================================================= #
        # LoRA í•™ìŠµì´ í™œì„±í™”ëœ ê²½ìš°, í•™ìŠµ ì±…ì„ì„ OnlineLora ê°ì²´ì— ìœ„ì„í•©ë‹ˆë‹¤.
        if (
            learning_enabled
            and self.is_lora_enabled
            and self.workspace.online_learner is not None
        ):
            self.workspace.online_learner.update(trans_obs_0, actions, e_obses)
            # online_learnerì—ì„œ ê³„ì‚°í•œ lossë¥¼ logsì— ì¶”ê°€í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì €ì¥
            if hasattr(self.workspace.online_learner, 'last_visual_loss'):
                self._computed_visual_loss = self.workspace.online_learner.last_visual_loss
            if hasattr(self.workspace.online_learner, 'last_proprio_loss'):
                self._computed_proprio_loss = self.workspace.online_learner.last_proprio_loss
            if hasattr(self.workspace.online_learner, 'last_loss'):
                self._computed_total_loss = self.workspace.online_learner.last_loss
        else:
            # LoRAê°€ ë¹„í™œì„±í™”ë˜ì–´ ìˆì–´ë„ lossë¥¼ ê³„ì‚°í•˜ê³  ì¶œë ¥ (ë¡œê·¸ í™•ì¸ìš©)
            if self.workspace is not None:
                # ìœ„ì—ì„œ ê³„ì‚°í•œ i_z_obsesë¥¼ ì¬ì‚¬ìš©
                i_z_obses_pred = i_z_obses
                
                # 2. ì •ë‹µ ì¤€ë¹„: ì‹¤ì œ í™˜ê²½ ê²°ê³¼(e_obses)ë¥¼ ì¸ì½”ë”©í•˜ì—¬ 'ì •ë‹µ' ì ì¬ ìƒíƒœë¥¼ ë§Œë“­ë‹ˆë‹¤.
                with torch.no_grad():
                    trans_obs_gt = self.preprocessor.transform_obs(e_obses)
                    trans_obs_gt = move_to_device(trans_obs_gt, self.device)
                    i_z_obses_gt = self.wm.encode_obs(trans_obs_gt)

                    # 3. ì†ì‹¤ ê³„ì‚°: ì˜ˆì¸¡ê³¼ ì •ë‹µ ì‚¬ì´ì˜ ì˜¤ì°¨(MSE Loss)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
                    print("Computing loss...")
                    frameskip = self.frameskip
                    gt_proprio_resampled = i_z_obses_gt["proprio"][:, ::frameskip, :]
                    gt_visual_resampled = i_z_obses_gt["visual"][:, ::frameskip, :, :]
                    
                    # Loss function ê°€ì ¸ì˜¤ê¸°
                    if hasattr(self.workspace, 'loss_fn') and self.workspace.loss_fn is not None:
                        loss_fn = self.workspace.loss_fn
                    else:
                        import torch.nn as nn
                        loss_fn = nn.MSELoss()
                    
                    # ì‹œê°ê³¼ proprioceptive ì†ì‹¤ì„ ê°ê° ê³„ì‚°
                    proprio_loss = loss_fn(i_z_obses_pred["proprio"], gt_proprio_resampled)
                    visual_loss = loss_fn(i_z_obses_pred["visual"], gt_visual_resampled)
     
                    # ê°€ì¤‘í•©ìœ¼ë¡œ ì „ì²´ ì†ì‹¤ ê³„ì‚° (ê¸°ë³¸ê°’ ì‚¬ìš©)
                    visual_weight = getattr(self.workspace, 'visual_loss_weight', 1.0)
                    proprio_weight = getattr(self.workspace, 'proprio_loss_weight', 0.3)
                    total_loss = visual_weight * visual_loss + proprio_weight * proprio_loss
                    
                    print(f"Visual loss: {visual_loss.item():.6f}, Proprio loss: {proprio_loss.item():.6f}")
                    print(f"Total loss: {total_loss.item():.6f}")
                    
                    # logsì— ì¶”ê°€í•˜ê¸° ìœ„í•´ ì„ì‹œë¡œ ì €ì¥
                    self._computed_visual_loss = float(visual_loss.item())
                    self._computed_proprio_loss = float(proprio_loss.item())
                    self._computed_total_loss = float(total_loss.item())
        # if self.is_lora_enabled:
        #     print("--- Starting LoRA Online Learning ---")
            
        #     # 1. ì˜ˆì¸¡: ì›”ë“œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ ë™ì¼í•œ í–‰ë™ìœ¼ë¡œ ë¯¸ë˜ë¥¼ 'ì˜ˆì¸¡'í•©ë‹ˆë‹¤.
        #     #    (ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°ì´ í™œì„±í™”ëœ ìƒíƒœì—ì„œ ì‹¤í–‰)
        #     # print("Step 1: Running world model rollout with gradients enabled...")
        #     torch.cuda.empty_cache()
        #     i_z_obses_pred, _ = self.wm.rollout(
        #         obs_0=trans_obs_0,
        #         act=actions,
        #     )

        #     # 2. ì •ë‹µ ì¤€ë¹„: ì‹¤ì œ í™˜ê²½ ê²°ê³¼(e_obses)ë¥¼ ì¸ì½”ë”©í•˜ì—¬ 'ì •ë‹µ' ì ì¬ ìƒíƒœë¥¼ ë§Œë“­ë‹ˆë‹¤.
        #     # print("Step 2: Encoding ground truth observations...")
        #     with torch.no_grad():
        #         trans_obs_gt = self.preprocessor.transform_obs(e_obses)
        #         trans_obs_gt = move_to_device(trans_obs_gt, self.device)
        #         i_z_obses_gt = self.wm.encode_obs(trans_obs_gt)

        #     # 3. ì†ì‹¤ ê³„ì‚°: ì˜ˆì¸¡ê³¼ ì •ë‹µ ì‚¬ì´ì˜ ì˜¤ì°¨(MSE Loss)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
        #     # .detach()ë¥¼ ì‚¬ìš©í•˜ì—¬ ì •ë‹µê°’ìœ¼ë¡œë¶€í„°ëŠ” ê·¸ë˜ë””ì–¸íŠ¸ê°€ íë¥´ì§€ ì•Šë„ë¡ í•¨)
        #     print("Computing loss...")
        #     # ì‹¤ì œ ê¶¤ì ì„ self.frameskip ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§(slicing)í•˜ì—¬ ì‹œì ì„ í†µì¼í•©ë‹ˆë‹¤.
        #     gt_proprio_resampled = i_z_obses_gt["proprio"][:, ::self.frameskip, :].detach()
        #     gt_visual_resampled = i_z_obses_gt["visual"][:, ::self.frameskip, :, :].detach()
            
        #     # ì‹œê°ê³¼ proprioceptive ì†ì‹¤ì„ ê°ê° ê³„ì‚°
        #     proprio_loss = self.workspace.loss_fn(i_z_obses_pred["proprio"], gt_proprio_resampled)
        #     visual_loss = self.workspace.loss_fn(i_z_obses_pred["visual"], gt_visual_resampled)
 
        #     # ê°€ì¤‘í•©ìœ¼ë¡œ ì „ì²´ ì†ì‹¤ ê³„ì‚° (ì„¤ì • ê°€ëŠ¥í•œ ê°€ì¤‘ì¹˜ ì‚¬ìš©)
        #     visual_weight = self.workspace.visual_loss_weight
        #     proprio_weight = self.workspace.proprio_loss_weight
        #     loss = visual_weight * visual_loss + proprio_weight * proprio_loss
            
        #     print(f"Visual loss: {visual_loss.item():.6f}, Proprio loss: {proprio_loss.item():.6f}")
        #     print(f"Total loss: {loss.item():.6f}")

        #     # 4. ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸: ê³„ì‚°ëœ ì†ì‹¤ì„ ë°”íƒ•ìœ¼ë¡œ LoRA ê°€ì¤‘ì¹˜ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        #     # print("Step 4: Backpropagation and parameter update...")
        #     self.lora_optimizer.zero_grad()
            
        #     # ê·¸ë˜ë””ì–¸íŠ¸ ê³„ì‚°
        #     loss.backward()
            
        #     # ê·¸ë˜ë””ì–¸íŠ¸ í¬ê¸° ì²´í¬
        #     total_grad_norm = 0
        #     trainable_params = 0
        #     for param in self.wm.parameters():
        #         if param.requires_grad and param.grad is not None:
        #             total_grad_norm += param.grad.data.norm(2).item() ** 2
        #             trainable_params += 1
            
        #     total_grad_norm = total_grad_norm ** 0.5
        #     print(f"Gradient norm: {total_grad_norm:.6f}, Trainable params: {trainable_params}")
            
        #     # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸
        #     self.lora_optimizer.step()
            
        #     # ë©”ëª¨ë¦¬ ì •ë¦¬
        #     del i_z_obses_pred, i_z_obses_gt, trans_obs_gt
        #     torch.cuda.empty_cache()
        #     print(f"--- LoRA Online Update Complete ---")
            
            # 5. ì¶”ê°€ ê²€ì¦: LoRA íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ í™•ì¸
            # if hasattr(self, '_prev_lora_params') and self._prev_lora_params is not None:
            #     # í˜„ì¬ í•™ìŠµ ê°€ëŠ¥í•œ(requires_grad=True) ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜´
            #     current_lora_params = [
            #         p.clone() for p in self.wm.predictor.parameters() if p.requires_grad
            #     ]
                
            #     if len(self._prev_lora_params) != len(current_lora_params):
            #         print("Warning: Number of trainable parameters changed. Skipping parameter change check for this step.")
                
            #     else:
            #         param_changes = []
            #         all_shapes_match = True
            #         for prev, curr in zip(self._prev_lora_params, current_lora_params):
            #             if prev.shape != curr.shape:
            #                 print(f"Warning: Parameter shape mismatch. Prev: {prev.shape}, Curr: {curr.shape}. Skipping check.")
            #                 all_shapes_match = False
            #                 break # ëª¨ì–‘ì´ í•˜ë‚˜ë¼ë„ ë‹¤ë¥´ë©´ ë¹„êµ ì¤‘ë‹¨
                        
            #             change = torch.norm(curr - prev).item()
            #             param_changes.append(change)

            #         if all_shapes_match and param_changes:
            #             avg_param_change = sum(param_changes) / len(param_changes)
            #             print(f"Average LoRA parameter change: {avg_param_change:.8f}")
            
            # params_to_save = [
            #     p.clone() for p in self.wm.predictor.parameters() if p.requires_grad
            # ]
            # self._prev_lora_params = params_to_save
            
            # # ë©”ëª¨ë¦¬ ì •ë¦¬
            # del params_to_save
            # if 'current_lora_params' in locals():
            #     del current_lora_params
            
        # LoRA í•™ìŠµ ì™„ë£Œ í›„ ìµœì¢… ë©”ëª¨ë¦¬ ì •ë¦¬
        # torch.cuda.empty_cache()
        # ======================================================= #
        e_visuals = e_obses["visual"]
        e_final_obs = self._get_trajdict_last(e_obses, action_len * self.frameskip + 1)
        e_final_state = self._get_traj_last(e_states, action_len * self.frameskip + 1)[
            :, 0
        ]  # reduce dim back

        # compute eval metrics
        logs, successes = self._compute_rollout_metrics(
            e_state=e_final_state,
            e_obs=e_final_obs,
            i_z_obs=i_final_z_obs,
        )
        
        # ê³„ì‚°í•œ visual_lossë¥¼ logsì— ì¶”ê°€ (learning_enabled ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
        if self.workspace is not None and hasattr(self, '_computed_visual_loss'):
            logs_to_add = {}
            if self._computed_visual_loss is not None:
                logs_to_add["visual_loss"] = self._computed_visual_loss
            if hasattr(self, '_computed_proprio_loss') and self._computed_proprio_loss is not None:
                logs_to_add["proprio_loss"] = self._computed_proprio_loss
            if hasattr(self, '_computed_total_loss') and self._computed_total_loss is not None:
                logs_to_add["total_loss"] = self._computed_total_loss
            if logs_to_add:
                logs.update(logs_to_add)
            # ì„ì‹œ ë³€ìˆ˜ ì •ë¦¬
            delattr(self, '_computed_visual_loss')
            if hasattr(self, '_computed_proprio_loss'):
                delattr(self, '_computed_proprio_loss')
            if hasattr(self, '_computed_total_loss'):
                delattr(self, '_computed_total_loss')

        # plot/save rollouts
        if save_video:
            if self.wm.decoder is not None:
                i_visuals = self.wm.decode_obs(i_z_obses)[0]["visual"]
                i_visuals = self._mask_traj(
                    i_visuals, action_len + 1
                )  # we have action_len + 1 states
                e_visuals_t = self.preprocessor.transform_obs_visual(e_visuals)
                e_visuals_t = self._mask_traj(
                    e_visuals_t, action_len * self.frameskip + 1
                )
                self._plot_rollout_compare(
                    e_visuals=e_visuals_t,
                    i_visuals=i_visuals,
                    successes=successes,
                    save_video=True,
                    filename=filename,
                )
            else:
                # decoderê°€ ì—†ìœ¼ë©´ í™˜ê²½ ì˜ìƒë§Œ ì €ì¥
                self._plot_rollout_env_only(
                    e_visuals=e_visuals,
                    successes=successes,
                    save_video=True,
                    filename=filename,
                )

        return logs, successes, e_obses, e_states

    def force_recenter_for_next_rollout(self):
        """Force environments to recenter on the very next rollout."""
        def _force_wrapper(wrapper):
            if hasattr(wrapper, "force_recenter_next_rollout"):
                wrapper.force_recenter_next_rollout()
            else:
                if hasattr(wrapper, "_force_recenter_after_set_states"):
                    wrapper._force_recenter_after_set_states = True
                inner_env = getattr(wrapper, "env", None)
                if inner_env is not None:
                    if hasattr(inner_env, "reset_recentering_counters"):
                        inner_env.reset_recentering_counters()
                    if hasattr(inner_env, "_force_recenter_after_set_states"):
                        inner_env._force_recenter_after_set_states = True

        if hasattr(self.env, "envs"):
            for env_wrapper in self.env.envs:
                _force_wrapper(env_wrapper)
        elif hasattr(self.env, "workers"):
            for worker in self.env.workers:
                env_wrapper = getattr(worker, "env", None)
                if env_wrapper is not None:
                    _force_wrapper(env_wrapper)
        else:
            _force_wrapper(self.env)

        if hasattr(self, "_initial_cd_measured"):
            self._initial_cd_measured = False

    def _compute_rollout_metrics(self, e_state, e_obs, i_z_obs):
        """
        Args
            e_state
            e_obs
            i_z_obs
        Return
            logs
            successes
        """
        eval_results = self.env.eval_state(self.state_g, e_state)
        successes = eval_results['success']

        logs = {
            f"success_rate" if key == "success" else f"mean_{key}": np.mean(value) if key != "success" else np.mean(value.astype(float))
            for key, value in eval_results.items()
        }

        print("Success rate: ", logs['success_rate'])
        print(eval_results)

        visual_dists = np.linalg.norm(e_obs["visual"] - self.obs_g["visual"], axis=1)
        mean_visual_dist = np.mean(visual_dists)
        proprio_dists = np.linalg.norm(e_obs["proprio"] - self.obs_g["proprio"], axis=1)
        mean_proprio_dist = np.mean(proprio_dists)

        e_obs = move_to_device(self.preprocessor.transform_obs(e_obs), self.device)
        e_z_obs = self.wm.encode_obs(e_obs)
        div_visual_emb = torch.norm(e_z_obs["visual"] - i_z_obs["visual"]).item()
        div_proprio_emb = torch.norm(e_z_obs["proprio"] - i_z_obs["proprio"]).item()

        logs.update({
            "mean_visual_dist": mean_visual_dist,
            "mean_proprio_dist": mean_proprio_dist,
            "mean_div_visual_emb": div_visual_emb,
            "mean_div_proprio_emb": div_proprio_emb,
        })

        return logs, successes

    def _plot_rollout_compare(
        self, e_visuals, i_visuals, successes, save_video=False, filename=""
    ):
        """
        i_visuals may have less frames than e_visuals due to frameskip, so pad accordingly
        e_visuals: (b, t, h, w, c)
        i_visuals: (b, t, h, w, c)
        goal: (b, h, w, c)
        """
        e_visuals = e_visuals[: self.n_plot_samples]
        i_visuals = i_visuals[: self.n_plot_samples]
        goal_visual = self.obs_g["visual"][: self.n_plot_samples]
        goal_visual = self.preprocessor.transform_obs_visual(goal_visual)

        i_visuals = i_visuals.unsqueeze(2)
        i_visuals = torch.cat(
            [i_visuals] + [i_visuals] * (self.frameskip - 1),
            dim=2,
        )  # pad i_visuals (due to frameskip)
        i_visuals = rearrange(i_visuals, "b t n c h w -> b (t n) c h w")
        i_visuals = i_visuals[:, : i_visuals.shape[1] - (self.frameskip - 1)]

        correction = 0.3  # to distinguish env visuals and imagined visuals

        if save_video:
            for idx in range(e_visuals.shape[0]):
                success_tag = "success" if successes[idx] else "failure"
                frames = []
                for i in range(e_visuals.shape[1]):
                    e_obs = e_visuals[idx, i, ...]
                    i_obs = i_visuals[idx, i, ...]
                    e_obs = torch.cat(
                        [e_obs.cpu(), goal_visual[idx, 0] - correction], dim=2
                    )
                    i_obs = torch.cat(
                        [i_obs.cpu(), goal_visual[idx, 0] - correction], dim=2
                    )
                    frame = torch.cat([e_obs - correction, i_obs], dim=1)
                    frame = rearrange(frame, "c w1 w2 -> w1 w2 c")
                    frame = rearrange(frame, "w1 w2 c -> (w1) w2 c")
                    frame = frame.detach().cpu().numpy()
                    frames.append(frame)
                video_writer = imageio.get_writer(
                    f"{filename}_{idx}_{success_tag}.mp4", fps=12
                )

                for frame in frames:
                    frame = frame * 2 - 1 if frame.min() >= 0 else frame
                    video_writer.append_data(
                        (((np.clip(frame, -1, 1) + 1) / 2) * 255).astype(np.uint8)
                    )
                video_writer.close()

        # pad i_visuals or subsample e_visuals
        if not self.plot_full:
            e_visuals = e_visuals[:, :: self.frameskip]
            i_visuals = i_visuals[:, :: self.frameskip]

        n_columns = e_visuals.shape[1]
        assert (
            i_visuals.shape[1] == n_columns
        ), f"Rollout lengths do not match, {e_visuals.shape[1]} and {i_visuals.shape[1]}"

        # add a goal column
        e_visuals = torch.cat([e_visuals.cpu(), goal_visual - correction], dim=1)
        i_visuals = torch.cat([i_visuals.cpu(), goal_visual - correction], dim=1)
        rollout = torch.cat([e_visuals.cpu() - correction, i_visuals.cpu()], dim=1)
        n_columns += 1

        imgs_for_plotting = rearrange(rollout, "b h c w1 w2 -> (b h) c w1 w2")
        imgs_for_plotting = (
            imgs_for_plotting * 2 - 1
            if imgs_for_plotting.min() >= 0
            else imgs_for_plotting
        )
        utils.save_image(
            imgs_for_plotting,
            f"{filename}.png",
            nrow=n_columns,  # nrow is the number of columns
            normalize=True,
            value_range=(-1, 1),
        )

    def _plot_rollout_env_only(self, e_visuals, successes, save_video=False, filename=""):
        """
        ë””ì½”ë”ê°€ ì—†ì„ ë•Œ, í™˜ê²½ì—ì„œ ê´€ì¸¡ëœ í”„ë ˆì„(e_visuals)ê³¼ ëª©í‘œ ì´ë¯¸ì§€ë¥¼ ì´ìš©í•´
        PNG/MP4ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
        """
        # ì‹œê° ë°ì´í„° ì •ê·œí™” ë° ìƒ˜í”Œ ìˆ˜ ì œí•œ
        e_visuals = self.preprocessor.transform_obs_visual(e_visuals)
        e_visuals = e_visuals[: self.n_plot_samples]
        goal_visual = self.obs_g["visual"][: self.n_plot_samples]
        goal_visual = self.preprocessor.transform_obs_visual(goal_visual)

        correction = 0.3

        # ë¹„ë””ì˜¤ ì €ì¥: ê° ìƒ˜í”Œë³„ë¡œ ì‹¤í–‰ í”„ë ˆì„ê³¼ ëª©í‘œ í•œ ì¥ì„ ì¢Œìš°ë¡œ ë¶™ì—¬ ì €ì¥
        if save_video:
            for idx in range(e_visuals.shape[0]):
                success_tag = "success" if successes[idx] else "failure"
                frames = []
                for i in range(e_visuals.shape[1]):
                    e_obs = e_visuals[idx, i, ...]
                    frame = torch.cat(
                        [e_obs.cpu() - correction, goal_visual[idx, 0] - correction],
                        dim=2,
                    )
                    frame = rearrange(frame, "c w1 w2 -> w1 w2 c")
                    frame = frame.detach().cpu().numpy()
                    frames.append(frame)
                video_writer = imageio.get_writer(
                    f"{filename}_{idx}_{success_tag}.mp4", fps=12
                )
                for frame in frames:
                    frame = frame * 2 - 1 if frame.min() >= 0 else frame
                    video_writer.append_data(
                        (((np.clip(frame, -1, 1) + 1) / 2) * 255).astype(np.uint8)
                    )
                video_writer.close()

        # ì´ë¯¸ì§€ ê·¸ë¦¬ë“œ ì €ì¥: ë§ˆì§€ë§‰ í”„ë ˆì„ê³¼ ëª©í‘œ ì´ë¯¸ì§€ë¥¼ ì¢Œìš°ë¡œ ë¶™ì—¬ í•œ ì¥ìœ¼ë¡œ ì €ì¥
        if not self.plot_full:
            e_visuals = e_visuals[:, :: self.frameskip]
        last_frames = e_visuals[:, -1]  # (b, c, h, w)
        goal_frames = goal_visual[:, 0]  # (b, c, h, w)
        rollout = torch.cat([last_frames.cpu() - correction, goal_frames - correction], dim=2)
        utils.save_image(
            rollout,
            f"{filename}.png",
            nrow=1,
            normalize=True,
            value_range=(-1, 1),
        )