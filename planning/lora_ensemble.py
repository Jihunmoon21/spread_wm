import torch
import torch.nn as nn
import math
import time
import os
import gzip
import pickle
import io
import numpy as np
from typing import Dict, List, Tuple, Optional, Any
from collections import OrderedDict, deque
from contextlib import redirect_stdout
from utils import move_to_device


class LoRAEnsembleManager:
    """
    LoRA ì•™ìƒë¸”ì„ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤
    
    ê¸°ëŠ¥:
    - ì—¬ëŸ¬ LoRA ëª¨ë¸ì„ ì•™ìƒë¸”ë¡œ ê´€ë¦¬
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì €ì¥/ë¡œë“œ
    - ì•™ìƒë¸” ë©¤ë²„ ì„±ëŠ¥ ì¶”ì 
    - LoRA ê°€ì¤‘ì¹˜ í†µí•©
    """
    
    def __init__(self, base_model, max_ensemble_size: int = 11, 
                 cache_dir: str = "./lora_cache", max_memory_mb: int = 200):
        self.base_model = base_model
        self.max_ensemble_size = max_ensemble_size
        self.cache_dir = cache_dir
        self.max_memory_mb = max_memory_mb
        
        # ì•™ìƒë¸” ë©¤ë²„ ê´€ë¦¬
        self.ensemble_members = OrderedDict()  # {task_id: member_info}
        self.current_task_id = 0
        self.memory_usage = 0
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = {}  # {task_id: [performance_scores]}
        self.access_frequency = {}   # {task_id: access_count}
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        
        print(f"LoRAEnsembleManager initialized:")
        print(f"  - Max ensemble size: {max_ensemble_size}")
        print(f"  - Cache directory: {cache_dir}")
        print(f"  - Max memory usage: {max_memory_mb}MB")
    
    def add_ensemble_member(self, task_id: int, lora_weights: Dict, 
                           performance: Dict, metadata: Optional[Dict] = None) -> bool:
        """
        ìƒˆë¡œìš´ ì•™ìƒë¸” ë©¤ë²„ ì¶”ê°€
        
        Args:
            task_id: íƒœìŠ¤í¬ ID
            lora_weights: LoRA ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬
            performance: ì„±ëŠ¥ ì§€í‘œ {'loss': float, 'accuracy': float, ...}
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            bool: ì¶”ê°€ ì„±ê³µ ì—¬ë¶€
        """
        try:
            if task_id in self.ensemble_members:
                print(f"â„¹ï¸  Ensemble member for Task {task_id} already exists. Skipping update to preserve original weights.")
                return False

            # ì•™ìƒë¸” í¬ê¸° ì œí•œ í™•ì¸
            if len(self.ensemble_members) >= self.max_ensemble_size:
                self._remove_oldest_member()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            lora_size_mb = self._calculate_lora_size(lora_weights)
            if self.memory_usage + lora_size_mb > self.max_memory_mb:
                self._cleanup_memory()
            
            # ì•™ìƒë¸” ë©¤ë²„ ì •ë³´ ìƒì„±
            member_info = {
                'task_id': task_id,
                'lora_weights': lora_weights,
                'performance': performance,
                'metadata': metadata or {},
                'created_at': time.time(),
                'last_accessed': time.time(),
                'access_count': 0,
                'size_mb': lora_size_mb
            }
            
            # ì•ˆì „í•œ ê¹Šì€ ë³µì‚¬ë¡œ ì €ì¥ (ì°¸ì¡° ì˜¤ì—¼ ë°©ì§€)
            safe_weights = {}
            for layer_key, layer_vals in lora_weights.items():
                if isinstance(layer_vals, dict):
                    w_A = layer_vals.get('w_A', None)
                    w_B = layer_vals.get('w_B', None)
                    if w_A is not None and w_B is not None:
                        safe_weights[layer_key] = {
                            'w_A': w_A.clone().detach().cpu(),
                            'w_B': w_B.clone().detach().cpu(),
                        }
            member_info['lora_weights'] = safe_weights

            # ì•™ìƒë¸”ì— ì¶”ê°€
            self.ensemble_members[task_id] = member_info
            self.memory_usage += lora_size_mb
            
            # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            if task_id not in self.performance_history:
                self.performance_history[task_id] = []
            self.performance_history[task_id].append(performance)
            
            # ì ‘ê·¼ ë¹ˆë„ ì´ˆê¸°í™”
            self.access_frequency[task_id] = 0
            
            print(f"âœ… Added ensemble member: Task {task_id}")
            print(f"   - Performance: {performance}")
            print(f"   - Size: {lora_size_mb:.2f}MB")
            print(f"   - Total members: {len(self.ensemble_members)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Failed to add ensemble member: {e}")
            return False
    
    def get_best_member(self, input_data: torch.Tensor, 
                       metric: str = 'loss') -> Optional[Dict]:
        """
        ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ì•™ìƒë¸” ë©¤ë²„ ì„ íƒ
        
        Args:
            input_data: ì…ë ¥ ë°ì´í„°
            metric: ì„±ëŠ¥ ì§€í‘œ ('loss', 'accuracy', etc.)
            
        Returns:
            Dict: ìµœì ì˜ ì•™ìƒë¸” ë©¤ë²„ ì •ë³´
        """
        if not self.ensemble_members:
            print("âš ï¸  No ensemble members available")
            return None
        
        best_member = None
        best_score = float('inf') if metric == 'loss' else float('-inf')
        
        for task_id, member_info in self.ensemble_members.items():
            # ì ‘ê·¼ ë¹ˆë„ ì—…ë°ì´íŠ¸
            self.access_frequency[task_id] += 1
            member_info['last_accessed'] = time.time()
            member_info['access_count'] += 1
            
            # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            performance = member_info['performance']
            score = performance.get(metric, float('inf'))
            
            # ìµœì  ì„±ëŠ¥ í™•ì¸
            is_better = (score < best_score) if metric == 'loss' else (score > best_score)
            if is_better:
                best_score = score
                best_member = member_info
        
        if best_member:
            print(f"ğŸ¯ Selected best member: Task {best_member['task_id']} "
                  f"(score: {best_score:.6f})")
        
        return best_member
    
    def save_ensemble_to_disk(self, save_path: str) -> bool:
        """
        ì•™ìƒë¸”ì„ ë””ìŠ¤í¬ì— ì €ì¥
        
        Args:
            save_path: ì €ì¥ ê²½ë¡œ
            
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            ensemble_data = {
                'ensemble_members': dict(self.ensemble_members),
                'performance_history': self.performance_history,
                'access_frequency': self.access_frequency,
                'current_task_id': self.current_task_id,
                'metadata': {
                    'max_ensemble_size': self.max_ensemble_size,
                    'cache_dir': self.cache_dir,
                    'max_memory_mb': self.max_memory_mb,
                    'saved_at': time.time()
                }
            }
            
            # ì••ì¶•í•˜ì—¬ ì €ì¥
            compressed_data = gzip.compress(pickle.dumps(ensemble_data))
            
            with open(save_path, 'wb') as f:
                f.write(compressed_data)
            
            print(f"ğŸ’¾ Ensemble saved to: {save_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to save ensemble: {e}")
            return False
    
    def load_ensemble_from_disk(self, load_path: str) -> bool:
        """
        ë””ìŠ¤í¬ì—ì„œ ì•™ìƒë¸” ë¡œë“œ
        
        Args:
            load_path: ë¡œë“œ ê²½ë¡œ
            
        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with open(load_path, 'rb') as f:
                compressed_data = f.read()
            
            ensemble_data = pickle.loads(gzip.decompress(compressed_data))
            
            # ì•™ìƒë¸” ë°ì´í„° ë³µì›
            self.ensemble_members = OrderedDict(ensemble_data['ensemble_members'])
            self.performance_history = ensemble_data['performance_history']
            self.access_frequency = ensemble_data['access_frequency']
            self.current_task_id = ensemble_data['current_task_id']
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬ê³„ì‚°
            self.memory_usage = sum(
                member['size_mb'] for member in self.ensemble_members.values()
            )
            
            print(f"ğŸ“‚ Ensemble loaded from: {load_path}")
            print(f"   - Members: {len(self.ensemble_members)}")
            print(f"   - Memory usage: {self.memory_usage:.2f}MB")
            
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load ensemble: {e}")
            return False
    
    def get_ensemble_info(self) -> Dict:
        """
        ì•™ìƒë¸” ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict: ì•™ìƒë¸” ìƒíƒœ ì •ë³´
        """
        return {
            'member_count': len(self.ensemble_members),
            'max_ensemble_size': self.max_ensemble_size,
            'memory_usage_mb': self.memory_usage,
            'max_memory_mb': self.max_memory_mb,
            'current_task_id': self.current_task_id,
            'members': list(self.ensemble_members.keys()),
            'cache_dir': self.cache_dir
        }
    
    def _calculate_lora_size(self, lora_weights: Dict) -> float:
        """LoRA ê°€ì¤‘ì¹˜ í¬ê¸° ê³„ì‚° (MB ë‹¨ìœ„)"""
        total_params = 0
        for layer_name, weights in lora_weights.items():
            if isinstance(weights, torch.Tensor):
                total_params += weights.numel()
            elif isinstance(weights, dict):
                for sub_weights in weights.values():
                    if isinstance(sub_weights, torch.Tensor):
                        total_params += sub_weights.numel()
        
        # float32 ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        size_mb = (total_params * 4) / (1024 * 1024)
        return size_mb
    
    def _remove_oldest_member(self):
        """ê°€ì¥ ì˜¤ë˜ëœ ì•™ìƒë¸” ë©¤ë²„ ì œê±°"""
        if not self.ensemble_members:
            return
        
        # ê°€ì¥ ì˜¤ë˜ëœ ë©¤ë²„ ì°¾ê¸°
        oldest_task_id = min(self.ensemble_members.keys(), 
                           key=lambda x: self.ensemble_members[x]['created_at'])
        
        oldest_member = self.ensemble_members[oldest_task_id]
        self.memory_usage -= oldest_member['size_mb']
        
        # ë””ìŠ¤í¬ì— ì €ì¥ í›„ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
        self._save_member_to_disk(oldest_task_id, oldest_member)
        del self.ensemble_members[oldest_task_id]
        
        print(f"ğŸ—‘ï¸  Removed oldest member: Task {oldest_task_id}")
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.memory_usage <= self.max_memory_mb:
            return
        
        # ì ‘ê·¼ ë¹ˆë„ê°€ ë‚®ì€ ë©¤ë²„ë“¤ì„ ë””ìŠ¤í¬ë¡œ ì´ë™
        sorted_members = sorted(
            self.ensemble_members.items(),
            key=lambda x: x[1]['access_count']
        )
        
        for task_id, member_info in sorted_members:
            if self.memory_usage <= self.max_memory_mb * 0.8:  # 80%ê¹Œì§€ ì •ë¦¬
                break
            
            self._save_member_to_disk(task_id, member_info)
            self.memory_usage -= member_info['size_mb']
            print(f"ğŸ’¾ Moved to disk: Task {task_id}")
    
    def _save_member_to_disk(self, task_id: int, member_info: Dict):
        """ë©¤ë²„ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥ (ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê±´ë„ˆëœ€)"""
        save_path = os.path.join(self.cache_dir, f"lora_task_{task_id}.pth")
        
        # íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ë®ì–´ì“°ì§€ ì•ŠìŒ
        if os.path.exists(save_path):
            print(f"â„¹ï¸  LoRA file for Task {task_id} already exists at {save_path}. Skipping save to preserve original weights.")
            return
        
        # LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥ (ë©”íƒ€ë°ì´í„°ëŠ” ë³„ë„ ê´€ë¦¬)
        torch.save(member_info['lora_weights'], save_path)
        print(f"ğŸ’¾ Saved LoRA member for Task {task_id} to {save_path}")
    


class EnsembleOnlineLora:
    """
    ì•™ìƒë¸” ê¸°ë°˜ Online-LoRA í•™ìŠµ ì‹œìŠ¤í…œ
    
    ê¸°ì¡´ OnlineLoraë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ì„œ
    ì•™ìƒë¸” ê¸°ë°˜ì˜ ì§€ëŠ¥ì ì¸ LoRA ì ì¸µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, workspace):
        """
        ì•™ìƒë¸” Online-LoRA ì´ˆê¸°í™”
        
        Args:
            workspace: PlanWorkspace ì¸ìŠ¤í„´ìŠ¤
        """
        self.workspace = workspace
        self.wm = workspace.wm
        # lora.ensemble_cfg ë˜ëŠ” lora ë£¨íŠ¸ì—ì„œ ì½ê¸°
        self.cfg = workspace.cfg_dict.get("lora", {}).get("ensemble_cfg", workspace.cfg_dict.get("lora", {}))
        self.device = next(self.wm.parameters()).device
        
        # ê¸°ì¡´ OnlineLoraë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©
        from planning.online import OnlineLora
        self.base_online_lora = OnlineLora(workspace)
        
        # ê¸°ì¡´ Online-LoRAì˜ ì†ì„±ë“¤ì„ ì°¸ì¡°
        self.is_online_lora = self.base_online_lora.is_online_lora
        self.hybrid_enabled = self.base_online_lora.hybrid_enabled
        self.task_based_stacking = self.base_online_lora.task_based_stacking
        self.loss_based_stacking = self.base_online_lora.loss_based_stacking
        self.max_stacks_per_task = self.base_online_lora.max_stacks_per_task
        self.stacks_in_current_task = self.base_online_lora.stacks_in_current_task
        self.current_task_id = self.base_online_lora.current_task_id
        self.task_changed = self.base_online_lora.task_changed  # íƒœìŠ¤í¬ ì „í™˜ ê°ì§€ í”Œë˜ê·¸
        self.stack_history = self.base_online_lora.stack_history
        self.last_loss = self.base_online_lora.last_loss  # ì´ˆê¸°ê°’ ë³µì‚¬ (update()ì—ì„œ ë™ê¸°í™”)
        self.last_visual_loss = getattr(self.base_online_lora, "last_visual_loss", None)
        self.last_proprio_loss = getattr(self.base_online_lora, "last_proprio_loss", None)
        self.optimizer = self.base_online_lora.optimizer
        self.loss_fn = self.base_online_lora.loss_fn
        self.visual_loss_weight = self.base_online_lora.visual_loss_weight
        self.proprio_loss_weight = self.base_online_lora.proprio_loss_weight
        
        # ì•™ìƒë¸” ì „ìš© ì„¤ì •
        self.ensemble_evaluation_steps = self.cfg.get("evaluation_steps", 10)
        
        # ì•™ìƒë¸” ì €ì¥ ì •ì±… (ê¸°ë³¸: ìŠ¤íƒ ì§í›„ ì €ì¥ ë¹„í™œì„±í™”, íƒœìŠ¤í¬ ì¢…ë£Œ ì‹œ ì €ì¥)
        self.save_on_stack = self.cfg.get("save_on_stack", False)
        self.save_on_task_end = self.cfg.get("save_on_task_end", True)

        # ì•™ìƒë¸” ê´€ë¦¬ì ì´ˆê¸°í™”
        self.ensemble_manager = LoRAEnsembleManager(
            base_model=self.wm,
            max_ensemble_size=self.cfg.get("max_ensemble_size", 5),
            cache_dir=self.cfg.get("cache_dir", "./lora_cache"),
            max_memory_mb=self.cfg.get("max_memory_mb", 200)
        )

        # ìµœê·¼ ì•™ìƒë¸” í‰ê°€ ê²°ê³¼ ì €ì¥
        self.last_ensemble_evaluation_summary: Optional[Dict[str, Any]] = None
        self.created_task_ids = set()
        
        print(f"EnsembleOnlineLora initialized:")
        print(f"  - Base OnlineLora: {self.is_online_lora}")
        print(f"  - Ensemble enabled: {self.hybrid_enabled}")
        print(f"  - Task-based stacking: {self.task_based_stacking}")
        
        # ğŸ”§ OnlineLoraì— ì½œë°± ì„¤ì •
        self.base_online_lora.on_lora_stack_callback = self._on_lora_stacked
        
        # ì•™ìƒë¸” ì „ìš© ì½œë°± ì´ˆê¸°í™”
        self.on_lora_stack_callback = None
    
    def update(self, trans_obs_0, actions, e_obses):
        """
        í•˜ë‚˜ì˜ í•™ìŠµ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë©”ì†Œë“œ (ì‹œê°„ ì¸¡ì • í¬í•¨)
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            e_obses: ì‹¤ì œ ê´€ì¸¡ ì‹œí€€ìŠ¤
        """
        start_time = time.time()
        # ê¸°ì¡´ OnlineLoraì˜ í•™ìŠµ ë¡œì§ ì‚¬ìš© (ë°˜í™˜ê°’ ì—†ìŒ, ë‚´ë¶€ì ìœ¼ë¡œ last_loss ì„¤ì •)
        self.base_online_lora.update(trans_obs_0, actions, e_obses)
        adaptation_time = time.time() - start_time
        print(f"Ensemble LoRA adaptation time: {adaptation_time:.4f} seconds")
        
        # ğŸ”§ adaptation time ì €ì¥ (base_online_loraì— ì´ë¯¸ ì €ì¥ë˜ì§€ë§Œ, Ensemble ë ˆë²¨ì—ì„œë„ ì¶”ì )
        if not hasattr(self, 'adaptation_times'):
            self.adaptation_times = []
        self.adaptation_times.append(adaptation_time)
        
        # ğŸ”§ ëª¨ë“  ìƒíƒœ ë™ê¸°í™” (base_online_loraì—ì„œ ì„¤ì •ëœ ê°’ ì‚¬ìš©)
        self.last_loss = self.base_online_lora.last_loss
        self.last_visual_loss = getattr(self.base_online_lora, "last_visual_loss", None)
        self.last_proprio_loss = getattr(self.base_online_lora, "last_proprio_loss", None)
        self.last_visual_loss = getattr(self.base_online_lora, "last_visual_loss", None)
        self.last_proprio_loss = getattr(self.base_online_lora, "last_proprio_loss", None)
        self.task_changed = self.base_online_lora.task_changed
        self.stacks_in_current_task = self.base_online_lora.stacks_in_current_task
        self.current_task_id = self.base_online_lora.current_task_id
        
        # ì•™ìƒë¸” ê¸°ë°˜ ì ì¸µ ë¡œì§ (í–¥í›„ êµ¬í˜„)
        if self.last_loss is not None and self.hybrid_enabled:
            self._manage_ensemble_stacking(self.last_loss)
    
    def compute_loss_only(self, trans_obs_0, actions, e_obses):
        """
        ì˜¨ë¼ì¸ í•™ìŠµ ì—†ì´ í˜„ì¬ ëª¨ë¸ì˜ ì†ì‹¤ì„ í‰ê°€í•©ë‹ˆë‹¤.
        """
        metrics = self.base_online_lora.compute_loss_only(trans_obs_0, actions, e_obses)
        if metrics:
            self.last_loss = self.base_online_lora.last_loss
            self.last_visual_loss = getattr(self.base_online_lora, "last_visual_loss", None)
            self.last_proprio_loss = getattr(self.base_online_lora, "last_proprio_loss", None)
        return metrics
    
    def trigger_task_based_stacking(self, task_id, reason="task_change"):
        """
        íƒœìŠ¤í¬ ê¸°ë°˜ ì ì¸µì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤ (ì•™ìƒë¸” ê¸°ë°˜)
        
        Args:
            task_id: ìƒˆë¡œìš´ íƒœìŠ¤í¬ ID
            reason: ì ì¸µ ì´ìœ 
            
        Returns:
            bool: ì ì¸µ ì„±ê³µ ì—¬ë¶€
        """
        # ğŸ”§ ì•™ìƒë¸” ì „ìš© ëª¨ë“œì—ì„œëŠ” task_based_stackingì´ falseì—¬ë„ ì‘ë™
        if not self.hybrid_enabled:
            return False
        
        # ğŸ”§ íƒœìŠ¤í¬ê°€ ë³€ê²½ëœ ê²½ìš° - base_online_loraì™€ ë™ê¸°í™”
        if task_id != self.current_task_id:
            self.current_task_id = task_id
            self.stacks_in_current_task = 0
            # base_online_loraì˜ ê°’ë„ ë™ê¸°í™”
            self.base_online_lora.current_task_id = task_id
            self.base_online_lora.stacks_in_current_task = 0
            print(f"ğŸ”„ Task changed to {task_id}. Resetting stack counter.")
        
        # ğŸ”§ ìµœëŒ€ ì ì¸µ íšŸìˆ˜ í™•ì¸ - base_online_loraì˜ ì‹¤ì œ ê°’ ì‚¬ìš©
        actual_stacks = getattr(self.base_online_lora, 'stacks_in_current_task', 0)
        if actual_stacks >= self.max_stacks_per_task:
            print(f"âš ï¸  Max stacks per task ({self.max_stacks_per_task}) reached. Skipping ensemble-based stacking.")
            return False
        
        # ì•™ìƒë¸” ê¸°ë°˜ ì ì¸µ ê²°ì •
        should_stack, best_member = self._should_stack_lora_ensemble(task_id)
        
        if should_stack:
            print(f"ğŸ¯ Ensemble-based LoRA stacking triggered (Task {task_id}, Reason: {reason})")
            
            # ğŸ”§ ì•™ìƒë¸” ì „ìš© ëª¨ë“œì—ì„œëŠ” ì§ì ‘ ì ì¸µ ìˆ˜í–‰ (task_based_stackingì´ falseì—¬ë„)
            if not self.task_based_stacking:
                # ì§ì ‘ ì ì¸µ ìˆ˜í–‰
                stacking_success = self._perform_ensemble_lora_stacking(task_id, best_member, reason)
                
                if stacking_success:
                    # ğŸ”§ ì¹´ìš´í„° ë™ê¸°í™” - base_online_loraì˜ ê°’ ì‚¬ìš©
                    self.stacks_in_current_task = getattr(self.base_online_lora, 'stacks_in_current_task', 0)
                    print(f"âœ… Ensemble-based stacking successful. Total stacks in task: {self.stacks_in_current_task}")
                    print(f"ğŸ”§ Final sync: base_online_lora.stacks_in_current_task = {self.base_online_lora.stacks_in_current_task}")
                    return True
                else:
                    print(f"âŒ Ensemble-based stacking failed.")
                    return False
            else:
                # ê¸°ì¡´ OnlineLoraì˜ íƒœìŠ¤í¬ ê¸°ë°˜ ì ì¸µ ì‚¬ìš©
                stacking_success = self.base_online_lora.trigger_task_based_stacking(task_id, reason)
                
                if stacking_success:
                    # ğŸ”§ ì¹´ìš´í„° ë™ê¸°í™” - base_online_loraì˜ ê°’ ì‚¬ìš©
                    self.stacks_in_current_task = getattr(self.base_online_lora, 'stacks_in_current_task', 0)
                    print(f"âœ… Ensemble-based stacking successful. Total stacks in task: {self.stacks_in_current_task}")
                    print(f"ğŸ”§ Final sync: base_online_lora.stacks_in_current_task = {self.base_online_lora.stacks_in_current_task}")
                    return True
                else:
                    print(f"âŒ Base OnlineLora stacking failed. Skipping ensemble management.")
                    return False
        else:
            print(f"ğŸ“Š Using existing ensemble for Task {task_id} (performance sufficient)")
            return False
    
    def _perform_training_step(self, trans_obs_0, actions, e_obses):
        """ì‹¤ì œ ì˜ˆì¸¡, ì†ì‹¤ ê³„ì‚°, ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            print("--- Starting Ensemble LoRA Online Learning ---")
            
            # 1. ì˜ˆì¸¡ (ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™”)
            step_start = time.time()
            i_z_obses_pred, _ = self.wm.rollout(obs_0=trans_obs_0, act=actions)
            rollout_time = time.time() - step_start

            # 2. ì •ë‹µ ì¤€ë¹„ (ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”)
            encode_start = time.time()
            with torch.no_grad():
                trans_obs_gt = self.workspace.data_preprocessor.transform_obs(e_obses)
                trans_obs_gt = move_to_device(trans_obs_gt, self.device)
                i_z_obses_gt = self.wm.encode_obs(trans_obs_gt)
            encode_time = time.time() - encode_start

            # 3. ì†ì‹¤ ê³„ì‚°
            loss_start = time.time()
            print("Computing ensemble loss...")
            frameskip = self.workspace.frameskip
            gt_proprio_resampled = i_z_obses_gt["proprio"][:, ::frameskip, :].detach()
            gt_visual_resampled = i_z_obses_gt["visual"][:, ::frameskip, :, :].detach()
            
            proprio_loss = self.loss_fn(i_z_obses_pred["proprio"], gt_proprio_resampled)
            visual_loss = self.loss_fn(i_z_obses_pred["visual"], gt_visual_resampled)
            
            total_loss = self.visual_loss_weight * visual_loss + self.proprio_loss_weight * proprio_loss
            loss_time = time.time() - loss_start
            
            print(f"Visual loss: {visual_loss.item():.6f}, Proprio loss: {proprio_loss.item():.6f}")
            print(f"Total loss: {total_loss.item():.6f}")

            # 4. ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸
            backward_start = time.time()
            if self.optimizer is None:
                # ì²« ë²ˆì§¸ í•™ìŠµ ì‹œ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
                params_to_train = [p for p in self.wm.parameters() if p.requires_grad]
                self.optimizer = torch.optim.Adam(params_to_train, lr=self.cfg.get("lr", 1e-4))
            
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()
            backward_time = time.time() - backward_start
            
            print(f"Ensemble LoRA step timing - Rollout: {rollout_time:.4f}s, Encode: {encode_time:.4f}s, Loss: {loss_time:.4f}s, Backward: {backward_time:.4f}s")

            return total_loss.item()

        except Exception as e:
            print(f"Error during ensemble training step: {e}")
            return None
        
        finally:
            # 5. ë©”ëª¨ë¦¬ ì •ë¦¬
            if 'i_z_obses_pred' in locals(): del i_z_obses_pred
            if 'i_z_obses_gt' in locals(): del i_z_obses_gt
            if 'total_loss' in locals(): del total_loss
            torch.cuda.empty_cache()
            print("--- Ensemble LoRA Online Update Complete ---")
    
    def _should_stack_lora_ensemble(self, task_id, input_data=None, target_data=None):
        """
        ì•™ìƒë¸” ê¸°ë°˜ìœ¼ë¡œ LoRA ì ì¸µ ì—¬ë¶€ ê²°ì •
        
        Args:
            task_id: í˜„ì¬ íƒœìŠ¤í¬ ID
            input_data: ì…ë ¥ ë°ì´í„° (ì„ íƒì‚¬í•­)
            target_data: íƒ€ê²Ÿ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            Tuple[bool, Dict]: (ì ì¸µ ì—¬ë¶€, ìµœì  ë©¤ë²„ ì •ë³´)
        """
        # ì•™ìƒë¸” ë©¤ë²„ ìƒíƒœ í™•ì¸
        
        # ğŸ”§ ì•™ìƒë¸” ë©¤ë²„ ìƒì„¸ ì •ë³´ ì¶œë ¥
        if self.ensemble_manager.ensemble_members:
            for task_id_member, member_info in self.ensemble_manager.ensemble_members.items():
                print(f"   - Member Task {task_id_member}: {member_info.get('performance', {}).get('loss', 'N/A')}")
        
        # í˜„ì¬ ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        if not self.ensemble_manager.ensemble_members:
            print("ğŸ“Š No ensemble members available. Stacking new LoRA.")
            return True, None
        
        # ğŸ”§ ì•™ìƒë¸” ë©¤ë²„ê°€ ìˆìœ¼ë©´ í•­ìƒ ì‹¤ì‹œê°„ ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰
        print(f"ğŸ“Š Found {len(self.ensemble_manager.ensemble_members)} ensemble members. Performing real-time evaluation...")
        
        # ì‹¤ì‹œê°„ í‰ê°€ë¥¼ ìœ„í•´ continual_plan.pyì˜ evaluate_members_for_new_task í˜¸ì¶œ
        # ì´ ë©”ì„œë“œëŠ” ê° ë©¤ë²„ì— ëŒ€í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•¨
        print("ğŸ“Š Performing real-time ensemble evaluation...")
        
        # ì‹¤ì‹œê°„ í‰ê°€ëŠ” continual_plan.pyì—ì„œ ìˆ˜í–‰ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ LoRA ì ì¸µë§Œ ìˆ˜í–‰
        print("ğŸ“Š Real-time evaluation will be performed by continual_plan.py")
        print("ğŸ“Š Proceeding with LoRA stacking...")
        return True, None
    
    def _perform_ensemble_lora_stacking(self, task_id, best_member, reason):
        """
        ì•™ìƒë¸” ê¸°ë°˜ LoRA ì ì¸µ ìˆ˜í–‰ (OnlineLoraì— ìœ„ì„)
        
        Args:
            task_id: íƒœìŠ¤í¬ ID
            best_member: ìµœì  ë©¤ë²„ ì •ë³´ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            reason: ì ì¸µ ì´ìœ 
            
        Returns:
            bool: ì ì¸µ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ğŸ”§ OnlineLoraì˜ _perform_lora_stacking ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ
            stacking_success = self.base_online_lora._perform_lora_stacking("ensemble_based", task_id, reason)
            
            if stacking_success:
                # ğŸ”§ base_online_loraì˜ ì¹´ìš´í„° ì§ì ‘ ì—…ë°ì´íŠ¸
                if hasattr(self.base_online_lora, 'stacks_in_current_task'):
                    self.base_online_lora.stacks_in_current_task += 1
                    print(f"ğŸ”§ Updated base_online_lora.stacks_in_current_task to {self.base_online_lora.stacks_in_current_task}")
                
                # ğŸ”§ EnsembleOnlineLoraì˜ ì¹´ìš´í„°ë„ ë™ê¸°í™”
                self.stacks_in_current_task = self.base_online_lora.stacks_in_current_task
                
                # ì ì¸µ ì™„ë£Œ ë¡œê·¸
                print(f"Ensemble-based LoRA stacking completed successfully!")
                print(f"   - Task ID: {task_id}")
                print(f"   - Reason: {reason}")
                print(f"   - Stacks in current task: {self.stacks_in_current_task}/{self.max_stacks_per_task}")
                
                return True
            else:
                print(f"âŒ OnlineLora stacking failed.")
                return False
            
        except Exception as e:
            print(f"âŒ Error during ensemble-based LoRA stacking: {e}")
            return False
    
    def _manage_ensemble_stacking(self, current_loss_value):
        """
        ì•™ìƒë¸” ê¸°ë°˜ ì ì¸µ ê´€ë¦¬ (í–¥í›„ êµ¬í˜„)
        
        Args:
            current_loss_value: í˜„ì¬ loss ê°’
        """
        # í–¥í›„ loss ê¸°ë°˜ ì•™ìƒë¸” ì ì¸µ ë¡œì§ êµ¬í˜„
        pass
    
    def _extract_current_stacked_lora_weights(self):
        """
        í˜„ì¬ ì ì¸µëœ LoRA ê°€ì¤‘ì¹˜ ì¶”ì¶œ (ëª¨ë“  ì ì¸µ íš¨ê³¼ í¬í•¨)
        
        Returns:
            Dict: í˜„ì¬ ì ì¸µëœ LoRA ê°€ì¤‘ì¹˜ (ì²« ë²ˆì§¸ ìŠ¤íƒ - ëª¨ë“  ì ì¸µ íš¨ê³¼ í¬í•¨)
        """
        lora_weights = {}
        
        try:
            # LoRA_ViT_spreadì—ì„œ ì ì¸µëœ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
            if hasattr(self.wm.predictor, 'w_As') and hasattr(self.wm.predictor, 'w_Bs'):
                w_As = self.wm.predictor.w_As
                w_Bs = self.wm.predictor.w_Bs
                
                print(f"ğŸ“Š Extracting LoRA weights from {len(w_As)} total layers...")
                
                # ğŸ”§ ì²« ë²ˆì§¸ ìŠ¤íƒ ì¶”ì¶œ (ëª¨ë“  ì ì¸µ íš¨ê³¼ê°€ í¬í•¨ëœ ë ˆì´ì–´ë“¤)
                # ViTì— 6ê°œì˜ attention blockì´ ìˆê³ , ê° ë¸”ë¡ë§ˆë‹¤ q, v 2ê°œì”© = ì´ 12ê°œ
                layers_per_stack = 12  # ê° LoRA ìŠ¤íƒë‹¹ ë ˆì´ì–´ ìˆ˜
                
                # ì²« ë²ˆì§¸ ìŠ¤íƒ (ëª¨ë“  ì ì¸µ íš¨ê³¼ê°€ í¬í•¨ëœ ë ˆì´ì–´ë“¤) ì¶”ì¶œ
                # ğŸ”§ w + wnewë¥¼ ì¶”ì¶œí•˜ì—¬ ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜ ì €ì¥
                wnew_As = getattr(self.wm.predictor, 'wnew_As', [])
                wnew_Bs = getattr(self.wm.predictor, 'wnew_Bs', [])
                
                for i in range(min(layers_per_stack, len(w_As))):
                    layer_key = f'layer_{i}'
                    
                    # w + wnew ê³„ì‚° (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜)
                    w_A_combined = w_As[i].weight.data.clone().detach()
                    w_B_combined = w_Bs[i].weight.data.clone().detach()
                    
                    if i < len(wnew_As) and i < len(wnew_Bs):
                        w_A_combined += wnew_As[i].weight.data.clone().detach()
                        w_B_combined += wnew_Bs[i].weight.data.clone().detach()
                    
                    lora_weights[layer_key] = {
                        'w_A': w_A_combined,  # w + wnew (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜)
                        'w_B': w_B_combined   # w + wnew (ì‹¤ì œ ì‚¬ìš©ë˜ëŠ” ê°€ì¤‘ì¹˜)
                    }
                
                print(f"âœ… Successfully extracted {len(lora_weights)} LoRA layers with all stacking effects")
                
                # ğŸ”§ ë””ë²„ê¹…: ì ì¸µ íš¨ê³¼ í™•ì¸
                if len(w_As) > layers_per_stack:
                    print(f"ğŸ“Š LoRA Stacking Info:")
                    print(f"   - Total layers: {len(w_As)}")
                    print(f"   - Layers per stack: {layers_per_stack}")
                    print(f"   - Number of stacks: {len(w_As) // layers_per_stack}")
                    print(f"   - Extracted: First stack (all stacking effects included)")
                else:
                    print(f"ğŸ“Š Single LoRA stack detected (no stacking yet)")
                    
            else:
                print(f"âš ï¸  w_As or w_Bs not found in predictor. Available attributes:")
                if hasattr(self.wm.predictor, '__dict__'):
                    for attr in dir(self.wm.predictor):
                        if not attr.startswith('_'):
                            print(f"   - {attr}")
                
        except Exception as e:
            print(f"âŒ Error extracting LoRA weights: {e}")
            import traceback
            traceback.print_exc()
        
        return lora_weights
    
    def _apply_lora_weights(self, lora_weights):
        """
        ì•™ìƒë¸” ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ í˜„ì¬ ëª¨ë¸ì— ì ìš© (ì²« ë²ˆì§¸ ìŠ¤íƒì— ì ìš©)
        
        Args:
            lora_weights: Dict - {'layer_0': {'w_A': tensor, 'w_B': tensor}, ...}
            
        Returns:
            bool: ì ìš© ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not hasattr(self.wm, 'predictor'):
                print(f"âš ï¸  World model doesn't have predictor. Cannot apply LoRA weights.")
                return False
                
            predictor = self.wm.predictor
            
            if not (hasattr(predictor, 'w_As') and hasattr(predictor, 'w_Bs')):
                print(f"âš ï¸  Predictor doesn't have w_As/w_Bs. Cannot apply LoRA weights.")
                return False
            
            # ğŸ”§ ì•™ìƒë¸” ì¶”ë¡ ì„ ìœ„í•œ LoRA ì ìš© ë°©ì‹
            # ì²« ë²ˆì§¸ ìŠ¤íƒ (ëª¨ë“  ì ì¸µ íš¨ê³¼ê°€ í¬í•¨ëœ ë ˆì´ì–´ë“¤)ì— ì•™ìƒë¸” ë©¤ë²„ì˜ ê°€ì¤‘ì¹˜ ì ìš©
            
            # í˜„ì¬ LoRA ìƒíƒœ ë°±ì—…
            original_w_As = [w_A.weight.data.clone() for w_A in predictor.w_As]
            original_w_Bs = [w_B.weight.data.clone() for w_B in predictor.w_Bs]
            
            # ì•™ìƒë¸” ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ ì ìš©
            w_As = predictor.w_As
            w_Bs = predictor.w_Bs
            
            with torch.no_grad():
                # ğŸ”§ ì²« ë²ˆì§¸ ìŠ¤íƒì—ë§Œ ì•™ìƒë¸” ë©¤ë²„ì˜ ê°€ì¤‘ì¹˜ ì ìš©
                layers_per_stack = 12
                
                for i in range(min(layers_per_stack, len(w_As))):
                    layer_key = f'layer_{i}'
                    
                    if layer_key in lora_weights:
                        w_As[i].weight.data.copy_(lora_weights[layer_key]['w_A'])
                        w_Bs[i].weight.data.copy_(lora_weights[layer_key]['w_B'])
                    else:
                        print(f"âš ï¸  Layer {layer_key} not found in lora_weights")
                        # ì›ë˜ ê°€ì¤‘ì¹˜ ë³µì›
                        self._restore_lora_weights(original_w_As, original_w_Bs)
                        return False
            
            print(f"âœ… Successfully applied LoRA weights from ensemble member to first stack")
            return True
            
        except Exception as e:
            print(f"âŒ Error applying LoRA weights: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _restore_lora_weights(self, original_w_As, original_w_Bs):
        """ì›ë˜ LoRA ê°€ì¤‘ì¹˜ ë³µì›"""
        try:
            predictor = self.wm.predictor
            w_As = predictor.w_As
            w_Bs = predictor.w_Bs
            
            with torch.no_grad():
                for i, (orig_A, orig_B) in enumerate(zip(original_w_As, original_w_Bs)):
                    w_As[i].weight.data.copy_(orig_A)
                    w_Bs[i].weight.data.copy_(orig_B)
            
            print(f"âœ… Restored original LoRA weights")
            
        except Exception as e:
            print(f"âŒ Error restoring LoRA weights: {e}")
    
    def _on_lora_stacked(self, steps, loss, task_id, stack_type, reason):
        """
        OnlineLoraì—ì„œ LoRA ì ì¸µ í›„ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜
        
        Args:
            steps: ì ì¸µê¹Œì§€ì˜ ìŠ¤í… ìˆ˜
            loss: í˜„ì¬ loss ê°’
            task_id: íƒœìŠ¤í¬ ID
            stack_type: ì ì¸µ íƒ€ì… ("task_based" ë˜ëŠ” "loss_based")
            reason: ì ì¸µ ì´ìœ 
        """
        print(f"ğŸ”„ LoRA stacked in base model. save_on_stack={self.save_on_stack}")
        print(f"   - Task ID: {task_id}")
        print(f"   - Stack Type: {stack_type}")
        print(f"   - Reason: {reason}")
        print(f"   - Steps: {steps}")
        loss_str = f"{loss:.6f}" if loss is not None else "N/A"
        print(f"   - Loss: {loss_str}")
        
        # ì €ì¥ ì •ì±…: ê¸°ë³¸ì€ ìŠ¤íƒ ì§í›„ ì €ì¥í•˜ì§€ ì•Šê³  íƒœìŠ¤í¬ ì¢…ë£Œ ì‹œ ì €ì¥
        if not self.save_on_stack:
            # ë©”íƒ€ë§Œ ê¸°ë¡í•´ë‘ê³  ì €ì¥ì€ ì—°ê¸°
            self._pending_stack_info = {
                'task_id': task_id,
                'steps': steps,
                'stack_type': stack_type,
                'reason': reason,
                'loss_at_stack': float(loss) if loss is not None else None,
                'timestamp': time.time(),
            }
            print("â„¹ï¸ Deferring ensemble save until task end (finalized state)")
            return
        
        # save_on_stack=Trueì¸ ê²½ìš°ì—ë§Œ ì¦‰ì‹œ ì €ì¥
        try:
            self._save_current_lora_member_impl(task_id=task_id, reason=f"{stack_type}:{reason}", loss_value=loss, steps=steps)
        except Exception as e:
            print(f"âŒ Error in immediate save during _on_lora_stacked: {e}")
            import traceback
            traceback.print_exc()

    def save_current_lora_member(self, task_id: int, reason: str = "task_end") -> bool:
        """íƒœìŠ¤í¬ ì¢…ë£Œ/ìˆ˜ë ´ ì‹œì ì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ì•™ìƒë¸”ì— ì €ì¥."""
        if not self.save_on_task_end:
            print("â„¹ï¸ save_on_task_end is disabled; skipping final save")
            return False
        if task_id in self.created_task_ids:
            print(f"â„¹ï¸ LoRA member for Task {task_id} already saved in this session. Skipping save.")
            return False
        
        # ğŸ”§ stacks_in_current_task ë™ê¸°í™” ë¬¸ì œ í•´ê²°
        # base_online_loraì˜ ì‹¤ì œ ê°’ì„ ì§ì ‘ ì°¸ì¡°
        try:
            actual_stacks = getattr(self.base_online_lora, 'stacks_in_current_task', 0)
            print(f"ğŸ” Checking stacking status: actual_stacks={actual_stacks}")
            
            if actual_stacks == 0:
                print("â„¹ï¸ No LoRA stacking in current task; skipping final save at task end")
                return False
        except Exception as e:
            print(f"âš ï¸  Could not determine stacking status: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œì—ë„ ì €ì¥ì„ ì‹œë„ (ì•ˆì „í•œ í´ë°±)
        
        try:
            last_loss = getattr(self.base_online_lora, 'last_loss', None)
            steps = getattr(self.base_online_lora, 'steps_since_last_stack', 0)
            return self._save_current_lora_member_impl(task_id=task_id, reason=reason, loss_value=last_loss, steps=steps)
        except Exception as e:
            print(f"âŒ Error in save_current_lora_member: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _save_current_lora_member_impl(self, task_id: int, reason: str, loss_value: float, steps: int) -> bool:
        # ğŸ”§ ì ì¸µ ì—†ì´ ë©¤ë²„ë¥¼ ì‚¬ìš© ì¤‘ì´ë©´ ì €ì¥í•˜ì§€ ì•ŠìŒ
        if getattr(self, 'using_member_without_stacking', False):
            print(f"â„¹ï¸  Using member without stacking - skipping ensemble save for Task {task_id}")
            return False
        
        current_weights = self._extract_current_stacked_lora_weights()
        if not current_weights:
            print("âš ï¸ No LoRA weights extracted. Skipping save.")
            return False
        
        # ğŸ”§ ì €ì¥ ì‹œì  fingerprint ê³„ì‚° ë° ì¶œë ¥
        try:
            fingerprint_parts = self._fingerprint_weights(current_weights, sample_layers=4)
            fingerprint = "|".join(fingerprint_parts) if fingerprint_parts else "EMPTY"
            print(f"ğŸ§¬ Save-time fingerprint for Task {task_id}")
        except Exception as e:
            print(f"âš ï¸ Could not compute save-time fingerprint: {e}")
            fingerprint = None
        
        performance = {
            'loss': float(loss_value) if loss_value is not None else float('inf'),
            'steps': int(steps) if steps is not None else 0,
            'stack_type': reason,
        }
        metadata = {
            'reason': reason,
            'saved_at': time.time(),
            'fingerprint': fingerprint,  # ğŸ”§ fingerprint ì €ì¥
        }
        saved = self.ensemble_manager.add_ensemble_member(
            task_id=task_id,
            lora_weights=current_weights,
            performance=performance,
            metadata=metadata,
        )
        if saved:
            print(f"ğŸ’¾ Saved finalized LoRA member for Task {task_id} (reason={reason})")
            self.created_task_ids.add(task_id)
        return saved
    
    
    
    
    
    
    
    
    def check_task_change(self, new_task_id):
        """
        íƒœìŠ¤í¬ ì „í™˜ì„ ê°ì§€í•˜ê³  task_changed í”Œë˜ê·¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        base_online_loraì˜ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë™ê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            new_task_id (int): ìƒˆë¡œìš´ íƒœìŠ¤í¬ ID
            
        Returns:
            bool: íƒœìŠ¤í¬ê°€ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
        """
        # base_online_loraì˜ check_task_change í˜¸ì¶œ
        task_changed = self.base_online_lora.check_task_change(new_task_id)
        
        # ğŸ”§ ëª¨ë“  ìƒíƒœ ë™ê¸°í™”
        self.task_changed = self.base_online_lora.task_changed
        self.current_task_id = self.base_online_lora.current_task_id
        self.stacks_in_current_task = self.base_online_lora.stacks_in_current_task
        self.last_loss = self.base_online_lora.last_loss
        self.last_visual_loss = getattr(self.base_online_lora, "last_visual_loss", None)
        self.last_proprio_loss = getattr(self.base_online_lora, "last_proprio_loss", None)
        
        # ğŸ”§ íƒœìŠ¤í¬ê°€ ë³€ê²½ë˜ë©´ using_member_without_stacking í”Œë˜ê·¸ ë¦¬ì…‹
        if task_changed:
            if hasattr(self, 'using_member_without_stacking'):
                print(f"ğŸ”„ Task changed - resetting using_member_without_stacking flag")
                self.using_member_without_stacking = False
                self.base_member_task_id = None
        
        return task_changed
    
    def reset_task_changed_flag(self):
        """task_changed í”Œë˜ê·¸ë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤."""
        self.base_online_lora.reset_task_changed_flag()
        # ğŸ”§ ëª¨ë“  ìƒíƒœ ë™ê¸°í™”
        self.task_changed = self.base_online_lora.task_changed
        self.current_task_id = self.base_online_lora.current_task_id
        self.stacks_in_current_task = self.base_online_lora.stacks_in_current_task
        self.last_loss = self.base_online_lora.last_loss

    # =====================
    # Debug/Verification Utils
    # =====================

    def _tensor_hash(self, tensor_obj) -> str:
        """Return SHA256 hash of a tensor's CPU bytes (float32 assumed)."""
        try:
            import hashlib
            arr = tensor_obj.detach().cpu().contiguous().numpy()
            return hashlib.sha256(arr.tobytes()).hexdigest()
        except Exception:
            return "NA"

    def _fingerprint_weights(self, lora_weights: dict, sample_layers: int = 4) -> list:
        """Create a simple fingerprint list [hashA0, hashB0, hashA1, hashB1, ...]."""
        if not isinstance(lora_weights, dict) or not lora_weights:
            return []
        keys = sorted(list(lora_weights.keys()))[:sample_layers]
        parts = []
        for k in keys:
            lw = lora_weights.get(k, {})
            if not isinstance(lw, dict):
                continue
            w_a = lw.get('w_A', None)
            w_b = lw.get('w_B', None)
            if w_a is not None:
                parts.append(self._tensor_hash(w_a))
            if w_b is not None:
                parts.append(self._tensor_hash(w_b))
        return parts

    def compute_member_fingerprint(self, task_id: int, sample_layers: int = 4) -> str:
        """Return a compact string fingerprint for a given member."""
        if task_id not in self.ensemble_manager.ensemble_members:
            return ""
        lora_weights = self.ensemble_manager.ensemble_members[task_id].get('lora_weights', {})
        parts = self._fingerprint_weights(lora_weights, sample_layers)
        return "|".join(parts) if parts else "EMPTY"

    def log_all_member_fingerprints(self, sample_layers: int = 4):
        """Print fingerprints for all ensemble members and detect duplicates."""
        if not self.ensemble_manager.ensemble_members:
            print("âš ï¸  No ensemble members available for fingerprinting")
            return
        fingerprints = {}
        for m_task_id, m_info in self.ensemble_manager.ensemble_members.items():
            fp = self.compute_member_fingerprint(m_task_id, sample_layers)
            fingerprints[m_task_id] = fp
            print(f"ğŸ” Ensemble fingerprint - Task {m_task_id}: {fp}")
        # duplicate groups
        fp_groups = {}
        for tid, fp in fingerprints.items():
            fp_groups.setdefault(fp, []).append(tid)
        dups = [grp for grp in fp_groups.values() if len(grp) > 1]
        if dups:
            print(f"âš ï¸  Detected identical fingerprints (sampled layers): {dups}")
        else:
            print("âœ… All ensemble member fingerprints differ (on sampled layers)")

    def verify_ensemble_save_load_integrity(self, tmp_path: str, sample_layers: int = 4) -> bool:
        """Save ensemble to disk, reload into a fresh manager, and compare fingerprints."""
        if not self.ensemble_manager.ensemble_members:
            print("âš ï¸  No ensemble members to verify")
            return False
        # capture current fingerprints
        before = {tid: self.compute_member_fingerprint(tid, sample_layers)
                  for tid in self.ensemble_manager.ensemble_members.keys()}
        # save
        ok = self.ensemble_manager.save_ensemble_to_disk(tmp_path)
        if not ok:
            print("âŒ Failed to save ensemble for integrity check")
            return False
        # load into a fresh manager
        temp_mgr = LoRAEnsembleManager(
            base_model=self.wm,
            max_ensemble_size=self.ensemble_manager.max_ensemble_size,
            cache_dir=self.ensemble_manager.cache_dir,
            max_memory_mb=self.ensemble_manager.max_memory_mb,
        )
        ok2 = temp_mgr.load_ensemble_from_disk(tmp_path)
        if not ok2:
            print("âŒ Failed to load ensemble for integrity check")
            return False
        # compare
        after = {}
        for tid, info in temp_mgr.ensemble_members.items():
            lora_weights = info.get('lora_weights', {})
            parts = self._fingerprint_weights(lora_weights, sample_layers)
            after[tid] = "|".join(parts) if parts else "EMPTY"
        # report
        all_ok = True
        for tid in before.keys():
            b = before.get(tid, "")
            a = after.get(tid, "")
            same = (a == b)
            print(f"ğŸ§ª Integrity [{tid}]: {'OK' if same else 'MISMATCH'}")
            if not same:
                print(f"   before: {b}")
                print(f"   after : {a}")
                all_ok = False
        if all_ok:
            print("âœ… Save/Load integrity verified (fingerprints match)")
        return all_ok

    def summarize_member_differences(self, sample_layers: int = 4):
        """Print simple differences between member weights (hash equality and L1 mean)."""
        import itertools
        if not self.ensemble_manager.ensemble_members:
            print("âš ï¸  No ensemble members available for difference summary")
            return
        members = list(self.ensemble_manager.ensemble_members.items())
        for (tid_a, a), (tid_b, b) in itertools.combinations(members, 2):
            lw_a = a.get('lora_weights', {})
            lw_b = b.get('lora_weights', {})
            keys = sorted(list(set(lw_a.keys()) & set(lw_b.keys())))[:sample_layers]
            same_hash = 0
            total = 0
            l1_accum = 0.0
            for k in keys:
                wa_a = lw_a[k]['w_A']; wb_a = lw_a[k]['w_B']
                wa_b = lw_b[k]['w_A']; wb_b = lw_b[k]['w_B']
                ha = (self._tensor_hash(wa_a), self._tensor_hash(wb_a))
                hb = (self._tensor_hash(wa_b), self._tensor_hash(wb_b))
                same_hash += int(ha == hb)
                total += 1
                try:
                    import torch
                    l1_accum += torch.mean(torch.abs(wa_a.detach().cpu() - wa_b.detach().cpu())).item()
                    l1_accum += torch.mean(torch.abs(wb_a.detach().cpu() - wb_b.detach().cpu())).item()
                except Exception:
                    pass
            hash_eq_ratio = (same_hash / total) if total else 0.0
            avg_l1 = (l1_accum / (2 * total)) if total else 0.0
            print(f"ğŸ” Diff Task {tid_a} vs {tid_b}: hash_eq_ratio={hash_eq_ratio:.2f}, avg_L1={avg_l1:.6f}")
    
    def _apply_lora_weights(self, lora_weights, metadata=None):
        """
        LoRA ê°€ì¤‘ì¹˜ë¥¼ í˜„ì¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤.
        LoRAEnsembleManagerì˜ ë¡œì§ì„ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Args:
            lora_weights: Dict - {'layer_0': {'w_A': tensor, 'w_B': tensor}, ...}
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            bool: ì ìš© ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not hasattr(self.wm, 'predictor'):
                print(f"âš ï¸  World model doesn't have predictor. Cannot apply LoRA weights.")
                return False
                
            predictor = self.wm.predictor
            
            if not (hasattr(predictor, 'w_As') and hasattr(predictor, 'w_Bs')):
                print(f"âš ï¸  Predictor doesn't have w_As/w_Bs. Cannot apply LoRA weights.")
                return False
            
            # ğŸ”§ wì™€ wnew ì´ˆê¸°í™”: ì•™ìƒë¸” ë©¤ë²„ ë¡œë“œ ì „ì— ì´ì „ ê°’ ì œê±°
            with torch.no_grad():
                # w_Asì™€ w_Bs ì´ˆê¸°í™”
                for w_A in predictor.w_As:
                    nn.init.zeros_(w_A.weight)
                for w_B in predictor.w_Bs:
                    nn.init.zeros_(w_B.weight)
                
                # wnew_Asì™€ wnew_Bs ì´ˆê¸°í™”
                if hasattr(predictor, 'wnew_As') and hasattr(predictor, 'wnew_Bs'):
                    for wnew_A in predictor.wnew_As:
                        nn.init.zeros_(wnew_A.weight)
                    for wnew_B in predictor.wnew_Bs:
                        nn.init.zeros_(wnew_B.weight)
            
            print(f"ğŸ”§ Reset all LoRA weights (w, wnew) to zeros before loading ensemble member")
            
            # ì²« ë²ˆì§¸ ìŠ¤íƒ (ëª¨ë“  ì ì¸µ íš¨ê³¼ê°€ í¬í•¨ëœ ë ˆì´ì–´ë“¤)ì— ì•™ìƒë¸” ë©¤ë²„ì˜ ê°€ì¤‘ì¹˜ ì ìš©
            w_As = predictor.w_As
            w_Bs = predictor.w_Bs
            
            layers_per_stack = 12
            
            with torch.no_grad():
                for i in range(min(layers_per_stack, len(w_As))):
                    layer_key = f'layer_{i}'
                    
                    if layer_key in lora_weights:
                        w_As[i].weight.data.copy_(lora_weights[layer_key]['w_A'])
                        w_Bs[i].weight.data.copy_(lora_weights[layer_key]['w_B'])
                    else:
                        print(f"âš ï¸  Layer {layer_key} not found in lora_weights")
                        return False
            
            return True
                
        except Exception as e:
            print(f"âŒ Error applying LoRA weights: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _restore_lora_weights(self, original_w_As, original_w_Bs):
        """
        ì›ë˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë³µì›í•©ë‹ˆë‹¤.
        LoRAEnsembleManagerì˜ ë¡œì§ì„ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Args:
            original_w_As: ë°±ì—…ëœ w_As ë¦¬ìŠ¤íŠ¸
            original_w_Bs: ë°±ì—…ëœ w_Bs ë¦¬ìŠ¤íŠ¸
        """
        try:
            predictor = self.wm.predictor
            w_As = predictor.w_As
            w_Bs = predictor.w_Bs
            
            with torch.no_grad():
                for i, (orig_A, orig_B) in enumerate(zip(original_w_As, original_w_Bs)):
                    w_As[i].weight.data.copy_(orig_A)
                    w_Bs[i].weight.data.copy_(orig_B)
            
            print(f"âœ… Restored original LoRA weights")
            
        except Exception as e:
            print(f"âŒ Error restoring LoRA weights: {e}")
            import traceback
            traceback.print_exc()

    # =====================
    # Ensemble Evaluation Methods (moved from continual_plan.py)
    # =====================

    def perform_task_change_ensemble_selection(self, workspace):
        """
        íƒœìŠ¤í¬ ì „í™˜ ì‹œ ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ëŒ€í•œ ì‹¤ì œ ì„±ëŠ¥ í‰ê°€ë¥¼ í†µí•œ ìµœì  ë©¤ë²„ ì„ íƒ ë° ì ì¸µ
        """
        print(f"ğŸ¯ Performing task change ensemble selection with task-specific evaluation...")

        # ìµœê·¼ í‰ê°€ ìš”ì•½ ì´ˆê¸°í™”
        self.last_ensemble_evaluation_summary = None
        
        ensemble_cfg = workspace.cfg_dict.get("lora", {}).get("ensemble_cfg", {})
        inference_cfg = ensemble_cfg.get("inference", {})
        
        # ì„¤ì • í™•ì¸
        task_change_evaluation = inference_cfg.get("task_change_evaluation", True)
        task_specific_evaluation = inference_cfg.get("task_specific_evaluation", True)
        select_best_member = inference_cfg.get("select_best_member", True)
        stack_on_selected = inference_cfg.get("stack_on_selected", True)
        evaluation_loss_threshold = inference_cfg.get("evaluation_loss_threshold", 0.1)
        
        if not task_change_evaluation:
            print(f"âš ï¸  Task change evaluation disabled, skipping ensemble selection")
            self.last_ensemble_evaluation_summary = {
                "status": "skipped",
                "reason": "task_change_evaluation_disabled",
            }
            return
        
        if not task_specific_evaluation:
            print(f"âš ï¸  Task-specific evaluation disabled, using stored performance")
            self.perform_legacy_ensemble_selection(workspace)
            self.last_ensemble_evaluation_summary = {
                "status": "legacy_selection",
                "reason": "task_specific_evaluation_disabled",
            }
            return
        
        try:
            # 1. ğŸ”§ ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ëŒ€í•œ ê° ë©¤ë²„ì˜ ì‹¤ì œ ì„±ëŠ¥ í‰ê°€
            print(f"ğŸ“Š Evaluating ensemble members for new task...")
            # ë©¤ë²„ê°€ ì—†ìœ¼ë©´ í‰ê°€ë¥¼ ê±´ë„ˆëœ€ (ì´ˆê¸° íƒœìŠ¤í¬ ë“±)
            if len(self.ensemble_manager.ensemble_members) == 0:
                print("â„¹ï¸  No ensemble members available for evaluation (first task or no previous members).")
                print("â„¹ï¸  Proceeding with normal planning without ensemble selection.")
                self.last_ensemble_evaluation_summary = {
                    "status": "skipped",
                    "reason": "no_ensemble_members",
                }
                return
            
            member_performances = self.evaluate_members_for_new_task(workspace)
            
            if not member_performances:
                print(f"âš ï¸  No valid member performances found")
                self.last_ensemble_evaluation_summary = {
                    "status": "skipped",
                    "reason": "no_valid_member_performance",
                }
                return
            
            # 2. ì‹¤ì œ ì„±ëŠ¥ ê¸°ë°˜ ìµœì  ë©¤ë²„ ì„ íƒ
            best_member_task_id, best_performance = min(member_performances, key=lambda x: x[1]['loss'])
            
            print(f"ğŸ“ˆ Task-Specific Performance Results:")
            for task_id, performance in member_performances:
                print(f"   - Task {task_id}: Loss {performance['loss']:.6f}")
            print(f"ğŸ† Best member for new task: Task {best_member_task_id} (Loss: {best_performance['loss']:.6f})")
            
            stacking_triggered = False
            stacking_applied = False
            stacking_reason = "loss_within_threshold"
            
            # 3. loss ì„ê³„ê°’ í™•ì¸ í›„ LoRA ì ì¸µ ì—¬ë¶€ ê²°ì •
            if best_performance['loss'] <= evaluation_loss_threshold:
                print(f"âœ… Best member loss ({best_performance['loss']:.6f}) < threshold ({evaluation_loss_threshold})")
                print(f"ğŸ¯ No LoRA stacking needed - using best member directly")
                
                # ìµœì  ë©¤ë²„ë¥¼ í˜„ì¬ ëª¨ë¸ì— ì ìš©í•˜ë˜ ìƒˆë¡œìš´ LoRA ì ì¸µì€ í•˜ì§€ ì•ŠìŒ
                self.apply_best_member_without_stacking(best_member_task_id)
            else:
                print(f"âš ï¸  Best member loss ({best_performance['loss']:.6f}) > threshold ({evaluation_loss_threshold})")
                print(f"ğŸ”§ LoRA stacking needed - stacking on best member")
                stacking_triggered = True
                stacking_reason = "loss_above_threshold"
                
                if stack_on_selected:
                    # ì„ íƒëœ ë©¤ë²„ ìœ„ì— ìƒˆë¡œìš´ LoRA ì ì¸µ
                    self.stack_on_selected_member(best_member_task_id, workspace)
                    stacking_applied = True
                else:
                    print(f"â„¹ï¸  Stacking on selected member disabled")
                    stacking_reason = "stacking_disabled"
            
            # ğŸ”’ í‰ê°€ ê²°ê³¼ ì €ì¥ (ë¶€ëª¨ í”„ë¡œì„¸ìŠ¤ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆë„ë¡)
            members_summary: List[Dict[str, Any]] = []
            for task_id, performance in member_performances:
                entry: Dict[str, Any] = {"task_id": int(task_id)}
                if isinstance(performance, dict):
                    for key, value in performance.items():
                        if isinstance(value, (int, float, bool)) or value is None:
                            entry[key] = value
                        else:
                            try:
                                entry[key] = float(value)
                            except Exception:
                                entry[key] = str(value)
                members_summary.append(entry)
            
            best_member_summary = {
                "task_id": int(best_member_task_id),
                "performance": {
                    key: (value if isinstance(value, (int, float, bool)) or value is None else str(value))
                    for key, value in (best_performance or {}).items()
                },
            }
            
            self.last_ensemble_evaluation_summary = {
                "status": "evaluated",
                "members": members_summary,
                "best_member": best_member_summary,
                "threshold": evaluation_loss_threshold,
                "stacking_triggered": stacking_triggered,
                "stacking_applied": stacking_applied,
                "stacking_reason": stacking_reason,
                "stack_on_selected": stack_on_selected,
                "select_best_member": select_best_member,
            }
                
        except Exception as e:
            print(f"âŒ Task change ensemble selection failed: {e}")
            import traceback
            traceback.print_exc()
            self.last_ensemble_evaluation_summary = {
                "status": "error",
                "reason": str(e),
            }
    
    def evaluate_members_for_new_task(self, workspace):
        """
        ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ëŒ€í•œ ê° ì•™ìƒë¸” ë©¤ë²„ì˜ ì‹¤ì œ ì„±ëŠ¥ í‰ê°€
        
        Returns:
            List[Tuple[str, Dict]]: (task_id, performance) íŠœí”Œ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸ” Evaluating each member for new task...")
        # ì•™ìƒë¸” ë©¤ë²„ ì—†ìœ¼ë©´ ë°”ë¡œ ë°˜í™˜
        if len(self.ensemble_manager.ensemble_members) == 0:
            print("âš ï¸  No ensemble members to evaluate.")
            return []

        # ë””ë²„ê¹…: í˜„ì¬ í‰ê°€ ëŒ€ìƒ ë©¤ë²„ ID ëª©ë¡ ì¶œë ¥
        try:
            member_ids = list(self.ensemble_manager.ensemble_members.keys())
            print(f"ğŸ§© Ensemble members to evaluate: {member_ids}")
        except Exception:
            pass

        ensemble_cfg = workspace.cfg_dict.get("ensemble_lora", {})
        inference_cfg = ensemble_cfg.get("inference", {})
        evaluation_steps = inference_cfg.get("evaluation_steps", 5)
        
        member_performances = []
        
        # í˜„ì¬ LoRA ìƒíƒœ ë°±ì—…
        original_w_As = None
        original_w_Bs = None
        
        try:
            if hasattr(self.wm.predictor, 'w_As') and hasattr(self.wm.predictor, 'w_Bs'):
                original_w_As = [w_A.weight.data.clone() for w_A in self.wm.predictor.w_As]
                original_w_Bs = [w_B.weight.data.clone() for w_B in self.wm.predictor.w_Bs]
        except Exception as e:
            print(f"âš ï¸  Warning: Could not backup LoRA weights: {e}")
        
        # ë©¤ë²„ ëª©ë¡ì„ ê³ ì • ìŠ¤ëƒ…ìƒ·ìœ¼ë¡œ ë³µì‚¬í•˜ì—¬ í‰ê°€ ì¤‘ ë³€ê²½ìœ¼ë¡œ ì¸í•œ ìŠ¤í‚µ ë°©ì§€
        members_snapshot = list(self.ensemble_manager.ensemble_members.items())
        for task_id, member_info in members_snapshot:
            try:
                print(f"ğŸ“Š Evaluating member Task {task_id} for new task...")
                
                # ğŸ”§ ê° ë©¤ë²„ í‰ê°€ ì „ì— ì™„ì „íˆ ì´ˆê¸° ìƒíƒœë¡œ ë¦¬ì…‹ (ì´ì „ ë©¤ë²„ ì˜í–¥ ì™„ì „ ì œê±°)
                if hasattr(self.wm.predictor, 'w_As') and hasattr(self.wm.predictor, 'w_Bs'):
                    for w_A in self.wm.predictor.w_As:
                        nn.init.zeros_(w_A.weight)
                    for w_B in self.wm.predictor.w_Bs:
                        nn.init.zeros_(w_B.weight)
                
                if hasattr(self.wm.predictor, 'wnew_As') and hasattr(self.wm.predictor, 'wnew_Bs'):
                    for wnew_A in self.wm.predictor.wnew_As:
                        nn.init.zeros_(wnew_A.weight)
                    for wnew_B in self.wm.predictor.wnew_Bs:
                        nn.init.zeros_(wnew_B.weight)
                
                print(f"ğŸ”§ Reset all LoRA weights (w, wnew) to zeros before loading ensemble member {task_id}")
                
                # í•´ë‹¹ ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ì ìš©
                lora_weights = member_info['lora_weights']
                # ë¡œë“œ ì „í›„ ì§€ë¬¸ ë¹„êµë¥¼ ìœ„í•œ ì €ì¥ ì‹œì  fingerprint ê°€ì ¸ì˜¤ê¸°(ìˆë‹¤ë©´)
                saved_fp = None
                try:
                    saved_fp = member_info.get('metadata', {}).get('fingerprint', None)
                except Exception:
                    saved_fp = None
                # í”Œë˜íŠ¼ ì €ì¥ë³¸ì´ë©´ wnew ì´ì¤‘ ì ìš© ë°©ì§€ë¥¼ ìœ„í•´ ë©”íƒ€ ì „ë‹¬
                meta_for_apply = member_info.get('metadata', {}) if isinstance(member_info, dict) else {}
                success = self._apply_lora_weights(lora_weights, metadata=meta_for_apply)
                
                if not success:
                    print(f"âŒ Failed to apply LoRA weights for member {task_id}")
                    continue
                # ì ìš© í›„ í˜„ì¬ ëª¨ë¸ ì²« ìŠ¤íƒ ì§€ë¬¸ ê³„ì‚° ë° ëŒ€ì¡° í™•ì¸
                try:
                    applied_weights = self._extract_current_stacked_lora_weights()
                    applied_fp_parts = self._fingerprint_weights(applied_weights, sample_layers=4)
                    applied_fp = "|".join(applied_fp_parts) if applied_fp_parts else "EMPTY"
                    
                    if saved_fp:
                        match_status = "âœ… MATCH" if applied_fp == saved_fp else "âŒ MISMATCH"
                        print(f"ğŸ” Fingerprint comparison: {match_status}")
                        if applied_fp != saved_fp:
                            print(f"âš ï¸  WARNING: Task {task_id} fingerprint mismatch detected!")
                    else:
                        print(f"ğŸ§¬ Load-time fingerprint (sampled): {applied_fp}")
                        print(f"âš ï¸  No saved fingerprint available for Task {task_id}")
                except Exception as e:
                    print(f"âš ï¸  Could not compute load-time fingerprint: {e}")
                
                # ìƒˆë¡œìš´ íƒœìŠ¤í¬ì— ëŒ€í•œ ì‹¤ì œ ì„±ëŠ¥ í‰ê°€
                # evaluator.pyì˜ eval_actionsë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í‰ê°€
                performance = self.evaluate_member_for_current_task(
                    workspace, actions=None, evaluation_steps=evaluation_steps
                )
                
                if performance is not None:
                    member_performances.append((task_id, performance))
                    print(f"   - Task {task_id}: Loss {performance['loss']:.6f}")
                else:
                    print(f"   - Task {task_id}: Evaluation failed")
                # ê° ë©¤ë²„ í‰ê°€ í›„ ì›ë˜ LoRA ìƒíƒœ ë³µì› (ë…ë¦½ì„± ë³´ì¥)
                if original_w_As is not None and original_w_Bs is not None:
                    self._restore_lora_weights(original_w_As, original_w_Bs)
                
            except Exception as e:
                print(f"âŒ Error evaluating member {task_id}: {e}")
                continue
        
        # ì›ë˜ LoRA ìƒíƒœ ë³µì›
        if original_w_As is not None and original_w_Bs is not None:
            self._restore_lora_weights(original_w_As, original_w_Bs)
        
        print(f"âœ… Evaluated {len(member_performances)} members for new task")
        return member_performances
    
    def evaluate_member_for_current_task(self, workspace, actions=None, evaluation_steps=5):
        """
        evaluator.pyì˜ eval_actionsë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ ì•™ìƒë¸” ë©¤ë²„ í‰ê°€
        (ë§ê° ì¸¡ì •ê³¼ ë™ì¼í•˜ê²Œ ë‹¨ì¼ í‰ê°€ë§Œ ìˆ˜í–‰, MPC planning ì—†ìŒ)
        
        Args:
            workspace: PlanWorkspace ì¸ìŠ¤í„´ìŠ¤
            actions: ì‚¬ìš©ë˜ì§€ ì•ŠìŒ (ë§ê° ì¸¡ì •ê³¼ ë™ì¼í•˜ê²Œ zero actions ì‚¬ìš©)
            evaluation_steps: ì‚¬ìš©ë˜ì§€ ì•ŠìŒ
            
        Returns:
            Dict: ì„±ëŠ¥ ì§€í‘œ
        """
        try:
            # ë§ê° ì¸¡ì •ê³¼ ë™ì¼í•œ ë°©ì‹: MPC planning ì—†ì´ ë‹¨ìˆœ í‰ê°€ë§Œ ìˆ˜í–‰
            # evaluate_loss_only()ì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
            if workspace.gt_actions is not None:
                actions_eval = (
                    workspace.gt_actions.to(device=workspace.device, dtype=torch.float32).detach()
                )
                action_len = np.full(actions_eval.shape[0], actions_eval.shape[1])
            else:
                batch_size = workspace.obs_0["visual"].shape[0]
                horizon = 1 if workspace.goal_H <= 0 else workspace.goal_H
                actions_eval = torch.zeros(
                    (batch_size, horizon, workspace.action_dim),
                    device=workspace.device,
                    dtype=torch.float32,
                )
                action_len = np.full(batch_size, horizon)

            workspace.evaluator.force_recenter_for_next_rollout()
            logs, successes, e_obses, _ = workspace.evaluator.eval_actions(
                actions_eval,
                action_len,
                save_video=False,
                filename="ensemble_eval",
                learning_enabled=False,
            )
            
            # logsì—ì„œ ì§ì ‘ loss ê°’ ì¶”ì¶œ (evaluator.pyì—ì„œ ì´ë¯¸ ê³„ì‚°ë˜ì–´ ìˆìŒ)
            visual_loss = logs.get('visual_loss', None)
            proprio_loss = logs.get('proprio_loss', None)
            total_loss = logs.get('total_loss', None)
            
            # total_lossê°€ ì—†ìœ¼ë©´ visual_lossì™€ proprio_loss í•©ì‚° ì‹œë„
            if total_loss is None:
                if visual_loss is not None and proprio_loss is not None:
                    total_loss = visual_loss + proprio_loss
                elif visual_loss is not None:
                    total_loss = visual_loss
                else:
                    print("   âŒ Failed to extract loss from logs!")
                    return None
            
            visual_str = f"{visual_loss:.6f}" if visual_loss is not None else "N/A"
            proprio_str = f"{proprio_loss:.6f}" if proprio_loss is not None else "N/A"
            print(f"   ğŸ“Š Ensemble evaluation loss: total={total_loss:.6f}, visual={visual_str}, proprio={proprio_str}")
            
            return {
                'loss': total_loss,
                'visual_loss': visual_loss if visual_loss is not None else (total_loss * 0.8),
                'proprio_loss': proprio_loss if proprio_loss is not None else (total_loss * 0.2),
                'success_rate': logs.get('success_rate', 0),
                'chamfer_distance': logs.get('chamfer_distance', None)
            }
            
        except Exception as e:
            print(f"âŒ Error evaluating member: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def _parse_loss_from_output(self, output_text):
        """
        í„°ë¯¸ë„ ì¶œë ¥ì—ì„œ loss ê°’ì„ íŒŒì‹±í•©ë‹ˆë‹¤.
        
        Args:
            output_text: ìº¡ì²˜ëœ í„°ë¯¸ë„ ì¶œë ¥
            
        Returns:
            float: íŒŒì‹±ëœ loss ê°’ ë˜ëŠ” None
        """
        try:
            # ë°©ë²• 1: PARSED_LOSS_START ë§ˆì»¤ ì‚¬ìš©
            if "PARSED_LOSS_START:" in output_text:
                start_marker = "PARSED_LOSS_START:"
                end_marker = ":PARSED_LOSS_END"
                
                start_idx = output_text.find(start_marker)
                if start_idx != -1:
                    start_idx += len(start_marker)
                    end_idx = output_text.find(end_marker, start_idx)
                    if end_idx != -1:
                        loss_str = output_text[start_idx:end_idx]
                        return float(loss_str)
            
            # ë°©ë²• 2: "Total loss: " íŒ¨í„´ ì‚¬ìš©
            if "Total loss: " in output_text:
                lines = output_text.split('\n')
                for line in lines:
                    if "Total loss: " in line:
                        # "Total loss: 0.071619" í˜•íƒœì—ì„œ ìˆ«ì ì¶”ì¶œ
                        parts = line.split("Total loss: ")
                        if len(parts) > 1:
                            loss_str = parts[1].strip()
                            return float(loss_str)
            
            return None
            
        except Exception as e:
            print(f"   âš ï¸  Error parsing loss: {e}")
            return None
    
    def apply_best_member_without_stacking(self, best_member_task_id):
        """
        ìµœì  ë©¤ë²„ë¥¼ í˜„ì¬ ëª¨ë¸ì— ì ìš©í•˜ë˜ ìƒˆë¡œìš´ LoRA ì ì¸µì€ í•˜ì§€ ì•ŠìŒ
        wnewëŠ” 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì˜¨ë¼ì¸ í•™ìŠµì€ ê°€ëŠ¥í•˜ê²Œ í•¨ (ë‹¨, ì•™ìƒë¸” ë©¤ë²„ë¡œ ì €ì¥í•˜ì§€ ì•ŠìŒ)
        
        Args:
            best_member_task_id: ìµœì  ë©¤ë²„ì˜ task_id
        """
        try:
            print(f"ğŸ¯ Applying best member {best_member_task_id} without stacking...")
            
            if best_member_task_id in self.ensemble_manager.ensemble_members:
                member_info = self.ensemble_manager.ensemble_members[best_member_task_id]
                lora_weights = member_info['lora_weights']
                
                # ìµœì  ë©¤ë²„ì˜ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ì ìš© (ì ì¸µ ì—†ì´)
                success = self._apply_lora_weights(lora_weights)
                
                if success:
                    # ğŸ”§ wnewë¥¼ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì˜¨ë¼ì¸ í•™ìŠµ ê°€ëŠ¥í•˜ê²Œ í•¨
                    if hasattr(self.wm.predictor, 'wnew_As') and hasattr(self.wm.predictor, 'wnew_Bs'):
                        import torch.nn as nn
                        for wnew_A in self.wm.predictor.wnew_As:
                            nn.init.zeros_(wnew_A.weight)
                        for wnew_B in self.wm.predictor.wnew_Bs:
                            nn.init.zeros_(wnew_B.weight)
                        print(f"ğŸ”§ Initialized wnew to zeros - online learning enabled without stacking")
                    
                    # ğŸ”§ ì ì¸µ ì—†ì´ ì‚¬ìš© ì¤‘ì„ì„ í”Œë˜ê·¸ë¡œ í‘œì‹œ (ì•™ìƒë¸” ì €ì¥ ë°©ì§€)
                    self.using_member_without_stacking = True
                    self.base_member_task_id = best_member_task_id
                    
                    print(f"âœ… Successfully applied best member's LoRA weights without stacking")
                    print(f"ğŸ¯ Using best member directly for new task (online learning enabled)")
                else:
                    print(f"âŒ Failed to apply best member's LoRA weights")
            else:
                print(f"âŒ Best member task {best_member_task_id} not found in ensemble members")
                
        except Exception as e:
            print(f"âŒ Error applying best member without stacking: {e}")
            import traceback
            traceback.print_exc()
    
    def perform_legacy_ensemble_selection(self, workspace):
        """
        ê¸°ì¡´ ë°©ì‹ì˜ ì•™ìƒë¸” ì„ íƒ (ì €ì¥ëœ ì„±ëŠ¥ ê¸°ë°˜)
        í´ë°±ìš© ë©”ì„œë“œ
        """
        print(f"ğŸ”„ Using legacy ensemble selection (stored performance)...")
        
        try:
            # ê¸°ì¡´ ë°©ì‹: ì €ì¥ëœ ì„±ëŠ¥ìœ¼ë¡œ ìµœì  ë©¤ë²„ ì„ íƒ
            best_member = self.ensemble_manager.get_best_member(
                input_data=workspace.obs_0,
                metric='loss'
            )
            
            if best_member is not None:
                best_task_id = best_member['task_id']
                print(f"ğŸ† Selected best ensemble member (legacy): Task {best_task_id}")
                
                # ì„ íƒëœ ë©¤ë²„ ìœ„ì— ìƒˆë¡œìš´ LoRA ì ì¸µ
                self.stack_on_selected_member(best_task_id, workspace)
            else:
                print(f"âš ï¸  No suitable ensemble member found (legacy)")
                
        except Exception as e:
            print(f"âŒ Legacy ensemble selection failed: {e}")
    
    def apply_latest_member_without_evaluation(self, workspace):
        """
        ëŒ€ì¡°êµ° ëª¨ë“œ: ì•™ìƒë¸” í‰ê°€ ì—†ì´ ê°€ì¥ ìµœê·¼ ë©¤ë²„ë§Œ ë¡œë“œí•˜ì—¬ ì‚¬ìš©
        
        Args:
            workspace: PlanWorkspace ì¸ìŠ¤í„´ìŠ¤
        """
        try:
            print(f"ğŸ”¬ Control Group Mode: Applying latest member without ensemble evaluation...")
            
            if not hasattr(self, 'ensemble_manager') or self.ensemble_manager is None:
                print("âš ï¸  No ensemble manager available")
                return
            
            if len(self.ensemble_manager.ensemble_members) == 0:
                print("â„¹ï¸  No ensemble members available (first task). Proceeding with normal learning.")
                return
            
            # ê°€ì¥ ìµœê·¼ ë©¤ë²„ ì°¾ê¸°
            latest_task_id = max(self.ensemble_manager.ensemble_members.keys())
            latest_member = self.ensemble_manager.ensemble_members[latest_task_id]
            
            print(f"ğŸ“Œ Latest member: Task {latest_task_id}")
            
            # ìµœê·¼ ë©¤ë²„ ìœ„ì— ìƒˆë¡œìš´ LoRA ì ì¸µ
            self.stack_on_selected_member(latest_task_id, workspace)
            
            print(f"âœ… Control Group: Applied latest member (Task {latest_task_id}) and stacked new LoRA")
            
        except Exception as e:
            print(f"âŒ Control Group: Failed to apply latest member: {e}")
            import traceback
            traceback.print_exc()
    
    def stack_on_selected_member(self, selected_task_id, workspace):
        """
        ì„ íƒëœ ë©¤ë²„ ìœ„ì— ìƒˆë¡œìš´ LoRA ì ì¸µ
        
        LoRAEnsembleManagerì˜ ê¸°ì¡´ _apply_lora_weights ë©”ì„œë“œë¥¼ í™œìš©
        
        Args:
            selected_task_id: ì„ íƒëœ ë©¤ë²„ì˜ task_id
            workspace: PlanWorkspace ì¸ìŠ¤í„´ìŠ¤
        """
        try:
            print(f"ğŸ”§ Stacking new LoRA on selected member: Task {selected_task_id}")
            
            # ì„ íƒëœ ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ í˜„ì¬ ëª¨ë¸ì— ì ìš©
            if selected_task_id in self.ensemble_manager.ensemble_members:
                member_info = self.ensemble_manager.ensemble_members[selected_task_id]
                lora_weights = member_info['lora_weights']
                
                # ğŸ”§ EnsembleOnlineLoraì˜ _apply_lora_weights ë©”ì„œë“œ ì‚¬ìš©
                success = self._apply_lora_weights(lora_weights)
                
                if success:
                    print(f"âœ… Successfully applied selected member's LoRA weights")
                    print(f"ğŸ”„ New LoRA will be stacked on top of selected member")
                    
                    # ì‹¤ì œ LoRA ì ì¸µ ìˆ˜í–‰ (OnlineLora ê²½ë¡œë¡œ ìœ„ì„)
                    try:
                        # í˜„ì¬ íƒœìŠ¤í¬ IDë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì „ë‹¬
                        current_task_id = getattr(self, 'current_task_id', None)
                        if current_task_id is None:
                            current_task_id = selected_task_id
                        # ì¶”ì ìš©: ì„ íƒëœ ë©¤ë²„ ID ê¸°ë¡
                        try:
                            setattr(self, 'last_selected_member_task_id', selected_task_id)
                        except Exception:
                            pass
                        stacking_success = self._perform_ensemble_lora_stacking(
                            task_id=current_task_id,
                            best_member=None,
                            reason="task_change_eval"
                        )
                        if stacking_success:
                            print("âœ… Triggered actual LoRA stacking on selected member")
                            # ìŠ¤íƒ íˆìŠ¤í† ë¦¬ì— ì„ íƒ ë©¤ë²„ IDë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì¶”ê°€(ì´ì¤‘ ë³´í˜¸)
                            try:
                                if hasattr(self, 'stack_history') and isinstance(self.stack_history, list):
                                    self.stack_history.append({
                                        'type': 'task_based',
                                        'reason': 'task_change_eval',
                                        'selected_member_task_id': selected_task_id,
                                        'timestamp': time.time(),
                                    })
                            except Exception:
                                pass
                        else:
                            print("âŒ Failed to trigger actual LoRA stacking on selected member")
                    except Exception as e:
                        print(f"âŒ Error triggering actual LoRA stacking: {e}")
                else:
                    print(f"âŒ Failed to apply selected member's LoRA weights")
            else:
                print(f"âŒ Selected task {selected_task_id} not found in ensemble members")
                
        except Exception as e:
            print(f"âŒ Error stacking on selected member: {e}")
            import traceback
            traceback.print_exc()
