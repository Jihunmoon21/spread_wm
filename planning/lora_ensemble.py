import torch
import time
import os
import gzip
import pickle
from typing import Dict, List, Tuple, Optional
from collections import OrderedDict, deque
from utils import move_to_device


class LoRAEnsembleManager:
    """
    LoRA ì•™ìƒë¸”ì„ ê´€ë¦¬í•˜ëŠ” í•µì‹¬ í´ë˜ìŠ¤
    
    ê¸°ëŠ¥:
    - ì—¬ëŸ¬ LoRA ëª¨ë¸ì„ ì•™ìƒë¸”ë¡œ ê´€ë¦¬
    - ë©”ëª¨ë¦¬ íš¨ìœ¨ì ì¸ ì €ì¥/ë¡œë“œ
    - ì•™ìƒë¸” ë©¤ë²„ ì„±ëŠ¥ ì¶”ì 
    - LoRA ê°€ì¤‘ì¹˜ í†µí•©
    """
    
    def __init__(self, base_model, max_ensemble_size: int = 11, 
                 cache_dir: str = "./lora_cache", max_memory_mb: int = 200):
        self.base_model = base_model
        self.max_ensemble_size = max_ensemble_size
        self.cache_dir = cache_dir
        self.max_memory_mb = max_memory_mb
        
        # ì•™ìƒë¸” ë©¤ë²„ ê´€ë¦¬
        self.ensemble_members = OrderedDict()  # {task_id: member_info}
        self.current_task_id = 0
        self.memory_usage = 0
        
        # ì„±ëŠ¥ ì¶”ì 
        self.performance_history = {}  # {task_id: [performance_scores]}
        self.access_frequency = {}   # {task_id: access_count}
        
        # ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(cache_dir, exist_ok=True)
        
        print(f"LoRAEnsembleManager initialized:")
        print(f"  - Max ensemble size: {max_ensemble_size}")
        print(f"  - Cache directory: {cache_dir}")
        print(f"  - Max memory usage: {max_memory_mb}MB")
    
    def add_ensemble_member(self, task_id: int, lora_weights: Dict, 
                           performance: Dict, metadata: Optional[Dict] = None) -> bool:
        """
        ìƒˆë¡œìš´ ì•™ìƒë¸” ë©¤ë²„ ì¶”ê°€
        
        Args:
            task_id: íƒœìŠ¤í¬ ID
            lora_weights: LoRA ê°€ì¤‘ì¹˜ ë”•ì…”ë„ˆë¦¬
            performance: ì„±ëŠ¥ ì§€í‘œ {'loss': float, 'accuracy': float, ...}
            metadata: ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            
        Returns:
            bool: ì¶”ê°€ ì„±ê³µ ì—¬ë¶€
        """
        try:
            # ì•™ìƒë¸” í¬ê¸° ì œí•œ í™•ì¸
            if len(self.ensemble_members) >= self.max_ensemble_size:
                self._remove_oldest_member()
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ í™•ì¸
            lora_size_mb = self._calculate_lora_size(lora_weights)
            if self.memory_usage + lora_size_mb > self.max_memory_mb:
                self._cleanup_memory()
            
            # ì•™ìƒë¸” ë©¤ë²„ ì •ë³´ ìƒì„±
            member_info = {
                'task_id': task_id,
                'lora_weights': lora_weights,
                'performance': performance,
                'metadata': metadata or {},
                'created_at': time.time(),
                'last_accessed': time.time(),
                'access_count': 0,
                'size_mb': lora_size_mb
            }
            
            # ì•ˆì „í•œ ê¹Šì€ ë³µì‚¬ë¡œ ì €ì¥ (ì°¸ì¡° ì˜¤ì—¼ ë°©ì§€)
            safe_weights = {}
            for layer_key, layer_vals in lora_weights.items():
                if isinstance(layer_vals, dict):
                    w_A = layer_vals.get('w_A', None)
                    w_B = layer_vals.get('w_B', None)
                    if w_A is not None and w_B is not None:
                        safe_weights[layer_key] = {
                            'w_A': w_A.clone().detach().cpu(),
                            'w_B': w_B.clone().detach().cpu(),
                        }
            member_info['lora_weights'] = safe_weights

            # ì•™ìƒë¸”ì— ì¶”ê°€
            self.ensemble_members[task_id] = member_info
            self.memory_usage += lora_size_mb
            
            # ì„±ëŠ¥ íˆìŠ¤í† ë¦¬ ì—…ë°ì´íŠ¸
            if task_id not in self.performance_history:
                self.performance_history[task_id] = []
            self.performance_history[task_id].append(performance)
            
            # ì ‘ê·¼ ë¹ˆë„ ì´ˆê¸°í™”
            self.access_frequency[task_id] = 0
            
            print(f"âœ… Added ensemble member: Task {task_id}")
            print(f"   - Performance: {performance}")
            print(f"   - Size: {lora_size_mb:.2f}MB")
            print(f"   - Total members: {len(self.ensemble_members)}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Failed to add ensemble member: {e}")
            return False
    
    def get_best_member(self, input_data: torch.Tensor, 
                       metric: str = 'loss') -> Optional[Dict]:
        """
        ì…ë ¥ ë°ì´í„°ì— ëŒ€í•´ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì˜ ì•™ìƒë¸” ë©¤ë²„ ì„ íƒ
        
        Args:
            input_data: ì…ë ¥ ë°ì´í„°
            metric: ì„±ëŠ¥ ì§€í‘œ ('loss', 'accuracy', etc.)
            
        Returns:
            Dict: ìµœì ì˜ ì•™ìƒë¸” ë©¤ë²„ ì •ë³´
        """
        if not self.ensemble_members:
            print("âš ï¸  No ensemble members available")
            return None
        
        best_member = None
        best_score = float('inf') if metric == 'loss' else float('-inf')
        
        for task_id, member_info in self.ensemble_members.items():
            # ì ‘ê·¼ ë¹ˆë„ ì—…ë°ì´íŠ¸
            self.access_frequency[task_id] += 1
            member_info['last_accessed'] = time.time()
            member_info['access_count'] += 1
            
            # ì„±ëŠ¥ ì ìˆ˜ ê³„ì‚°
            performance = member_info['performance']
            score = performance.get(metric, float('inf'))
            
            # ìµœì  ì„±ëŠ¥ í™•ì¸
            is_better = (score < best_score) if metric == 'loss' else (score > best_score)
            if is_better:
                best_score = score
                best_member = member_info
        
        if best_member:
            print(f"ğŸ¯ Selected best member: Task {best_member['task_id']} "
                  f"(score: {best_score:.6f})")
        
        return best_member
    
    def evaluate_member_performance(self, member_info: Dict, trans_obs_0, actions, target_data) -> Dict:
        """
        íŠ¹ì • ì•™ìƒë¸” ë©¤ë²„ì˜ ì‹¤ì œ ì„±ëŠ¥ í‰ê°€ (ì›”ë“œ ëª¨ë¸ rollout ì‚¬ìš©)
        
        Args:
            member_info: ì•™ìƒë¸” ë©¤ë²„ ì •ë³´
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            target_data: íƒ€ê²Ÿ ë°ì´í„°
            
        Returns:
            Dict: ì„±ëŠ¥ ì§€í‘œ
        """
        try:
            # í•´ë‹¹ ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ë¡œë“œ
            lora_weights = member_info['lora_weights']
            
            # í˜„ì¬ LoRA ìƒíƒœ ë°±ì—…
            original_w_As = None
            original_w_Bs = None
            
            if hasattr(self.wm.predictor, 'w_As') and hasattr(self.wm.predictor, 'w_Bs'):
                original_w_As = [w_A.weight.data.clone() for w_A in self.wm.predictor.w_As]
                original_w_Bs = [w_B.weight.data.clone() for w_B in self.wm.predictor.w_Bs]
            
            # ëª¨ë¸ì— LoRA ê°€ì¤‘ì¹˜ ì ìš©
            success = self._apply_lora_weights(lora_weights)
            
            if not success:
                print(f"âŒ Failed to apply LoRA weights for member {member_info['task_id']}")
                return {'loss': float('inf'), 'mae': float('inf'), 'mse': float('inf')}
            
            # ğŸ”§ ì›”ë“œ ëª¨ë¸ rollout ìˆ˜í–‰
            with torch.no_grad():
                i_z_obses_pred, _ = self.wm.rollout(obs_0=trans_obs_0, act=actions)
                
                # ì†ì‹¤ ê³„ì‚° (visualê³¼ proprio ëª¨ë‘ ê³ ë ¤)
                loss_fn = torch.nn.MSELoss()
                
                # Visual loss
                visual_loss = loss_fn(i_z_obses_pred["visual"], target_data["visual"])
                
                # Proprio loss  
                proprio_loss = loss_fn(i_z_obses_pred["proprio"], target_data["proprio"])
                
                # Total loss (ê°€ì¤‘ì¹˜ ì ìš©)
                total_loss = visual_loss + 0.3 * proprio_loss
                
                # ì¶”ê°€ ì„±ëŠ¥ ì§€í‘œ ê³„ì‚°
                visual_mae = torch.mean(torch.abs(i_z_obses_pred["visual"] - target_data["visual"]))
                proprio_mae = torch.mean(torch.abs(i_z_obses_pred["proprio"] - target_data["proprio"]))
                
                performance = {
                    'loss': total_loss.item(),
                    'visual_loss': visual_loss.item(),
                    'proprio_loss': proprio_loss.item(),
                    'visual_mae': visual_mae.item(),
                    'proprio_mae': proprio_mae.item(),
                    'evaluated_at': time.time()
                }
                
                print(f"ğŸ“Š Member {member_info['task_id']} performance: "
                      f"Total Loss {total_loss.item():.6f}, Visual {visual_loss.item():.6f}, Proprio {proprio_loss.item():.6f}")
            
            # ì›ë˜ LoRA ìƒíƒœ ë³µì›
            if original_w_As is not None and original_w_Bs is not None:
                self._restore_lora_weights(original_w_As, original_w_Bs)
                
                return performance
                
        except Exception as e:
            print(f"âŒ Error evaluating member {member_info['task_id']}: {e}")
            return {'loss': float('inf'), 'mae': float('inf'), 'mse': float('inf')}
    
    def ensemble_predict(self, trans_obs_0, actions, method: str = 'weighted_average'):
        """
        ì•™ìƒë¸” ì¶”ë¡  ìˆ˜í–‰ (ì›”ë“œ ëª¨ë¸ rollout ì‚¬ìš©)
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            method: ì•™ìƒë¸” ë°©ë²• ('weighted_average', 'best_only', 'all')
            
        Returns:
            torch.Tensor: ì•™ìƒë¸” ì˜ˆì¸¡ ê²°ê³¼
        """
        if not self.ensemble_members:
            print("âš ï¸  No ensemble members available for prediction")
            return None
        
        predictions = []
        weights = []
        
        print(f"ğŸ”® Ensemble prediction with {len(self.ensemble_members)} members using {method}")
        
        # í˜„ì¬ LoRA ìƒíƒœ ë°±ì—… (ì•™ìƒë¸” ì¶”ë¡  í›„ ë³µì›ìš©)
        original_w_As = None
        original_w_Bs = None
        
        try:
            if hasattr(self.wm.predictor, 'w_As') and hasattr(self.wm.predictor, 'w_Bs'):
                original_w_As = [w_A.weight.data.clone() for w_A in self.wm.predictor.w_As]
                original_w_Bs = [w_B.weight.data.clone() for w_B in self.wm.predictor.w_Bs]
        except Exception as e:
            print(f"âš ï¸  Warning: Could not backup LoRA weights: {e}")
            original_w_As = None
            original_w_Bs = None
        
        for task_id, member_info in self.ensemble_members.items():
            try:
                # ê° ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ëª¨ë¸ì— ë¡œë“œ
                lora_weights = member_info['lora_weights']
                
                # ëª¨ë¸ì— LoRA ê°€ì¤‘ì¹˜ ì ìš©
                success = self._apply_lora_weights(lora_weights)
                
                if not success:
                    print(f"âŒ Failed to apply LoRA weights for member {task_id}")
                    continue
                
                # ğŸ”§ ì›”ë“œ ëª¨ë¸ rollout ìˆ˜í–‰
                with torch.no_grad():
                    i_z_obses_pred, _ = self.wm.rollout(obs_0=trans_obs_0, act=actions)
                    predictions.append(i_z_obses_pred)
                    
                    # ê°€ì¤‘ì¹˜ ê³„ì‚° (ì„±ëŠ¥ ê¸°ë°˜)
                    performance = member_info['performance']
                    loss = performance.get('loss', 1.0)
                    weight = 1.0 / (loss + 1e-8)
                    weights.append(weight)
                    
                    print(f"   - Member {task_id}: Loss {loss:.6f}, Weight {weight:.6f}")
                    
            except Exception as e:
                print(f"âŒ Error in member {task_id} prediction: {e}")
                continue
        
        # ì›ë˜ LoRA ìƒíƒœ ë³µì›
        if original_w_As is not None and original_w_Bs is not None:
            self._restore_lora_weights(original_w_As, original_w_Bs)
        
        if not predictions:
            print("âŒ No valid predictions from ensemble members")
            return None
        
        # ì•™ìƒë¸” ë°©ë²•ì— ë”°ë¥¸ ìµœì¢… ì˜ˆì¸¡ ê³„ì‚°
        if method == 'weighted_average':
            return self._weighted_average_predictions(predictions, weights)
        elif method == 'best_only':
            best_idx = weights.index(max(weights))
            return predictions[best_idx]
        elif method == 'all':
            return torch.mean(torch.stack(predictions), dim=0)
        else:
            raise ValueError(f"Unknown ensemble method: {method}")
    
    def _weighted_average_predictions(self, predictions: List[torch.Tensor], 
                                    weights: List[float]) -> torch.Tensor:
        """ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ì˜ˆì¸¡ ê²°ê³¼ í†µí•©"""
        if not predictions or not weights:
            return None
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(weights)
        normalized_weights = [w / total_weight for w in weights]
        
        # ê°€ì¤‘ í‰ê·  ê³„ì‚°
        weighted_sum = None
        for pred, weight in zip(predictions, normalized_weights):
            if weighted_sum is None:
                weighted_sum = weight * pred
            else:
                weighted_sum += weight * pred
        
        print(f"ğŸ”— Weighted average prediction computed with {len(predictions)} members")
        return weighted_sum
    
    def evaluate_ensemble_performance(self, trans_obs_0, actions, target_data) -> Dict:
        """
        ì „ì²´ ì•™ìƒë¸”ì˜ ì„±ëŠ¥ í‰ê°€ (ì›”ë“œ ëª¨ë¸ rollout ì‚¬ìš©)
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            target_data: íƒ€ê²Ÿ ë°ì´í„°
            
        Returns:
            Dict: ì•™ìƒë¸” ì„±ëŠ¥ ì§€í‘œ
        """
        if not self.ensemble_members:
            return {'avg_loss': float('inf'), 'best_loss': float('inf'), 
                   'worst_loss': float('inf'), 'member_count': 0}
        
        print(f"ğŸ“Š Evaluating ensemble performance with {len(self.ensemble_members)} members")
        
        member_performances = []
        individual_losses = []
        
        # ê° ë©¤ë²„ì˜ ì‹¤ì œ ì„±ëŠ¥ í‰ê°€
        for task_id, member_info in self.ensemble_members.items():
            performance = self.evaluate_member_performance(member_info, trans_obs_0, actions, target_data)
            member_performances.append(performance)
            individual_losses.append(performance['loss'])
        
        # ì•™ìƒë¸” ì „ì²´ ì„±ëŠ¥ ê³„ì‚°
        ensemble_performance = {
            'avg_loss': sum(individual_losses) / len(individual_losses),
            'best_loss': min(individual_losses),
            'worst_loss': max(individual_losses),
            'member_count': len(self.ensemble_members),
            'loss_std': torch.std(torch.tensor(individual_losses)).item() if len(individual_losses) > 1 else 0.0,
            'individual_performances': member_performances,
            'evaluated_at': time.time()
        }
        
        # ì•™ìƒë¸” ì˜ˆì¸¡ ì„±ëŠ¥ë„ í‰ê°€
        ensemble_prediction = self.ensemble_predict(trans_obs_0, actions, method='weighted_average')
        if ensemble_prediction is not None:
            with torch.no_grad():
                loss_fn = torch.nn.MSELoss()
                
                # Visual loss
                visual_loss = loss_fn(ensemble_prediction["visual"], target_data["visual"])
                
                # Proprio loss
                proprio_loss = loss_fn(ensemble_prediction["proprio"], target_data["proprio"])
                
                # Total loss
                total_loss = visual_loss + 0.3 * proprio_loss
                
                ensemble_performance['ensemble_loss'] = total_loss.item()
                ensemble_performance['ensemble_visual_loss'] = visual_loss.item()
                ensemble_performance['ensemble_proprio_loss'] = proprio_loss.item()
                
                # MAE ê³„ì‚°
                visual_mae = torch.mean(torch.abs(ensemble_prediction["visual"] - target_data["visual"]))
                proprio_mae = torch.mean(torch.abs(ensemble_prediction["proprio"] - target_data["proprio"]))
                ensemble_performance['ensemble_visual_mae'] = visual_mae.item()
                ensemble_performance['ensemble_proprio_mae'] = proprio_mae.item()
        
        print(f"ğŸ“ˆ Ensemble Performance Summary:")
        print(f"   - Average Loss: {ensemble_performance['avg_loss']:.6f}")
        print(f"   - Best Loss: {ensemble_performance['best_loss']:.6f}")
        print(f"   - Worst Loss: {ensemble_performance['worst_loss']:.6f}")
        print(f"   - Loss Std: {ensemble_performance['loss_std']:.6f}")
        print(f"   - Members: {ensemble_performance['member_count']}")
        if 'ensemble_loss' in ensemble_performance:
            print(f"   - Ensemble Loss: {ensemble_performance['ensemble_loss']:.6f}")
            print(f"   - Ensemble Visual Loss: {ensemble_performance['ensemble_visual_loss']:.6f}")
            print(f"   - Ensemble Proprio Loss: {ensemble_performance['ensemble_proprio_loss']:.6f}")
        
        return ensemble_performance
    
    def consolidate_lora_weights(self, task_ids: List[int], 
                               method: str = 'weighted_average') -> Dict:
        """
        ì—¬ëŸ¬ LoRA ê°€ì¤‘ì¹˜ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
        
        Args:
            task_ids: í†µí•©í•  íƒœìŠ¤í¬ ID ë¦¬ìŠ¤íŠ¸
            method: í†µí•© ë°©ë²• ('weighted_average', 'best_only', 'all')
            
        Returns:
            Dict: í†µí•©ëœ LoRA ê°€ì¤‘ì¹˜
        """
        if not task_ids:
            return {}
        
        # í•´ë‹¹ íƒœìŠ¤í¬ë“¤ì˜ LoRA ê°€ì¤‘ì¹˜ ìˆ˜ì§‘
        lora_weights_list = []
        weights = []
        
        for task_id in task_ids:
            if task_id in self.ensemble_members:
                member_info = self.ensemble_members[task_id]
                lora_weights_list.append(member_info['lora_weights'])
                
                # ê°€ì¤‘ì¹˜ ê³„ì‚° (ì„±ëŠ¥ ê¸°ë°˜)
                performance = member_info['performance']
                weight = 1.0 / (performance.get('loss', 1.0) + 1e-8)
                weights.append(weight)
        
        if not lora_weights_list:
            return {}
        
        # í†µí•© ë°©ë²•ì— ë”°ë¥¸ ì²˜ë¦¬
        if method == 'weighted_average':
            return self._weighted_average_consolidation(lora_weights_list, weights)
        elif method == 'best_only':
            best_idx = weights.index(max(weights))
            return lora_weights_list[best_idx]
        elif method == 'all':
            return self._simple_average_consolidation(lora_weights_list)
        else:
            raise ValueError(f"Unknown consolidation method: {method}")
    
    def save_ensemble_to_disk(self, save_path: str) -> bool:
        """
        ì•™ìƒë¸”ì„ ë””ìŠ¤í¬ì— ì €ì¥
        
        Args:
            save_path: ì €ì¥ ê²½ë¡œ
            
        Returns:
            bool: ì €ì¥ ì„±ê³µ ì—¬ë¶€
        """
        try:
            ensemble_data = {
                'ensemble_members': dict(self.ensemble_members),
                'performance_history': self.performance_history,
                'access_frequency': self.access_frequency,
                'current_task_id': self.current_task_id,
                'metadata': {
                    'max_ensemble_size': self.max_ensemble_size,
                    'cache_dir': self.cache_dir,
                    'max_memory_mb': self.max_memory_mb,
                    'saved_at': time.time()
                }
            }
            
            # ì••ì¶•í•˜ì—¬ ì €ì¥
            compressed_data = gzip.compress(pickle.dumps(ensemble_data))
            
            with open(save_path, 'wb') as f:
                f.write(compressed_data)
            
            print(f"ğŸ’¾ Ensemble saved to: {save_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Failed to save ensemble: {e}")
            return False
    
    def load_ensemble_from_disk(self, load_path: str) -> bool:
        """
        ë””ìŠ¤í¬ì—ì„œ ì•™ìƒë¸” ë¡œë“œ
        
        Args:
            load_path: ë¡œë“œ ê²½ë¡œ
            
        Returns:
            bool: ë¡œë“œ ì„±ê³µ ì—¬ë¶€
        """
        try:
            with open(load_path, 'rb') as f:
                compressed_data = f.read()
            
            ensemble_data = pickle.loads(gzip.decompress(compressed_data))
            
            # ì•™ìƒë¸” ë°ì´í„° ë³µì›
            self.ensemble_members = OrderedDict(ensemble_data['ensemble_members'])
            self.performance_history = ensemble_data['performance_history']
            self.access_frequency = ensemble_data['access_frequency']
            self.current_task_id = ensemble_data['current_task_id']
            
            # ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì¬ê³„ì‚°
            self.memory_usage = sum(
                member['size_mb'] for member in self.ensemble_members.values()
            )
            
            print(f"ğŸ“‚ Ensemble loaded from: {load_path}")
            print(f"   - Members: {len(self.ensemble_members)}")
            print(f"   - Memory usage: {self.memory_usage:.2f}MB")
            
            return True
            
        except Exception as e:
            print(f"âŒ Failed to load ensemble: {e}")
            return False
    
    def get_ensemble_info(self) -> Dict:
        """
        ì•™ìƒë¸” ì •ë³´ ë°˜í™˜
        
        Returns:
            Dict: ì•™ìƒë¸” ìƒíƒœ ì •ë³´
        """
        return {
            'member_count': len(self.ensemble_members),
            'max_ensemble_size': self.max_ensemble_size,
            'memory_usage_mb': self.memory_usage,
            'max_memory_mb': self.max_memory_mb,
            'current_task_id': self.current_task_id,
            'members': list(self.ensemble_members.keys()),
            'cache_dir': self.cache_dir
        }
    
    def _calculate_lora_size(self, lora_weights: Dict) -> float:
        """LoRA ê°€ì¤‘ì¹˜ í¬ê¸° ê³„ì‚° (MB ë‹¨ìœ„)"""
        total_params = 0
        for layer_name, weights in lora_weights.items():
            if isinstance(weights, torch.Tensor):
                total_params += weights.numel()
            elif isinstance(weights, dict):
                for sub_weights in weights.values():
                    if isinstance(sub_weights, torch.Tensor):
                        total_params += sub_weights.numel()
        
        # float32 ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
        size_mb = (total_params * 4) / (1024 * 1024)
        return size_mb
    
    def _remove_oldest_member(self):
        """ê°€ì¥ ì˜¤ë˜ëœ ì•™ìƒë¸” ë©¤ë²„ ì œê±°"""
        if not self.ensemble_members:
            return
        
        # ê°€ì¥ ì˜¤ë˜ëœ ë©¤ë²„ ì°¾ê¸°
        oldest_task_id = min(self.ensemble_members.keys(), 
                           key=lambda x: self.ensemble_members[x]['created_at'])
        
        oldest_member = self.ensemble_members[oldest_task_id]
        self.memory_usage -= oldest_member['size_mb']
        
        # ë””ìŠ¤í¬ì— ì €ì¥ í›„ ë©”ëª¨ë¦¬ì—ì„œ ì œê±°
        self._save_member_to_disk(oldest_task_id, oldest_member)
        del self.ensemble_members[oldest_task_id]
        
        print(f"ğŸ—‘ï¸  Removed oldest member: Task {oldest_task_id}")
    
    def _cleanup_memory(self):
        """ë©”ëª¨ë¦¬ ì •ë¦¬"""
        if self.memory_usage <= self.max_memory_mb:
            return
        
        # ì ‘ê·¼ ë¹ˆë„ê°€ ë‚®ì€ ë©¤ë²„ë“¤ì„ ë””ìŠ¤í¬ë¡œ ì´ë™
        sorted_members = sorted(
            self.ensemble_members.items(),
            key=lambda x: x[1]['access_count']
        )
        
        for task_id, member_info in sorted_members:
            if self.memory_usage <= self.max_memory_mb * 0.8:  # 80%ê¹Œì§€ ì •ë¦¬
                break
            
            self._save_member_to_disk(task_id, member_info)
            self.memory_usage -= member_info['size_mb']
            print(f"ğŸ’¾ Moved to disk: Task {task_id}")
    
    def _save_member_to_disk(self, task_id: int, member_info: Dict):
        """ë©¤ë²„ë¥¼ ë””ìŠ¤í¬ì— ì €ì¥"""
        save_path = os.path.join(self.cache_dir, f"lora_task_{task_id}.pth")
        
        # LoRA ê°€ì¤‘ì¹˜ë§Œ ì €ì¥ (ë©”íƒ€ë°ì´í„°ëŠ” ë³„ë„ ê´€ë¦¬)
        torch.save(member_info['lora_weights'], save_path)
    
    def _weighted_average_consolidation(self, lora_weights_list: List[Dict], 
                                      weights: List[float]) -> Dict:
        """ê°€ì¤‘ í‰ê·  í†µí•©"""
        if not lora_weights_list:
            return {}
        
        # ê°€ì¤‘ì¹˜ ì •ê·œí™”
        total_weight = sum(weights)
        normalized_weights = [w / total_weight for w in weights]
        
        consolidated = {}
        
        # ëª¨ë“  ë ˆì´ì–´ì— ëŒ€í•´ ê°€ì¤‘ í‰ê·  ê³„ì‚°
        for layer_name in lora_weights_list[0].keys():
            weighted_sum = None
            
            for i, lora_weights in enumerate(lora_weights_list):
                layer_weights = lora_weights[layer_name]
                weight = normalized_weights[i]
                
                if weighted_sum is None:
                    weighted_sum = weight * layer_weights
                else:
                    weighted_sum += weight * layer_weights
            
            consolidated[layer_name] = weighted_sum
        
        print(f"ğŸ”— Consolidated {len(lora_weights_list)} LoRA weights using weighted average")
        return consolidated
    
    def _simple_average_consolidation(self, lora_weights_list: List[Dict]) -> Dict:
        """ë‹¨ìˆœ í‰ê·  í†µí•©"""
        if not lora_weights_list:
            return {}
        
        consolidated = {}
        
        for layer_name in lora_weights_list[0].keys():
            layer_sum = None
            
            for lora_weights in lora_weights_list:
                layer_weights = lora_weights[layer_name]
                
                if layer_sum is None:
                    layer_sum = layer_weights
                else:
                    layer_sum += layer_weights
            
            # í‰ê·  ê³„ì‚°
            consolidated[layer_name] = layer_sum / len(lora_weights_list)
        
        print(f"ğŸ”— Consolidated {len(lora_weights_list)} LoRA weights using simple average")
        return consolidated


class EnsembleOnlineLora:
    """
    ì•™ìƒë¸” ê¸°ë°˜ Online-LoRA í•™ìŠµ ì‹œìŠ¤í…œ
    
    ê¸°ì¡´ OnlineLoraë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©í•˜ë©´ì„œ
    ì•™ìƒë¸” ê¸°ë°˜ì˜ ì§€ëŠ¥ì ì¸ LoRA ì ì¸µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    """
    
    def __init__(self, workspace):
        """
        ì•™ìƒë¸” Online-LoRA ì´ˆê¸°í™”
        
        Args:
            workspace: PlanWorkspace ì¸ìŠ¤í„´ìŠ¤
        """
        self.workspace = workspace
        self.wm = workspace.wm
        # lora.ensemble_cfg ë˜ëŠ” lora ë£¨íŠ¸ì—ì„œ ì½ê¸°
        self.cfg = workspace.cfg_dict.get("lora", {}).get("ensemble_cfg", workspace.cfg_dict.get("lora", {}))
        self.device = next(self.wm.parameters()).device
        
        # ê¸°ì¡´ OnlineLoraë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©
        from planning.online import OnlineLora
        self.base_online_lora = OnlineLora(workspace)
        
        # ê¸°ì¡´ Online-LoRAì˜ ì†ì„±ë“¤ì„ ì°¸ì¡°
        self.is_online_lora = self.base_online_lora.is_online_lora
        self.hybrid_enabled = self.base_online_lora.hybrid_enabled
        self.task_based_stacking = self.base_online_lora.task_based_stacking
        self.loss_based_stacking = self.base_online_lora.loss_based_stacking
        self.max_stacks_per_task = self.base_online_lora.max_stacks_per_task
        self.stacks_in_current_task = self.base_online_lora.stacks_in_current_task
        self.current_task_id = self.base_online_lora.current_task_id
        self.task_changed = self.base_online_lora.task_changed  # íƒœìŠ¤í¬ ì „í™˜ ê°ì§€ í”Œë˜ê·¸
        self.stack_history = self.base_online_lora.stack_history
        self.last_loss = self.base_online_lora.last_loss  # ì´ˆê¸°ê°’ ë³µì‚¬ (update()ì—ì„œ ë™ê¸°í™”)
        self.optimizer = self.base_online_lora.optimizer
        self.loss_fn = self.base_online_lora.loss_fn
        self.visual_loss_weight = self.base_online_lora.visual_loss_weight
        self.proprio_loss_weight = self.base_online_lora.proprio_loss_weight
        
        # ì•™ìƒë¸” ì „ìš© ì„¤ì •
        self.ensemble_evaluation_steps = self.cfg.get("evaluation_steps", 10)
        
        # ì•™ìƒë¸” ì €ì¥ ì •ì±… (ê¸°ë³¸: ìŠ¤íƒ ì§í›„ ì €ì¥ ë¹„í™œì„±í™”, íƒœìŠ¤í¬ ì¢…ë£Œ ì‹œ ì €ì¥)
        self.save_on_stack = self.cfg.get("save_on_stack", False)
        self.save_on_task_end = self.cfg.get("save_on_task_end", True)

        # ì•™ìƒë¸” ê´€ë¦¬ì ì´ˆê¸°í™”
        self.ensemble_manager = LoRAEnsembleManager(
            base_model=self.wm,
            max_ensemble_size=self.cfg.get("max_ensemble_size", 5),
            cache_dir=self.cfg.get("cache_dir", "./lora_cache"),
            max_memory_mb=self.cfg.get("max_memory_mb", 200)
        )
        
        print(f"EnsembleOnlineLora initialized:")
        print(f"  - Base OnlineLora: {self.is_online_lora}")
        print(f"  - Ensemble enabled: {self.hybrid_enabled}")
        print(f"  - Task-based stacking: {self.task_based_stacking}")
        
        # ğŸ”§ OnlineLoraì— ì½œë°± ì„¤ì •
        self.base_online_lora.on_lora_stack_callback = self._on_lora_stacked
        
        # ì•™ìƒë¸” ì „ìš© ì½œë°± ì´ˆê¸°í™”
        self.on_lora_stack_callback = None
    
    def update(self, trans_obs_0, actions, e_obses):
        """
        í•˜ë‚˜ì˜ í•™ìŠµ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë©”ì†Œë“œ
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            e_obses: ì‹¤ì œ ê´€ì¸¡ ì‹œí€€ìŠ¤
        """
        # ê¸°ì¡´ OnlineLoraì˜ í•™ìŠµ ë¡œì§ ì‚¬ìš© (ë°˜í™˜ê°’ ì—†ìŒ, ë‚´ë¶€ì ìœ¼ë¡œ last_loss ì„¤ì •)
        self.base_online_lora.update(trans_obs_0, actions, e_obses)
        
        # ë§ˆì§€ë§‰ Loss ê°’ ë™ê¸°í™” (base_online_loraì—ì„œ ì„¤ì •ëœ ê°’ ì‚¬ìš©)
        self.last_loss = self.base_online_lora.last_loss
        
        # task_changed í”Œë˜ê·¸ ë™ê¸°í™”
        self.task_changed = self.base_online_lora.task_changed
        
        # ì•™ìƒë¸” ê¸°ë°˜ ì ì¸µ ë¡œì§ (í–¥í›„ êµ¬í˜„)
        if self.last_loss is not None and self.hybrid_enabled:
            self._manage_ensemble_stacking(self.last_loss)
    
    def trigger_task_based_stacking(self, task_id, reason="task_change"):
        """
        íƒœìŠ¤í¬ ê¸°ë°˜ ì ì¸µì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤ (ì•™ìƒë¸” ê¸°ë°˜)
        
        Args:
            task_id: ìƒˆë¡œìš´ íƒœìŠ¤í¬ ID
            reason: ì ì¸µ ì´ìœ 
            
        Returns:
            bool: ì ì¸µ ì„±ê³µ ì—¬ë¶€
        """
        # ğŸ”§ ì•™ìƒë¸” ì „ìš© ëª¨ë“œì—ì„œëŠ” task_based_stackingì´ falseì—¬ë„ ì‘ë™
        if not self.hybrid_enabled:
            return False
        
        # íƒœìŠ¤í¬ê°€ ë³€ê²½ëœ ê²½ìš°
        if task_id != self.current_task_id:
            self.current_task_id = task_id
            self.stacks_in_current_task = 0
            print(f"ğŸ”„ Task changed to {task_id}. Resetting stack counter.")
        
        # ìµœëŒ€ ì ì¸µ íšŸìˆ˜ í™•ì¸
        if self.stacks_in_current_task >= self.max_stacks_per_task:
            print(f"âš ï¸  Max stacks per task ({self.max_stacks_per_task}) reached. Skipping ensemble-based stacking.")
            return False
        
        # ì•™ìƒë¸” ê¸°ë°˜ ì ì¸µ ê²°ì •
        should_stack, best_member = self._should_stack_lora_ensemble(task_id)
        
        if should_stack:
            print(f"ğŸ¯ Ensemble-based LoRA stacking triggered (Task {task_id}, Reason: {reason})")
            
            # ğŸ”§ ì•™ìƒë¸” ì „ìš© ëª¨ë“œì—ì„œëŠ” ì§ì ‘ ì ì¸µ ìˆ˜í–‰ (task_based_stackingì´ falseì—¬ë„)
            if not self.task_based_stacking:
                # ì§ì ‘ ì ì¸µ ìˆ˜í–‰
                stacking_success = self._perform_ensemble_lora_stacking(task_id, best_member, reason)
                
                if stacking_success:
                    self.stacks_in_current_task += 1
                    print(f"âœ… Ensemble-based stacking successful. Total stacks in task: {self.stacks_in_current_task}")
                    return True
                else:
                    print(f"âŒ Ensemble-based stacking failed.")
                    return False
            else:
                # ê¸°ì¡´ OnlineLoraì˜ íƒœìŠ¤í¬ ê¸°ë°˜ ì ì¸µ ì‚¬ìš©
                stacking_success = self.base_online_lora.trigger_task_based_stacking(task_id, reason)
                
                if stacking_success:
                    self.stacks_in_current_task += 1
                    print(f"âœ… Ensemble-based stacking successful. Total stacks in task: {self.stacks_in_current_task}")
                    return True
                else:
                    print(f"âŒ Base OnlineLora stacking failed. Skipping ensemble management.")
                    return False
        else:
            print(f"ğŸ“Š Using existing ensemble for Task {task_id} (performance sufficient)")
            return False
    
    def _perform_training_step(self, trans_obs_0, actions, e_obses):
        """ì‹¤ì œ ì˜ˆì¸¡, ì†ì‹¤ ê³„ì‚°, ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            print("--- Starting Ensemble LoRA Online Learning ---")
            
            # 1. ì˜ˆì¸¡ (ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™”)
            i_z_obses_pred, _ = self.wm.rollout(obs_0=trans_obs_0, act=actions)

            # 2. ì •ë‹µ ì¤€ë¹„ (ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”)
            with torch.no_grad():
                trans_obs_gt = self.workspace.data_preprocessor.transform_obs(e_obses)
                trans_obs_gt = move_to_device(trans_obs_gt, self.device)
                i_z_obses_gt = self.wm.encode_obs(trans_obs_gt)

            # 3. ì†ì‹¤ ê³„ì‚°
            print("Computing ensemble loss...")
            frameskip = self.workspace.frameskip
            gt_proprio_resampled = i_z_obses_gt["proprio"][:, ::frameskip, :].detach()
            gt_visual_resampled = i_z_obses_gt["visual"][:, ::frameskip, :, :].detach()
            
            proprio_loss = self.loss_fn(i_z_obses_pred["proprio"], gt_proprio_resampled)
            visual_loss = self.loss_fn(i_z_obses_pred["visual"], gt_visual_resampled)
            
            total_loss = self.visual_loss_weight * visual_loss + self.proprio_loss_weight * proprio_loss
            
            print(f"Visual loss: {visual_loss.item():.6f}, Proprio loss: {proprio_loss.item():.6f}")
            print(f"Total loss: {total_loss.item():.6f}")

            # 4. ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸
            if self.optimizer is None:
                # ì²« ë²ˆì§¸ í•™ìŠµ ì‹œ ì˜µí‹°ë§ˆì´ì € ì´ˆê¸°í™”
                params_to_train = [p for p in self.wm.parameters() if p.requires_grad]
                self.optimizer = torch.optim.Adam(params_to_train, lr=self.cfg.get("lr", 1e-4))
            
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()

            return total_loss.item()

        except Exception as e:
            print(f"Error during ensemble training step: {e}")
            return None
        
        finally:
            # 5. ë©”ëª¨ë¦¬ ì •ë¦¬
            if 'i_z_obses_pred' in locals(): del i_z_obses_pred
            if 'i_z_obses_gt' in locals(): del i_z_obses_gt
            if 'total_loss' in locals(): del total_loss
            torch.cuda.empty_cache()
            print("--- Ensemble LoRA Online Update Complete ---")
    
    def _should_stack_lora_ensemble(self, task_id, input_data=None, target_data=None):
        """
        ì•™ìƒë¸” ê¸°ë°˜ìœ¼ë¡œ LoRA ì ì¸µ ì—¬ë¶€ ê²°ì •
        
        Args:
            task_id: í˜„ì¬ íƒœìŠ¤í¬ ID
            input_data: ì…ë ¥ ë°ì´í„° (ì„ íƒì‚¬í•­)
            target_data: íƒ€ê²Ÿ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            Tuple[bool, Dict]: (ì ì¸µ ì—¬ë¶€, ìµœì  ë©¤ë²„ ì •ë³´)
        """
        # ì•™ìƒë¸” ë©¤ë²„ ìƒíƒœ í™•ì¸
        
        # ğŸ”§ ì•™ìƒë¸” ë©¤ë²„ ìƒì„¸ ì •ë³´ ì¶œë ¥
        if self.ensemble_manager.ensemble_members:
            for task_id_member, member_info in self.ensemble_manager.ensemble_members.items():
                print(f"   - Member Task {task_id_member}: {member_info.get('performance', {}).get('loss', 'N/A')}")
        
        # í˜„ì¬ ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€
        if not self.ensemble_manager.ensemble_members:
            print("ğŸ“Š No ensemble members available. Stacking new LoRA.")
            return True, None
        
        # ğŸ”§ ì•™ìƒë¸” ë©¤ë²„ê°€ ìˆìœ¼ë©´ í•­ìƒ ì‹¤ì‹œê°„ ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰
        print(f"ğŸ“Š Found {len(self.ensemble_manager.ensemble_members)} ensemble members. Performing real-time evaluation...")
        
        # ì‹¤ì‹œê°„ í‰ê°€ë¥¼ ìœ„í•´ continual_plan.pyì˜ evaluate_members_for_new_task í˜¸ì¶œ
        # ì´ ë©”ì„œë“œëŠ” ê° ë©¤ë²„ì— ëŒ€í•´ ì‹¤ì‹œê°„ìœ¼ë¡œ ì„±ëŠ¥ì„ í‰ê°€í•¨
        print("ğŸ“Š Performing real-time ensemble evaluation...")
        
        # ì‹¤ì‹œê°„ í‰ê°€ëŠ” continual_plan.pyì—ì„œ ìˆ˜í–‰ë˜ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ LoRA ì ì¸µë§Œ ìˆ˜í–‰
        print("ğŸ“Š Real-time evaluation will be performed by continual_plan.py")
        print("ğŸ“Š Proceeding with LoRA stacking...")
        return True, None
    
    def _perform_ensemble_lora_stacking(self, task_id, best_member, reason):
        """
        ì•™ìƒë¸” ê¸°ë°˜ LoRA ì ì¸µ ìˆ˜í–‰ (OnlineLoraì— ìœ„ì„)
        
        Args:
            task_id: íƒœìŠ¤í¬ ID
            best_member: ìµœì  ë©¤ë²„ ì •ë³´ (ì‚¬ìš©í•˜ì§€ ì•ŠìŒ)
            reason: ì ì¸µ ì´ìœ 
            
        Returns:
            bool: ì ì¸µ ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"Performing ensemble-based LoRA stacking (delegating to OnlineLora)...")
            
            # ğŸ”§ OnlineLoraì˜ _perform_lora_stacking ë©”ì„œë“œ ì§ì ‘ í˜¸ì¶œ
            stacking_success = self.base_online_lora._perform_lora_stacking("ensemble_based", task_id, reason)
            
            if stacking_success:
                # ì ì¸µ ì™„ë£Œ ë¡œê·¸
                print(f"Ensemble-based LoRA stacking completed successfully!")
                print(f"   - Task ID: {task_id}")
                print(f"   - Reason: {reason}")
                print(f"   - Stacks in current task: {self.stacks_in_current_task + 1}/{self.max_stacks_per_task}")
                
                return True
            else:
                print(f"âŒ OnlineLora stacking failed.")
                return False
            
        except Exception as e:
            print(f"âŒ Error during ensemble-based LoRA stacking: {e}")
            return False
    
    def _manage_ensemble_stacking(self, current_loss_value):
        """
        ì•™ìƒë¸” ê¸°ë°˜ ì ì¸µ ê´€ë¦¬ (í–¥í›„ êµ¬í˜„)
        
        Args:
            current_loss_value: í˜„ì¬ loss ê°’
        """
        # í–¥í›„ loss ê¸°ë°˜ ì•™ìƒë¸” ì ì¸µ ë¡œì§ êµ¬í˜„
        pass
    
    def _extract_current_stacked_lora_weights(self):
        """
        í˜„ì¬ ì ì¸µëœ LoRA ê°€ì¤‘ì¹˜ ì¶”ì¶œ (ëª¨ë“  ì ì¸µ íš¨ê³¼ í¬í•¨)
        
        Returns:
            Dict: í˜„ì¬ ì ì¸µëœ LoRA ê°€ì¤‘ì¹˜ (ì²« ë²ˆì§¸ ìŠ¤íƒ - ëª¨ë“  ì ì¸µ íš¨ê³¼ í¬í•¨)
        """
        lora_weights = {}
        
        try:
            # LoRA_ViT_spreadì—ì„œ ì ì¸µëœ ê°€ì¤‘ì¹˜ ì¶”ì¶œ
            if hasattr(self.wm.predictor, 'w_As') and hasattr(self.wm.predictor, 'w_Bs'):
                w_As = self.wm.predictor.w_As
                w_Bs = self.wm.predictor.w_Bs
                
                print(f"ğŸ“Š Extracting LoRA weights from {len(w_As)} total layers...")
                
                # ğŸ”§ ì²« ë²ˆì§¸ ìŠ¤íƒ ì¶”ì¶œ (ëª¨ë“  ì ì¸µ íš¨ê³¼ê°€ í¬í•¨ëœ ë ˆì´ì–´ë“¤)
                # ViTì— 6ê°œì˜ attention blockì´ ìˆê³ , ê° ë¸”ë¡ë§ˆë‹¤ q, v 2ê°œì”© = ì´ 12ê°œ
                layers_per_stack = 12  # ê° LoRA ìŠ¤íƒë‹¹ ë ˆì´ì–´ ìˆ˜
                
                # ì²« ë²ˆì§¸ ìŠ¤íƒ (ëª¨ë“  ì ì¸µ íš¨ê³¼ê°€ í¬í•¨ëœ ë ˆì´ì–´ë“¤) ì¶”ì¶œ
                for i in range(min(layers_per_stack, len(w_As))):
                    layer_key = f'layer_{i}'
                    lora_weights[layer_key] = {
                        'w_A': w_As[i].weight.data.clone().detach(),  # ëª¨ë“  ì ì¸µ íš¨ê³¼ í¬í•¨
                        'w_B': w_Bs[i].weight.data.clone().detach()   # ëª¨ë“  ì ì¸µ íš¨ê³¼ í¬í•¨
                    }
                
                print(f"âœ… Successfully extracted {len(lora_weights)} LoRA layers with all stacking effects")
                
                # ğŸ”§ ë””ë²„ê¹…: ì ì¸µ íš¨ê³¼ í™•ì¸
                if len(w_As) > layers_per_stack:
                    print(f"ğŸ“Š LoRA Stacking Info:")
                    print(f"   - Total layers: {len(w_As)}")
                    print(f"   - Layers per stack: {layers_per_stack}")
                    print(f"   - Number of stacks: {len(w_As) // layers_per_stack}")
                    print(f"   - Extracted: First stack (all stacking effects included)")
                else:
                    print(f"ğŸ“Š Single LoRA stack detected (no stacking yet)")
                    
            else:
                print(f"âš ï¸  w_As or w_Bs not found in predictor. Available attributes:")
                if hasattr(self.wm.predictor, '__dict__'):
                    for attr in dir(self.wm.predictor):
                        if not attr.startswith('_'):
                            print(f"   - {attr}")
                
        except Exception as e:
            print(f"âŒ Error extracting LoRA weights: {e}")
            import traceback
            traceback.print_exc()
        
        return lora_weights
    
    def _apply_lora_weights(self, lora_weights):
        """
        ì•™ìƒë¸” ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ í˜„ì¬ ëª¨ë¸ì— ì ìš© (ì²« ë²ˆì§¸ ìŠ¤íƒì— ì ìš©)
        
        Args:
            lora_weights: Dict - {'layer_0': {'w_A': tensor, 'w_B': tensor}, ...}
            
        Returns:
            bool: ì ìš© ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not hasattr(self.wm, 'predictor'):
                print(f"âš ï¸  World model doesn't have predictor. Cannot apply LoRA weights.")
                return False
                
            predictor = self.wm.predictor
            
            if not (hasattr(predictor, 'w_As') and hasattr(predictor, 'w_Bs')):
                print(f"âš ï¸  Predictor doesn't have w_As/w_Bs. Cannot apply LoRA weights.")
                return False
            
            # ğŸ”§ ì•™ìƒë¸” ì¶”ë¡ ì„ ìœ„í•œ LoRA ì ìš© ë°©ì‹
            # ì²« ë²ˆì§¸ ìŠ¤íƒ (ëª¨ë“  ì ì¸µ íš¨ê³¼ê°€ í¬í•¨ëœ ë ˆì´ì–´ë“¤)ì— ì•™ìƒë¸” ë©¤ë²„ì˜ ê°€ì¤‘ì¹˜ ì ìš©
            
            # í˜„ì¬ LoRA ìƒíƒœ ë°±ì—…
            original_w_As = [w_A.weight.data.clone() for w_A in predictor.w_As]
            original_w_Bs = [w_B.weight.data.clone() for w_B in predictor.w_Bs]
            
            # ì•™ìƒë¸” ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ ì ìš©
            w_As = predictor.w_As
            w_Bs = predictor.w_Bs
            
            with torch.no_grad():
                # ğŸ”§ ì²« ë²ˆì§¸ ìŠ¤íƒì—ë§Œ ì•™ìƒë¸” ë©¤ë²„ì˜ ê°€ì¤‘ì¹˜ ì ìš©
                layers_per_stack = 12
                
                for i in range(min(layers_per_stack, len(w_As))):
                    layer_key = f'layer_{i}'
                    
                    if layer_key in lora_weights:
                        w_As[i].weight.data.copy_(lora_weights[layer_key]['w_A'])
                        w_Bs[i].weight.data.copy_(lora_weights[layer_key]['w_B'])
                    else:
                        print(f"âš ï¸  Layer {layer_key} not found in lora_weights")
                        # ì›ë˜ ê°€ì¤‘ì¹˜ ë³µì›
                        self._restore_lora_weights(original_w_As, original_w_Bs)
                        return False
            
            print(f"âœ… Successfully applied LoRA weights from ensemble member to first stack")
            return True
            
        except Exception as e:
            print(f"âŒ Error applying LoRA weights: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _restore_lora_weights(self, original_w_As, original_w_Bs):
        """ì›ë˜ LoRA ê°€ì¤‘ì¹˜ ë³µì›"""
        try:
            predictor = self.wm.predictor
            w_As = predictor.w_As
            w_Bs = predictor.w_Bs
            
            with torch.no_grad():
                for i, (orig_A, orig_B) in enumerate(zip(original_w_As, original_w_Bs)):
                    w_As[i].weight.data.copy_(orig_A)
                    w_Bs[i].weight.data.copy_(orig_B)
            
            print(f"âœ… Restored original LoRA weights")
            
        except Exception as e:
            print(f"âŒ Error restoring LoRA weights: {e}")
    
    def _on_lora_stacked(self, steps, loss, task_id, stack_type, reason):
        """
        OnlineLoraì—ì„œ LoRA ì ì¸µ í›„ í˜¸ì¶œë˜ëŠ” ì½œë°± í•¨ìˆ˜
        
        Args:
            steps: ì ì¸µê¹Œì§€ì˜ ìŠ¤í… ìˆ˜
            loss: í˜„ì¬ loss ê°’
            task_id: íƒœìŠ¤í¬ ID
            stack_type: ì ì¸µ íƒ€ì… ("task_based" ë˜ëŠ” "loss_based")
            reason: ì ì¸µ ì´ìœ 
        """
        print(f"ğŸ”„ LoRA stacked in base model. save_on_stack={self.save_on_stack}")
        print(f"   - Task ID: {task_id}")
        print(f"   - Stack Type: {stack_type}")
        print(f"   - Reason: {reason}")
        print(f"   - Steps: {steps}")
        loss_str = f"{loss:.6f}" if loss is not None else "N/A"
        print(f"   - Loss: {loss_str}")
        
        # ì €ì¥ ì •ì±…: ê¸°ë³¸ì€ ìŠ¤íƒ ì§í›„ ì €ì¥í•˜ì§€ ì•Šê³  íƒœìŠ¤í¬ ì¢…ë£Œ ì‹œ ì €ì¥
        if not self.save_on_stack:
            # ë©”íƒ€ë§Œ ê¸°ë¡í•´ë‘ê³  ì €ì¥ì€ ì—°ê¸°
            self._pending_stack_info = {
                'task_id': task_id,
                'steps': steps,
                'stack_type': stack_type,
                'reason': reason,
                'loss_at_stack': float(loss) if loss is not None else None,
                'timestamp': time.time(),
            }
            print("â„¹ï¸ Deferring ensemble save until task end (finalized state)")
            return
        
        # save_on_stack=Trueì¸ ê²½ìš°ì—ë§Œ ì¦‰ì‹œ ì €ì¥
        try:
            self._save_current_lora_member_impl(task_id=task_id, reason=f"{stack_type}:{reason}", loss_value=loss, steps=steps)
        except Exception as e:
            print(f"âŒ Error in immediate save during _on_lora_stacked: {e}")
            import traceback
            traceback.print_exc()

    def save_current_lora_member(self, task_id: int, reason: str = "task_end") -> bool:
        """íƒœìŠ¤í¬ ì¢…ë£Œ/ìˆ˜ë ´ ì‹œì ì˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ì•™ìƒë¸”ì— ì €ì¥."""
        if not self.save_on_task_end:
            print("â„¹ï¸ save_on_task_end is disabled; skipping final save")
            return False
        try:
            last_loss = getattr(self.base_online_lora, 'last_loss', None)
            steps = getattr(self.base_online_lora, 'steps_since_last_stack', 0)
            return self._save_current_lora_member_impl(task_id=task_id, reason=reason, loss_value=last_loss, steps=steps)
        except Exception as e:
            print(f"âŒ Error in save_current_lora_member: {e}")
            import traceback
            traceback.print_exc()
            return False

    def _save_current_lora_member_impl(self, task_id: int, reason: str, loss_value: float, steps: int) -> bool:
        current_weights = self._extract_current_stacked_lora_weights()
        if not current_weights:
            print("âš ï¸ No LoRA weights extracted. Skipping save.")
            return False
        performance = {
            'loss': float(loss_value) if loss_value is not None else float('inf'),
            'steps': int(steps) if steps is not None else 0,
            'stack_type': reason,
        }
        metadata = {
            'reason': reason,
            'saved_at': time.time(),
        }
        saved = self.ensemble_manager.add_ensemble_member(
            task_id=task_id,
            lora_weights=current_weights,
            performance=performance,
            metadata=metadata,
        )
        if saved:
            print(f"ğŸ’¾ Saved finalized LoRA member for Task {task_id} (reason={reason})")
        return saved
    
    def get_ensemble_info(self):
        """ì•™ìƒë¸” ì •ë³´ ë°˜í™˜"""
        return self.ensemble_manager.get_ensemble_info()
    
    def save_ensemble(self, save_path):
        """ì•™ìƒë¸” ì €ì¥"""
        return self.ensemble_manager.save_ensemble_to_disk(save_path)
    
    def load_ensemble(self, load_path):
        """ì•™ìƒë¸” ë¡œë“œ"""
        return self.ensemble_manager.load_ensemble_from_disk(load_path)
    
    def test_ensemble_inference(self, trans_obs_0, actions, target_data=None):
        """
        ì•™ìƒë¸” ì¶”ë¡  í…ŒìŠ¤íŠ¸ ë° ê²€ì¦
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤  
            target_data: íƒ€ê²Ÿ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            Tuple: (individual_predictions, ensemble_prediction)
        """
        print(f"ğŸ§ª Testing ensemble inference with {len(self.ensemble_manager.ensemble_members)} members")
        
        if not self.ensemble_manager.ensemble_members:
            print(f"âš ï¸  No ensemble members available for testing")
            return None, None
        
        # ê° ë©¤ë²„ë³„ ê°œë³„ ì˜ˆì¸¡
        individual_predictions = {}
        for task_id, member_info in self.ensemble_manager.ensemble_members.items():
            try:
                lora_weights = member_info['lora_weights']
                success = self.ensemble_manager._apply_lora_weights(lora_weights)
                
                if not success:
                    print(f"âŒ Failed to apply LoRA weights for member {task_id}")
                    continue
                
                with torch.no_grad():
                    pred = self.wm.rollout(obs_0=trans_obs_0, act=actions)[0]
                    individual_predictions[task_id] = pred
                    
                    print(f"âœ… Member {task_id} prediction successful")
                    
            except Exception as e:
                print(f"âŒ Member {task_id} prediction failed: {e}")
        
        # ì•™ìƒë¸” ì˜ˆì¸¡
        ensemble_pred = self.ensemble_manager.ensemble_predict(trans_obs_0, actions, method='weighted_average')
        
        if ensemble_pred is not None:
            print(f"âœ… Ensemble prediction successful")
            
            # íƒ€ê²Ÿ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì„±ëŠ¥ í‰ê°€
            if target_data is not None:
                loss_fn = torch.nn.MSELoss()
                visual_loss = loss_fn(ensemble_pred["visual"], target_data["visual"])
                proprio_loss = loss_fn(ensemble_pred["proprio"], target_data["proprio"])
                total_loss = visual_loss + 0.3 * proprio_loss
                
                print(f"ğŸ“Š Ensemble Performance:")
                print(f"   - Total Loss: {total_loss.item():.6f}")
                print(f"   - Visual Loss: {visual_loss.item():.6f}")
                print(f"   - Proprio Loss: {proprio_loss.item():.6f}")
        
        return individual_predictions, ensemble_pred
    
    def test_ensemble_performance(self, trans_obs_0, actions, target_data):
        """
        ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ í…ŒìŠ¤íŠ¸
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            target_data: íƒ€ê²Ÿ ë°ì´í„°
            
        Returns:
            Dict: ì•™ìƒë¸” ì„±ëŠ¥ ì§€í‘œ
        """
        print(f"ğŸ§ª Testing ensemble performance evaluation...")
        
        if not self.ensemble_manager.ensemble_members:
            print(f"âš ï¸  No ensemble members available for testing")
            return None
        
        # ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ ìˆ˜í–‰
        performance = self.ensemble_manager.evaluate_ensemble_performance(
            trans_obs_0, actions, target_data
        )
        
        print(f"ğŸ“Š Ensemble Performance Test Results:")
        print(f"   - Member Count: {performance['member_count']}")
        print(f"   - Average Loss: {performance['avg_loss']:.6f}")
        print(f"   - Best Loss: {performance['best_loss']:.6f}")
        print(f"   - Worst Loss: {performance['worst_loss']:.6f}")
        print(f"   - Loss Std: {performance['loss_std']:.6f}")
        
        if 'ensemble_loss' in performance:
            print(f"   - Ensemble Loss: {performance['ensemble_loss']:.6f}")
            print(f"   - Ensemble Visual Loss: {performance['ensemble_visual_loss']:.6f}")
            print(f"   - Ensemble Proprio Loss: {performance['ensemble_proprio_loss']:.6f}")
        
        return performance
    
    def test_ensemble_methods(self, trans_obs_0, actions, target_data=None):
        """
        ë‹¤ì–‘í•œ ì•™ìƒë¸” ë°©ë²• í…ŒìŠ¤íŠ¸
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            target_data: íƒ€ê²Ÿ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            Dict: ê° ë°©ë²•ë³„ ì˜ˆì¸¡ ê²°ê³¼
        """
        print(f"ğŸ§ª Testing different ensemble methods...")
        
        if not self.ensemble_manager.ensemble_members:
            print(f"âš ï¸  No ensemble members available for testing")
            return None
        
        methods = ['weighted_average', 'best_only', 'all']
        results = {}
        
        for method in methods:
            try:
                print(f"ğŸ“Š Testing method: {method}")
                pred = self.ensemble_manager.ensemble_predict(trans_obs_0, actions, method=method)
                
                if pred is not None:
                    results[method] = pred
                    
                    # íƒ€ê²Ÿ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ì„±ëŠ¥ í‰ê°€
                    if target_data is not None:
                        loss_fn = torch.nn.MSELoss()
                        visual_loss = loss_fn(pred["visual"], target_data["visual"])
                        proprio_loss = loss_fn(pred["proprio"], target_data["proprio"])
                        total_loss = visual_loss + 0.3 * proprio_loss
                        
                        print(f"   - {method}: Total Loss {total_loss.item():.6f}")
                    else:
                        print(f"   - {method}: Prediction successful")
                else:
                    print(f"   - {method}: Prediction failed")
                    
            except Exception as e:
                print(f"âŒ Error testing method {method}: {e}")
                results[method] = None
        
        return results
    
    def debug_ensemble_members(self):
        """ì•™ìƒë¸” ë©¤ë²„ ë””ë²„ê¹… ì •ë³´ ì¶œë ¥"""
        print(f"ğŸ” Debugging ensemble members...")
        
        if not self.ensemble_manager.ensemble_members:
            print(f"âš ï¸  No ensemble members available")
            return
        
        print(f"ğŸ“Š Ensemble Member Details:")
        print(f"   - Total Members: {len(self.ensemble_manager.ensemble_members)}")
        print(f"   - Memory Usage: {self.ensemble_manager.memory_usage:.2f}MB")
        print(f"   - Max Memory: {self.ensemble_manager.max_memory_mb}MB")
        
        for task_id, member_info in self.ensemble_manager.ensemble_members.items():
            print(f"\n   ğŸ“‹ Member {task_id}:")
            print(f"      - Performance: {member_info['performance']}")
            print(f"      - Size: {member_info['size_mb']:.2f}MB")
            print(f"      - Created: {member_info['created_at']:.2f}")
            print(f"      - Last Accessed: {member_info['last_accessed']:.2f}")
            print(f"      - Access Count: {member_info['access_count']}")
            
            # LoRA ê°€ì¤‘ì¹˜ ì •ë³´
            lora_weights = member_info['lora_weights']
            print(f"      - LoRA Layers: {len(lora_weights)}")
            for layer_key, layer_data in lora_weights.items():
                w_A_shape = layer_data['w_A'].shape
                w_B_shape = layer_data['w_B'].shape
                print(f"        - {layer_key}: w_A {w_A_shape}, w_B {w_B_shape}")
    
    def verify_ensemble_stacking_effects(self):
        """
        ì•™ìƒë¸” ë©¤ë²„ë“¤ì˜ ì ì¸µ íš¨ê³¼ ê²€ì¦
        
        Returns:
            Dict: ê° ë©¤ë²„ë³„ ì ì¸µ íš¨ê³¼ ì •ë³´
        """
        print(f"ğŸ” Verifying ensemble stacking effects...")
        
        if not self.ensemble_manager.ensemble_members:
            print(f"âš ï¸  No ensemble members available for verification")
            return None
        
        stacking_info = {}
        
        for task_id, member_info in self.ensemble_manager.ensemble_members.items():
            lora_weights = member_info['lora_weights']
            
            # ê° ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ í¬ê¸° ë¶„ì„
            layer_info = {}
            total_params = 0
            
            for layer_key, layer_data in lora_weights.items():
                w_A_params = layer_data['w_A'].numel()
                w_B_params = layer_data['w_B'].numel()
                layer_params = w_A_params + w_B_params
                
                layer_info[layer_key] = {
                    'w_A_shape': layer_data['w_A'].shape,
                    'w_B_shape': layer_data['w_B'].shape,
                    'w_A_params': w_A_params,
                    'w_B_params': w_B_params,
                    'total_params': layer_params
                }
                
                total_params += layer_params
            
            stacking_info[task_id] = {
                'layer_info': layer_info,
                'total_params': total_params,
                'size_mb': member_info['size_mb'],
                'performance': member_info['performance']
            }
            
            print(f"ğŸ“Š Task {task_id} Stacking Analysis:")
            print(f"   - Total Parameters: {total_params:,}")
            print(f"   - Size: {member_info['size_mb']:.2f}MB")
            print(f"   - Performance: {member_info['performance']}")
            
            # ì²« ë²ˆì§¸ ë ˆì´ì–´ì˜ ê°€ì¤‘ì¹˜ í¬ê¸°ë¡œ ì ì¸µ íš¨ê³¼ ì¶”ì •
            if layer_info:
                first_layer = list(layer_info.values())[0]
                estimated_base_params = first_layer['total_params']
                stacking_ratio = total_params / estimated_base_params if estimated_base_params > 0 else 1.0
                
                print(f"   - Estimated Stacking Ratio: {stacking_ratio:.2f}x")
                if stacking_ratio > 1.0:
                    print(f"   - âœ… Multiple LoRA stacks detected (all stacking effects included)")
            else:
                    print(f"   - â„¹ï¸  Single LoRA stack (no stacking yet)")
        
        return stacking_info
    
    def test_ensemble_stacking_consistency(self, trans_obs_0, actions):
        """
        ì•™ìƒë¸” ë©¤ë²„ë“¤ì˜ ì ì¸µ íš¨ê³¼ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            
        Returns:
            Dict: ì¼ê´€ì„± í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        print(f"ğŸ§ª Testing ensemble stacking consistency...")
        
        if not self.ensemble_manager.ensemble_members:
            print(f"âš ï¸  No ensemble members available for testing")
            return None
        
        # í˜„ì¬ ëª¨ë¸ì˜ ì˜ˆì¸¡ (ê¸°ì¤€)
        with torch.no_grad():
            current_pred = self.wm.rollout(obs_0=trans_obs_0, act=actions)[0]
        
        consistency_results = {}
        
        for task_id, member_info in self.ensemble_manager.ensemble_members.items():
            try:
                lora_weights = member_info['lora_weights']
                
                # ì•™ìƒë¸” ë©¤ë²„ì˜ LoRA ê°€ì¤‘ì¹˜ ì ìš©
                success = self.ensemble_manager._apply_lora_weights(lora_weights)
                
                if not success:
                    print(f"âŒ Failed to apply LoRA weights for member {task_id}")
                    continue
                
                # ì•™ìƒë¸” ë©¤ë²„ë¡œ ì˜ˆì¸¡ ìˆ˜í–‰
                with torch.no_grad():
                    member_pred = self.wm.rollout(obs_0=trans_obs_0, act=actions)[0]
                
                # í˜„ì¬ ì˜ˆì¸¡ê³¼ì˜ ì°¨ì´ ê³„ì‚°
                visual_diff = torch.mean(torch.abs(member_pred["visual"] - current_pred["visual"]))
                proprio_diff = torch.mean(torch.abs(member_pred["proprio"] - current_pred["proprio"]))
                
                consistency_results[task_id] = {
                    'visual_diff': visual_diff.item(),
                    'proprio_diff': proprio_diff.item(),
                    'total_diff': (visual_diff + 0.3 * proprio_diff).item(),
                    'success': True
                }
                
                print(f"ğŸ“Š Task {task_id} Consistency Check:")
                print(f"   - Visual Diff: {visual_diff.item():.6f}")
                print(f"   - Proprio Diff: {proprio_diff.item():.6f}")
                print(f"   - Total Diff: {(visual_diff + 0.3 * proprio_diff).item():.6f}")
                
                if visual_diff.item() > 1e-3 or proprio_diff.item() > 1e-3:
                    print(f"   - âœ… Significant difference detected (stacking effects working)")
                else:
                    print(f"   - âš ï¸  Minimal difference (may indicate stacking issue)")
                
            except Exception as e:
                print(f"âŒ Error testing member {task_id}: {e}")
                consistency_results[task_id] = {
                    'error': str(e),
                    'success': False
                }
        
        # ì›ë˜ ìƒíƒœ ë³µì›
        if hasattr(self.wm.predictor, 'w_As') and hasattr(self.wm.predictor, 'w_Bs'):
            original_w_As = [w_A.weight.data.clone() for w_A in self.wm.predictor.w_As]
            original_w_Bs = [w_B.weight.data.clone() for w_B in self.wm.predictor.w_Bs]
            self.ensemble_manager._restore_lora_weights(original_w_As, original_w_Bs)
        
        return consistency_results
    
    def comprehensive_ensemble_test(self, trans_obs_0, actions, target_data=None):
        """
        ì¢…í•©ì ì¸ ì•™ìƒë¸” í…ŒìŠ¤íŠ¸ (ì ì¸µ íš¨ê³¼ ê²€ì¦ í¬í•¨)
        
        Args:
            trans_obs_0: ë³€í™˜ëœ ì´ˆê¸° ê´€ì¸¡
            actions: í–‰ë™ ì‹œí€€ìŠ¤
            target_data: íƒ€ê²Ÿ ë°ì´í„° (ì„ íƒì‚¬í•­)
            
        Returns:
            Dict: ëª¨ë“  í…ŒìŠ¤íŠ¸ ê²°ê³¼
        """
        print(f"ğŸ§ª Running comprehensive ensemble test with stacking verification...")
        
        test_results = {
            'ensemble_info': self.get_ensemble_info(),
            'lora_loading_test': False,
            'ensemble_inference_test': None,
            'ensemble_performance_test': None,
            'ensemble_methods_test': None,
            'stacking_verification': None,
            'stacking_consistency_test': None,
            'debug_info': None
        }
        
        # 1. LoRA ë¡œë”© í…ŒìŠ¤íŠ¸
        print(f"\n1ï¸âƒ£ Testing LoRA loading...")
        test_results['lora_loading_test'] = self.test_lora_loading()
        
        # 2. ì•™ìƒë¸” ì¶”ë¡  í…ŒìŠ¤íŠ¸
        print(f"\n2ï¸âƒ£ Testing ensemble inference...")
        individual_preds, ensemble_pred = self.test_ensemble_inference(trans_obs_0, actions, target_data)
        test_results['ensemble_inference_test'] = {
            'individual_predictions': individual_preds,
            'ensemble_prediction': ensemble_pred
        }
        
        # 3. ì•™ìƒë¸” ì„±ëŠ¥ í‰ê°€ í…ŒìŠ¤íŠ¸
        if target_data is not None:
            print(f"\n3ï¸âƒ£ Testing ensemble performance evaluation...")
            test_results['ensemble_performance_test'] = self.test_ensemble_performance(trans_obs_0, actions, target_data)
        
        # 4. ë‹¤ì–‘í•œ ì•™ìƒë¸” ë°©ë²• í…ŒìŠ¤íŠ¸
        print(f"\n4ï¸âƒ£ Testing different ensemble methods...")
        test_results['ensemble_methods_test'] = self.test_ensemble_methods(trans_obs_0, actions, target_data)
        
        # 5. ğŸ”§ ì ì¸µ íš¨ê³¼ ê²€ì¦
        print(f"\n5ï¸âƒ£ Verifying ensemble stacking effects...")
        test_results['stacking_verification'] = self.verify_ensemble_stacking_effects()
        
        # 6. ğŸ”§ ì ì¸µ íš¨ê³¼ ì¼ê´€ì„± í…ŒìŠ¤íŠ¸
        print(f"\n6ï¸âƒ£ Testing ensemble stacking consistency...")
        test_results['stacking_consistency_test'] = self.test_ensemble_stacking_consistency(trans_obs_0, actions)
        
        # 7. ë””ë²„ê¹… ì •ë³´
        print(f"\n7ï¸âƒ£ Debugging ensemble members...")
        self.debug_ensemble_members()
        test_results['debug_info'] = "Debug info printed to console"
        
        print(f"\nâœ… Comprehensive ensemble test with stacking verification completed!")
        
        # ğŸ”§ í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½
        print(f"\nğŸ“Š TEST SUMMARY:")
        print(f"   - LoRA Loading: {'âœ… PASS' if test_results['lora_loading_test'] else 'âŒ FAIL'}")
        print(f"   - Ensemble Inference: {'âœ… PASS' if test_results['ensemble_inference_test']['ensemble_prediction'] is not None else 'âŒ FAIL'}")
        
        if test_results['stacking_verification']:
            print(f"   - Stacking Verification: âœ… PASS")
            for task_id, info in test_results['stacking_verification'].items():
                print(f"     - Task {task_id}: {info['total_params']:,} params, {info['size_mb']:.2f}MB")
        
        if test_results['stacking_consistency_test']:
            print(f"   - Stacking Consistency: âœ… PASS")
            for task_id, result in test_results['stacking_consistency_test'].items():
                if result.get('success', False):
                    print(f"     - Task {task_id}: Diff {result['total_diff']:.6f}")
        
        return test_results
    
    def check_task_change(self, new_task_id):
        """
        íƒœìŠ¤í¬ ì „í™˜ì„ ê°ì§€í•˜ê³  task_changed í”Œë˜ê·¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        base_online_loraì˜ ë©”ì„œë“œë¥¼ í˜¸ì¶œí•˜ê³  ê²°ê³¼ë¥¼ ë™ê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            new_task_id (int): ìƒˆë¡œìš´ íƒœìŠ¤í¬ ID
            
        Returns:
            bool: íƒœìŠ¤í¬ê°€ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
        """
        # base_online_loraì˜ check_task_change í˜¸ì¶œ
        task_changed = self.base_online_lora.check_task_change(new_task_id)
        
        # ê²°ê³¼ ë™ê¸°í™”
        self.task_changed = self.base_online_lora.task_changed
        self.current_task_id = self.base_online_lora.current_task_id
        self.stacks_in_current_task = self.base_online_lora.stacks_in_current_task
        
        return task_changed
    
    def reset_task_changed_flag(self):
        """task_changed í”Œë˜ê·¸ë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤."""
        self.base_online_lora.reset_task_changed_flag()
        self.task_changed = self.base_online_lora.task_changed
    
    def _apply_lora_weights(self, lora_weights):
        """
        LoRA ê°€ì¤‘ì¹˜ë¥¼ í˜„ì¬ ëª¨ë¸ì— ì ìš©í•©ë‹ˆë‹¤.
        LoRAEnsembleManagerì˜ ë¡œì§ì„ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Args:
            lora_weights: Dict - {'layer_0': {'w_A': tensor, 'w_B': tensor}, ...}
            
        Returns:
            bool: ì ìš© ì„±ê³µ ì—¬ë¶€
        """
        try:
            if not hasattr(self.wm, 'predictor'):
                print(f"âš ï¸  World model doesn't have predictor. Cannot apply LoRA weights.")
                return False
                
            predictor = self.wm.predictor
            
            if not (hasattr(predictor, 'w_As') and hasattr(predictor, 'w_Bs')):
                print(f"âš ï¸  Predictor doesn't have w_As/w_Bs. Cannot apply LoRA weights.")
                return False
            
            # ì²« ë²ˆì§¸ ìŠ¤íƒ (ëª¨ë“  ì ì¸µ íš¨ê³¼ê°€ í¬í•¨ëœ ë ˆì´ì–´ë“¤)ì— ì•™ìƒë¸” ë©¤ë²„ì˜ ê°€ì¤‘ì¹˜ ì ìš©
            w_As = predictor.w_As
            w_Bs = predictor.w_Bs
            
            layers_per_stack = 12
            
            with torch.no_grad():
                for i in range(min(layers_per_stack, len(w_As))):
                    layer_key = f'layer_{i}'
                    
                    if layer_key in lora_weights:
                        w_As[i].weight.data.copy_(lora_weights[layer_key]['w_A'])
                        w_Bs[i].weight.data.copy_(lora_weights[layer_key]['w_B'])
                    else:
                        print(f"âš ï¸  Layer {layer_key} not found in lora_weights")
                        return False
            
            return True
                
        except Exception as e:
            print(f"âŒ Error applying LoRA weights: {e}")
            import traceback
            traceback.print_exc()
            return False
    
    def _restore_lora_weights(self, original_w_As, original_w_Bs):
        """
        ì›ë˜ LoRA ê°€ì¤‘ì¹˜ë¥¼ ë³µì›í•©ë‹ˆë‹¤.
        LoRAEnsembleManagerì˜ ë¡œì§ì„ ì§ì ‘ êµ¬í˜„í•©ë‹ˆë‹¤.
        
        Args:
            original_w_As: ë°±ì—…ëœ w_As ë¦¬ìŠ¤íŠ¸
            original_w_Bs: ë°±ì—…ëœ w_Bs ë¦¬ìŠ¤íŠ¸
        """
        try:
            predictor = self.wm.predictor
            w_As = predictor.w_As
            w_Bs = predictor.w_Bs
            
            with torch.no_grad():
                for i, (orig_A, orig_B) in enumerate(zip(original_w_As, original_w_Bs)):
                    w_As[i].weight.data.copy_(orig_A)
                    w_Bs[i].weight.data.copy_(orig_B)
            
            print(f"âœ… Restored original LoRA weights")
            
        except Exception as e:
            print(f"âŒ Error restoring LoRA weights: {e}")
            import traceback
            traceback.print_exc()
