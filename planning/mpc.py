import torch
import hydra
import copy
import numpy as np
from einops import rearrange, repeat
from utils import slice_trajdict_with_t
from .base_planner import BasePlanner


class MPCPlanner(BasePlanner):
    """
    an online planner so feedback from env is allowed
    """

    def __init__(
        self,
        max_iter,
        n_taken_actions,
        sub_planner,
        wm,
        env,  # for online exec
        action_dim,
        objective_fn,
        preprocessor,
        evaluator,
        wandb_run,
        logging_prefix="mpc",
        log_filename="logs.json",
        ensemble_manager=None,  # ğŸ”§ ì•™ìƒë¸” ë§¤ë‹ˆì € ì¶”ê°€
        **kwargs,
    ):
        super().__init__(
            wm,
            action_dim,
            objective_fn,
            preprocessor,
            evaluator,
            wandb_run,
            log_filename,
        )
        self.env = env
        self.max_iter = np.inf if max_iter is None else max_iter
        self.n_taken_actions = n_taken_actions
        self.logging_prefix = logging_prefix
        sub_planner["_target_"] = sub_planner["target"]
        self.sub_planner = hydra.utils.instantiate(
            sub_planner,
            wm=self.wm,
            action_dim=self.action_dim,
            objective_fn=self.objective_fn,
            preprocessor=self.preprocessor,
            evaluator=self.evaluator,  # evaluator is shared for mpc and sub_planner
            wandb_run=self.wandb_run,
            log_filename=None,
        )
        self.is_success = None
        self.action_len = None  # keep track of the step each traj reaches success
        self.iter = 0
        self.planned_actions = []
        self.ensemble_manager = ensemble_manager  # ğŸ”§ ì•™ìƒë¸” ë§¤ë‹ˆì € ì €ì¥
        self.iteration_metrics = []
        self.evaluated_iterations_count = 0  # ì‹¤ì œë¡œ í‰ê°€ê°€ ìˆ˜í–‰ëœ iteration íšŸìˆ˜

    def _apply_success_mask(self, actions):
        device = actions.device
        mask = torch.tensor(self.is_success).bool()
        actions[mask] = 0
        masked_actions = rearrange(
            actions[mask], "... (f d) -> ... f d", f=self.evaluator.frameskip
        )
        masked_actions = self.preprocessor.normalize_actions(masked_actions.cpu())
        masked_actions = rearrange(masked_actions, "... f d -> ... (f d)")
        actions[mask] = masked_actions.to(device)
        return actions

    def plan(self, obs_0, obs_g, actions=None):
        """
        actions is NOT used
        Returns:
            actions: (B, T, action_dim) torch.Tensor
        """

        n_evals = obs_0["visual"].shape[0]
        self.is_success = np.zeros(n_evals, dtype=bool)
        self.action_len = np.full(n_evals, np.inf)
        init_obs_0, init_state_0 = self.evaluator.get_init_cond()

        cur_obs_0 = obs_0
        memo_actions = None
        # ì‹¤ì œ í™˜ê²½ ê¸°ì¤€ CD ì¶”ì  (CD ì¦ê°€ í–‰ë™ ê±°ë¥´ê¸°)
        prev_cd = None  # ì´ì „ iterationì˜ CD ì €ì¥
        # ğŸ”§ ì„ì‹œ: ì„±ê³µ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ max_iterê¹Œì§€ ìˆ˜í–‰
        self.iter = 0  # plan() í˜¸ì¶œ ì‹œë§ˆë‹¤ iter ì´ˆê¸°í™”
        self.iteration_metrics = []
        self.evaluated_iterations_count = 0  # ì‹¤ì œë¡œ í‰ê°€ê°€ ìˆ˜í–‰ëœ iteration íšŸìˆ˜ ì´ˆê¸°í™”
        print(f"[MPC] plan() called: max_iter={self.max_iter}, initial iter={self.iter}")
        while self.iter < self.max_iter:
            self.sub_planner.logging_prefix = f"plan_{self.iter}"
            
            # ğŸ”§ MPCì—ì„œëŠ” ì¼ë°˜ í”Œë˜ë‹ ì‚¬ìš© (ì•™ìƒë¸”ì€ íƒœìŠ¤í¬ ì „í™˜ ì‹œì—ë§Œ ì‚¬ìš©)
            actions, _ = self.sub_planner.plan(
                obs_0=cur_obs_0,
                obs_g=obs_g,
                actions=memo_actions,
            )  # (b, t, act_dim)
            taken_actions = actions.detach()[:, : self.n_taken_actions]
            self._apply_success_mask(taken_actions)
            memo_actions = actions.detach()[:, self.n_taken_actions :]
            self.planned_actions.append(taken_actions)

            print(f"MPC iter {self.iter} Eval ------- ")
            # ğŸ”§ ê° iterationì—ì„œëŠ” ìƒˆë¡œ ì¶”ê°€ëœ actionë§Œ í‰ê°€ (í˜„ì¬ ìƒíƒœì—ì„œ ì‹œì‘)
            # ì²« ë²ˆì§¸ iterationì—ì„œë§Œ ì´ˆê¸° ì¡°ê±´ ì„¤ì •
            if self.iter == 0:
                print(f"[MPC FIX] Setting initial conditions for iter {self.iter}")
                # Reset the flag for initial CD measurement
                self.evaluator._initial_cd_measured = False
                self.evaluator.assign_init_cond(
                    obs_0=init_obs_0,
                    state_0=init_state_0,
                )
            else:
                print(f"[MPC FIX] Using updated conditions from previous iter for iter {self.iter}")
            
            # ìƒˆë¡œ ì¶”ê°€ëœ actionë§Œ í‰ê°€ (í˜„ì¬ í™˜ê²½ ìƒíƒœì—ì„œ ì‹œì‘)
            if self.iter < 3:
                self.evaluator.force_recenter_for_next_rollout()
            logs, successes, e_obses, e_states = self.evaluator.eval_actions(
                taken_actions,  # ì „ì²´ action_so_farê°€ ì•„ë‹Œ ìƒˆë¡œ ì¶”ê°€ëœ actionë§Œ
                self.action_len,
                filename=f"{self.logging_prefix}_plan{self.iter}",
                save_video=True,
            )
            
            # ğŸ”§ CD ì²´í¬ ì „ì— final loss ì €ì¥ (iter == max_iter - 1ì¼ ë•Œ, CD reverted ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´)
            if (
                getattr(self.evaluator, "workspace", None) is not None
                and getattr(self.evaluator.workspace, "final_mpc_visual_loss", None) is None
                and isinstance(self.max_iter, (int, float))
                and self.iter == self.max_iter - 1
            ):
                value = None
                visual_key = None
                
                # ì›ë³¸ logsì—ì„œ visual_loss ì°¾ê¸° (ë³€í™˜ ì „)
                print(f"[MPC] Searching for visual_loss in logs keys: {list(logs.keys())[:10]}...")  # ë””ë²„ê¹…
                for key in logs.keys():
                    lower_key = key.lower()
                    if "visual" in lower_key and "loss" in lower_key:
                        visual_key = key
                        value = logs[visual_key]
                        print(f"[MPC] Found visual_loss: key={visual_key}, value={value}")
                        break
                
                if value is None:
                    print(f"[MPC] âš ï¸  visual_loss not found in logs. Available keys: {list(logs.keys())}")
                
                # ê°’ ì •ê·œí™”
                if value is not None:
                    if isinstance(value, (list, tuple, np.ndarray)):
                        if len(value) > 0:
                            value = float(np.mean(value))
                        else:
                            value = None
                    elif isinstance(value, (torch.Tensor,)):
                        value = float(value.item())
                    elif isinstance(value, (np.floating, float, int, np.integer)):
                        value = float(value)
                    else:
                        value = None
                
                # ì €ì¥
                if value is not None:
                    self.evaluator.workspace.final_mpc_visual_loss = {
                        "key": visual_key,
                        "value": value,
                        "iteration": self.iter,
                    }
                    print(f"[MPC] Saved final visual_loss for iter {self.iter}: {value:.6f} (from {visual_key})")
            
            # ---- CD ì¦ê°€ í–‰ë™ ì°¨ë‹¨ ë¡œì§ ----
            # evaluator ë¡œê·¸ì—ì„œ mean_chamfer_distance ì¶”ì¶œ
            cur_cd = logs.get("mean_chamfer_distance", None)
            if cur_cd is None:
                # fallback: chamfer_distance ì§ì ‘ í™•ì¸
                cur_cd = logs.get("chamfer_distance", None)
            if cur_cd is not None:
                # ìŠ¤ì¹¼ë¼ ê°’ì´ë©´ ë°°ì—´ë¡œ ë³€í™˜
                if not isinstance(cur_cd, (list, tuple, np.ndarray)):
                    cur_cd = np.array([cur_cd])
                elif isinstance(cur_cd, (list, tuple)):
                    cur_cd = np.array(cur_cd)
                # ì²« ë²ˆì§¸ iterationì´ ì•„ë‹ˆê³ , ì´ì „ CDê°€ ìˆëŠ” ê²½ìš°ì—ë§Œ ë¹„êµ
                if prev_cd is not None:
                    # í˜„ì¬ CDê°€ ì´ì „ CDë³´ë‹¤ ë‚˜ìœ ê²½ìš° (ì¦ê°€)
                    if np.any(cur_cd > prev_cd):
                        print(
                            f"[MPC] CD increased: prev={prev_cd}, cur={cur_cd}. Reverting this step."
                        )
                        # ë°©ê¸ˆ ì¶”ê°€í•œ í–‰ë™ ì·¨ì†Œ
                        if len(self.planned_actions) > 0:
                            self.planned_actions.pop()
                        # memo_actionsë„ ì´ì „ ìƒíƒœë¡œ ë˜ëŒë¦¬ê¸° (ë‹¤ìŒ ê³„íšì„ ìœ„í•´)
                        memo_actions = None
                        
                        # ğŸ”§ CD ì²´í¬ ì „ì— logsì—ì„œ visual_lossì™€ chamfer_distance ì¶”ì¶œ
                        visual_loss_value = None
                        chamfer_distance_value = None
                        
                        for key, value in logs.items():
                            key_lower = key.lower()
                            # visual_loss ì¶”ì¶œ
                            if visual_loss_value is None and "visual" in key_lower and "loss" in key_lower and "dist" not in key_lower:
                                if isinstance(value, (list, tuple, np.ndarray)):
                                    if len(value) > 0:
                                        visual_loss_value = float(np.mean(value))
                                elif isinstance(value, (torch.Tensor,)):
                                    visual_loss_value = float(value.item())
                                elif isinstance(value, (np.floating, float, int, np.integer)):
                                    visual_loss_value = float(value)
                            
                            # chamfer_distance ì¶”ì¶œ (cur_cdì™€ ë™ì¼í•œ ê°’ì´ì§€ë§Œ logsì—ì„œ ì§ì ‘ ê°€ì ¸ì˜´)
                            if chamfer_distance_value is None and "chamfer" in key_lower:
                                if isinstance(value, (list, tuple, np.ndarray)):
                                    if len(value) > 0:
                                        chamfer_distance_value = float(np.mean(value))
                                elif isinstance(value, (torch.Tensor,)):
                                    chamfer_distance_value = float(value.item())
                                elif isinstance(value, (np.floating, float, int, np.integer)):
                                    chamfer_distance_value = float(value)
                        
                        # CD ì¦ê°€ëœ iterationë„ metricsì— ê¸°ë¡ (visual_lossì™€ chamfer_distance í¬í•¨)
                        reverted_logs = {
                            f"{self.logging_prefix}/reverted": True,
                            f"{self.logging_prefix}/prev_cd": float(prev_cd[0]),
                        }
                        # ğŸ”§ chamfer_distance ì¶”ê°€
                        if chamfer_distance_value is not None:
                            reverted_logs[f"{self.logging_prefix}/chamfer_distance"] = chamfer_distance_value
                        else:
                            # fallback: cur_cd ì‚¬ìš©
                            reverted_logs[f"{self.logging_prefix}/chamfer_distance"] = float(cur_cd[0])
                        
                        # ğŸ”§ visual_loss ì¶”ê°€
                        if visual_loss_value is not None:
                            reverted_logs[f"{self.logging_prefix}/visual_loss"] = visual_loss_value
                        
                        self.iteration_metrics.append({
                            "iter": self.iter,
                            "logs": reverted_logs,
                            "reverted": True,
                        })
                        print(f"[MPC] Added reverted iteration_metrics for iter {self.iter}, total count: {len(self.iteration_metrics)}")
                        
                        # iterëŠ” ì¦ê°€ì‹œí‚¤ë˜, ìƒíƒœ ì—…ë°ì´íŠ¸ ì—†ì´ ë‹¤ìŒ ë°˜ë³µìœ¼ë¡œ ì§„í–‰
                        self.iter += 1
                        print(f"[MPC] Iter {self.iter} (reverted due to CD increase)\n")
                        continue
                    else:
                        # ê°œì„  ë˜ëŠ” ë™ì¼: prev_cd ì—…ë°ì´íŠ¸
                        prev_cd = cur_cd.copy()
                        print(f"[MPC] CD improved or same: {cur_cd[0]:.6f}")
                else:
                    # ì²« ë²ˆì§¸ iteration: prev_cd ì´ˆê¸°í™”
                    prev_cd = cur_cd.copy()
                    print(f"[MPC] Initial CD: {cur_cd[0]:.6f}")
            
            new_successes = successes & ~self.is_success  # Identify new successes
            self.is_success = (
                self.is_success | successes
            )  # Update overall success status
            self.action_len[new_successes] = (
                (self.iter + 1) * self.n_taken_actions
            )  # Update only for the newly successful trajectories

            print("self.is_success: ", self.is_success)
            
            logs = {f"{self.logging_prefix}/{k}": v for k, v in logs.items()}
            logs.update({"step": self.iter + 1})
            
            # ğŸ”§ WandB ë¡œê¹…: visual_lossì™€ chamfer_distance ëª¨ë‘ ë§¤ iterationë§ˆë‹¤ í•­ìƒ ë¡œê¹…
            if self.wandb_run is not None:
                self.wandb_run.log(logs)
            
            # iteration_metricsì—ëŠ” ëª¨ë“  logs ì €ì¥ (í•„í„°ë§ ì—†ìŒ)
            self.dump_logs(logs)
            self.iteration_metrics.append(
                {
                    "iter": self.iter,
                    "logs": {k: v for k, v in logs.items()},
                    "reverted": False,
                }
            )
            self.evaluated_iterations_count += 1  # ì‹¤ì œ í‰ê°€ ìˆ˜í–‰ íšŸìˆ˜ ì¦ê°€
            print(f"[MPC] Added iteration_metrics for iter {self.iter}, total count: {len(self.iteration_metrics)}, evaluated count: {self.evaluated_iterations_count}")

            # update evaluator's init conditions with new env feedback
            e_final_obs = slice_trajdict_with_t(e_obses, start_idx=-1)
            cur_obs_0 = e_final_obs
            e_final_state = e_states[:, -1]
            print(f"[MPC FIX] Updating conditions for next iter: final_state shape {e_final_state.shape}")
            self.evaluator.assign_init_cond(
                obs_0=e_final_obs,
                state_0=e_final_state,
            )
            self.iter += 1
            self.sub_planner.logging_prefix = f"plan_{self.iter}"

        # ìµœì¢… ê²°ê³¼ ë°˜í™˜
        planned_actions = torch.cat(self.planned_actions, dim=1)
        
        # ğŸ”§ final output: ì´ˆê¸° ìƒíƒœì—ì„œ ì „ì²´ ê¶¤ì (action_so_far) í‰ê°€
        print("[MPC] Evaluating final output from initial state with full trajectory")
        self.evaluator.assign_init_cond(
            obs_0=init_obs_0,
            state_0=init_state_0,
        )
        
        # ğŸ”§ íƒœìŠ¤í¬ IDë¥¼ í¬í•¨í•œ final output íŒŒì¼ëª… ìƒì„±
        task_id = None
        if hasattr(self.evaluator, 'workspace') and self.evaluator.workspace is not None:
            task_id = getattr(self.evaluator.workspace, 'current_task_id', None)
        final_filename = f"output_final_task_{task_id:02d}" if task_id is not None else "output_final"
        
        # ì „ì²´ ê¶¤ì ì„ ì´ˆê¸° ìƒíƒœì—ì„œ í‰ê°€
        final_logs, final_successes, final_e_obses, final_e_states = self.evaluator.eval_actions(
            planned_actions,  # ì „ì²´ ê¶¤ì 
            self.action_len,
            filename=final_filename,
            save_video=True,
        )
        print(f"[MPC] Final output CD: {final_logs.get('mean_chamfer_distance', final_logs.get('chamfer_distance', 'N/A'))}")
        
        if self.wandb_run is not None:
            # mean_visual_distëŠ” visual_lossê°€ ì•„ë‹ˆë¯€ë¡œ ë¡œê¹…í•˜ì§€ ì•ŠìŒ
            # íƒœìŠ¤í¬ë³„ final output ë¡œê·¸ í‚¤ ì°¾ê¸°
            final_log_key = None
            for key in final_logs.keys():
                if "output_final" in key and "chamfer" in key.lower():
                    final_log_key = key
                    break
            if final_log_key:
                self.wandb_run.log({"chamfer_distance": final_logs[final_log_key]}, step=self.iter)

        # ğŸ”§ ìµœì¢… ê²°ê³¼ ìš”ì•½ ì¶œë ¥
        print("\n" + "="*80)
        print("[MPC] Final Results Summary")
        print("="*80)
        
        # Steps to Success ê³„ì‚°
        success_mask = self.is_success
        if np.any(success_mask):
            steps_to_success = self.action_len[success_mask]
            print(f"Steps to Success: {steps_to_success.tolist()}")
            print(f"  - Mean: {np.mean(steps_to_success):.2f}")
            print(f"  - Min: {np.min(steps_to_success):.2f}")
            print(f"  - Max: {np.max(steps_to_success):.2f}")
        else:
            print("Steps to Success: N/A (no successful trajectories)")
        
        # LoRA Adaptation Time í†µê³„
        if hasattr(self.evaluator, 'workspace') and self.evaluator.workspace is not None:
            if hasattr(self.evaluator.workspace, 'online_learner') and self.evaluator.workspace.online_learner is not None:
                online_learner = self.evaluator.workspace.online_learner
                # EnsembleOnlineLoraì¸ ê²½ìš° base_online_loraì—ì„œ ê°€ì ¸ì˜¤ê¸° (ë” ì •í™•í•œ ê°’)
                if hasattr(online_learner, 'base_online_lora'):
                    adaptation_times = online_learner.base_online_lora.adaptation_times
                elif hasattr(online_learner, 'adaptation_times'):
                    adaptation_times = online_learner.adaptation_times
                else:
                    adaptation_times = []
                
                if len(adaptation_times) > 0:
                    print(f"\nLoRA Adaptation Time (total {len(adaptation_times)} updates):")
                    print(f"  - Min: {min(adaptation_times):.4f} seconds")
                    print(f"  - Max: {max(adaptation_times):.4f} seconds")
                    print(f"  - Mean: {np.mean(adaptation_times):.4f} seconds")
                    print(f"  - Total: {sum(adaptation_times):.4f} seconds")
                else:
                    print("\nLoRA Adaptation Time: N/A (no LoRA updates performed)")
            else:
                print("\nLoRA Adaptation Time: N/A (LoRA not enabled)")
        else:
            print("\nLoRA Adaptation Time: N/A (workspace not available)")
        
        print("="*80 + "\n")

        return planned_actions, self.action_len

