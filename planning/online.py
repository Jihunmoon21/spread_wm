import torch
import time
from collections import deque
from utils import move_to_device # spread_wmì˜ ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜

class OnlineLora:
    """
    Online-LoRA í•™ìŠµê³¼ ê´€ë ¨ëœ ëª¨ë“  ë¡œì§ì„ ì „ë‹´í•˜ëŠ” í´ë˜ìŠ¤.
    - ì˜µí‹°ë§ˆì´ì €, ì†ì‹¤ í•¨ìˆ˜ ë“± í•™ìŠµ ê´€ë ¨ ê°ì²´ë¥¼ ë‚´ë¶€ì ìœ¼ë¡œ ìƒì„±í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.
    - Loss Windowë¥¼ í†µí•´ í•™ìŠµ ì•ˆì •ì„±ì„ ë†’ì´ê³ , LoRA ì ì¸µ ì‹œì ì„ íŒë‹¨í•©ë‹ˆë‹¤.
    """
    def __init__(self, workspace):
        """
        OnlineLora ëª¨ë“ˆì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.
        
        Args:
            workspace (PlanWorkspace): í•„ìš”í•œ ëª¨ë“  ê°ì²´(wm, cfg ë“±)ì— ì ‘ê·¼í•  ìˆ˜ ìˆëŠ” ìƒìœ„ ì›Œí¬ìŠ¤í˜ì´ìŠ¤.
        """
        self.workspace = workspace
        self.wm = workspace.wm
        self.cfg = workspace.cfg_dict.get("lora", {}) # lora ê´€ë ¨ ì„¤ì •ë§Œ ê°€ì ¸ì˜´
        self.device = next(self.wm.parameters()).device

        # --- 1. í•™ìŠµì— í•„ìš”í•œ ê°ì²´ë“¤ ì´ˆê¸°í™” ---
        self.is_online_lora = self.cfg.get("online", False) # lora ì ì¸µ ë“±ì„ í¬í•¨í•˜ëŠ” ì „ì²´ ì˜¨ë¼ì¸ ê¸°ëŠ¥ í™œì„±í™” ì—¬ë¶€
        
        # Loss ê°€ì¤‘ì¹˜ ì„¤ì • (ì„¤ì • íŒŒì¼ì— ì—†ìœ¼ë¯€ë¡œ ê¸°ë³¸ê°’ ì‚¬ìš©)
        self.visual_loss_weight = self.cfg.get("visual_loss_weight", 1.0)
        self.proprio_loss_weight = self.cfg.get("proprio_loss_weight", 0.3)

        # ê¸°ì¡´ ëª¨ë¸ íŒŒë¼ë¯¸í„° ê³ ì •
        print("INFO: Freezing all non-LoRA parameters for online learning.")
        for name, param in self.wm.named_parameters():
            if 'lora' not in name:
                param.requires_grad = False
        
        # í•™ìŠµ ëŒ€ìƒ LoRA íŒŒë¼ë¯¸í„° í•„í„°ë§ ë° ì˜µí‹°ë§ˆì´ì € ìƒì„±
        params_to_train = [p for p in self.wm.parameters() if p.requires_grad]
        if not params_to_train:
            raise ValueError("No trainable LoRA parameters found. Check if LoRA wrappers are correctly applied.")
        
        self.optimizer = torch.optim.Adam(params_to_train, lr=self.cfg.get("lr", 1e-4))
        self.loss_fn = torch.nn.MSELoss()

        # --- 2. Online-LoRA (ì ì¸µ) ê´€ë ¨ ë³€ìˆ˜ ì´ˆê¸°í™” ---
        # í•˜ì´ë¸Œë¦¬ë“œ ì ì¸µ ì„¤ì • (online ëª¨ë“œ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ í•­ìƒ ì´ˆê¸°í™”)
        self.hybrid_config = self.cfg.get("hybrid_stacking", {})
        self.hybrid_enabled = self.hybrid_config.get("enabled", False)
        self.task_based_stacking = self.hybrid_config.get("task_based_stacking", False)
        self.loss_based_stacking = self.hybrid_config.get("loss_based_stacking", False)
        self.max_stacks_per_task = self.hybrid_config.get("max_stacks_per_task", 3)
        self.stack_type_tracking = self.hybrid_config.get("stack_type_tracking", True)
        
        if self.is_online_lora:
            print("INFO: Initializing Online LoRA module for dynamic stacking.")
            self.loss_window = deque(maxlen=self.cfg.get("loss_window_length", 10))
            self.mean_threshold = self.cfg.get("loss_window_mean_threshold", 0.01)
            self.variance_threshold = self.cfg.get("loss_window_variance_threshold", 1e-5)
            self.min_steps_for_stack = self.cfg.get("min_steps_for_stack", 50)
            
            self.steps_since_last_stack = 0
            self.new_peak_detected = True
            self.last_loss_mean = float('inf')
            self.last_loss_var = float('inf')
            
            # íƒœìŠ¤í¬ë³„ ì ì¸µ ì¶”ì 
            self.stacks_in_current_task = 0
            self.current_task_id = 0
            self.task_changed = False  # íƒœìŠ¤í¬ ì „í™˜ ê°ì§€ í”Œë˜ê·¸
            self.stack_history = []  # ì ì¸µ íˆìŠ¤í† ë¦¬ (íƒ€ì…, ì‹œì , Loss ë“±)
            
            if self.hybrid_enabled:
                print(f"INFO: Hybrid stacking enabled - Task-based: {self.task_based_stacking}, Loss-based: {self.loss_based_stacking}")
                print(f"INFO: Max stacks per task: {self.max_stacks_per_task}")
        else:
            # online ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš°ì—ë„ ê¸°ë³¸ê°’ìœ¼ë¡œ ì´ˆê¸°í™”
            self.stacks_in_current_task = 0
            self.current_task_id = 0
            self.stack_history = []
            
        # ë§ˆì§€ë§‰ Loss ê°’ ì €ì¥ìš© (íƒœìŠ¤í¬ ì „í™˜ì„ ìœ„í•´)
        self.last_loss = None
        self.last_visual_loss = None
        self.last_proprio_loss = None
        
        # LoRA ì ì¸µ ì½œë°± í•¨ìˆ˜ (íƒœìŠ¤í¬ ì¶”ì ì„ ìœ„í•´)
        self.on_lora_stack_callback = None
        
        # ğŸ”§ LoRA adaptation time ì¶”ì  (ìµœì¢… ê²°ê³¼ ì¶œë ¥ìš©)
        self.adaptation_times = []


    def update(self, trans_obs_0, actions, e_obses):
        """
        í•˜ë‚˜ì˜ í•™ìŠµ ë‹¨ê³„ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë©”ì¸ ë©”ì†Œë“œ. PlanEvaluatorë¡œë¶€í„° í˜¸ì¶œë©ë‹ˆë‹¤.
        """
        # í•™ìŠµ ë‹¨ê³„ ìˆ˜í–‰ (ì˜ˆì¸¡, ì†ì‹¤ ê³„ì‚°, ì—­ì „íŒŒ, ì—…ë°ì´íŠ¸)
        start_time = time.time()
        total_loss_value = self._perform_training_step(trans_obs_0, actions, e_obses)
        adaptation_time = time.time() - start_time
        print(f"LoRA adaptation time: {adaptation_time:.4f} seconds")
        
        # ğŸ”§ adaptation time ì €ì¥ (ìµœì¢… ê²°ê³¼ ì¶œë ¥ìš©)
        self.adaptation_times.append(adaptation_time)
        
        # ë§ˆì§€ë§‰ Loss ê°’ ì €ì¥ (íƒœìŠ¤í¬ ì „í™˜ì„ ìœ„í•´)
        if total_loss_value is not None:
            self.last_loss = total_loss_value
        
        # í•™ìŠµì´ ì„±ê³µì ìœ¼ë¡œ ì´ë£¨ì–´ì¡Œê³ , online_lora ê¸°ëŠ¥ì´ í™œì„±í™”ë˜ì—ˆë‹¤ë©´
        if total_loss_value is not None and self.is_online_lora:
            # í•˜ì´ë¸Œë¦¬ë“œ ì ì¸µ ë¡œì§
            if self.hybrid_enabled:
                self._manage_hybrid_stacking(total_loss_value)
            else:
                # ê¸°ì¡´ Loss ê¸°ë°˜ ì ì¸µ ë¡œì§
                self._manage_loss_window_and_stacking(total_loss_value)


    def _perform_training_step(self, trans_obs_0, actions, e_obses):
        """ì‹¤ì œ ì˜ˆì¸¡, ì†ì‹¤ ê³„ì‚°, ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        try:
            print("--- Starting LoRA Online Learning ---")
            
            # 1. ì˜ˆì¸¡ (ê·¸ë˜ë””ì–¸íŠ¸ í™œì„±í™”)
            step_start = time.time()
            i_z_obses_pred, _ = self.wm.rollout(obs_0=trans_obs_0, act=actions)
            rollout_time = time.time() - step_start

            # 2. ì •ë‹µ ì¤€ë¹„ (ê·¸ë˜ë””ì–¸íŠ¸ ë¹„í™œì„±í™”)
            encode_start = time.time()
            with torch.no_grad():
                trans_obs_gt = self.workspace.data_preprocessor.transform_obs(e_obses)
                trans_obs_gt = move_to_device(trans_obs_gt, self.device)
                i_z_obses_gt = self.wm.encode_obs(trans_obs_gt)
            encode_time = time.time() - encode_start

            # 3. ì†ì‹¤ ê³„ì‚°
            loss_start = time.time()
            print("Computing loss...")
            frameskip = self.workspace.frameskip
            gt_proprio_resampled = i_z_obses_gt["proprio"][:, ::frameskip, :].detach()
            gt_visual_resampled = i_z_obses_gt["visual"][:, ::frameskip, :, :].detach()
            
            proprio_loss = self.loss_fn(i_z_obses_pred["proprio"], gt_proprio_resampled)
            visual_loss = self.loss_fn(i_z_obses_pred["visual"], gt_visual_resampled)
            
            total_loss = self.visual_loss_weight * visual_loss + self.proprio_loss_weight * proprio_loss
            loss_time = time.time() - loss_start
            
            print(f"Visual loss: {visual_loss.item():.6f}, Proprio loss: {proprio_loss.item():.6f}")
            print(f"Total loss: {total_loss.item():.6f}")

            # 4. ì—­ì „íŒŒ ë° ì—…ë°ì´íŠ¸
            backward_start = time.time()
            self.optimizer.zero_grad()
            total_loss.backward()
            self.optimizer.step()
            backward_time = time.time() - backward_start
            
            print(f"LoRA step timing - Rollout: {rollout_time:.4f}s, Encode: {encode_time:.4f}s, Loss: {loss_time:.4f}s, Backward: {backward_time:.4f}s")

            loss_value = total_loss.item()
            self.last_visual_loss = visual_loss.item()
            self.last_proprio_loss = proprio_loss.item()

            planner_iter = None
            if getattr(self.workspace, "planner", None) is not None:
                planner_iter = getattr(self.workspace.planner, "iter", None)

            if (
                getattr(self.workspace, "is_training_mode", True)
                and planner_iter == 1
                and getattr(self.workspace, "forward_transfer_metrics", None) is None
            ):
                self.workspace.forward_transfer_metrics = {
                    "total_loss": loss_value,
                    "visual_loss": self.last_visual_loss,
                    "proprio_loss": self.last_proprio_loss,
                    "iteration": planner_iter,
                }

            return loss_value

        except Exception as e:
            print(f"Error during training step: {e}")
            return None
        
        finally:
            # 5. ë©”ëª¨ë¦¬ ì •ë¦¬ (ì˜¤ë¥˜ ë°œìƒ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ì‹¤í–‰)
            # delì„ ìœ„í•´ ë³€ìˆ˜ê°€ ì„ ì–¸ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if 'i_z_obses_pred' in locals(): del i_z_obses_pred
            if 'i_z_obses_gt' in locals(): del i_z_obses_gt
            if 'total_loss' in locals(): del total_loss
            torch.cuda.empty_cache()
            print("--- LoRA Online Update Complete ---")
    # LoRA íŒŒë¼ë¯¸í„° ë³€í™”ëŸ‰ í™•ì¸ ì½”ë“œ ì¶”ê°€ ê°€ëŠ¥

    def _manage_loss_window_and_stacking(self, current_loss_value):
        """Loss Windowë¥¼ ì—…ë°ì´íŠ¸í•˜ê³ , LoRAë¥¼ ì¶”ê°€í• ì§€ ì—¬ë¶€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤."""
        self.loss_window.append(current_loss_value)
        self.steps_since_last_stack += 1
        
        if len(self.loss_window) < self.loss_window.maxlen:
            return

        loss_tensor = torch.tensor(list(self.loss_window))
        mean = torch.mean(loss_tensor)
        var = torch.var(loss_tensor)
        
        print(f"Loss Window >> Mean: {mean:.6f}, Variance: {var:.6f}")
        print(f"Steps since last stack: {self.steps_since_last_stack}/{self.min_steps_for_stack}")
        print(f"New peak detected: {self.new_peak_detected}")

        # `engine.py`ì˜ Loss Peak ê°ì§€ ë¡œì§
        if not self.new_peak_detected and mean > self.last_loss_mean + torch.sqrt(self.last_loss_var):
            self.new_peak_detected = True
            print("Loss peak detected after a plateau period.")

        # LoRA ì ì¸µ íŠ¸ë¦¬ê±° ì¡°ê±´
        if (self.new_peak_detected and
            mean < self.mean_threshold and
            var < self.variance_threshold and
            self.steps_since_last_stack > self.min_steps_for_stack):
            
            # ìµœëŒ€ ì ì¸µ íšŸìˆ˜ í™•ì¸
            if self.stacks_in_current_task >= self.max_stacks_per_task:
                print(f"âš ï¸  Max stacks per task ({self.max_stacks_per_task}) reached. Skipping loss-based stacking.")
                return
            
            print("! Loss plateau detected. Triggering LoRA stacking process !")
            
            # Loss ê¸°ë°˜ ì ì¸µ ìˆ˜í–‰
            success = self._perform_lora_stacking("loss_based", self.current_task_id, "loss_plateau")
            
            if success:
                # ì ì¸µ íšŸìˆ˜ ì¦ê°€
                self.stacks_in_current_task += 1
                
                # ì ì¸µ íˆìŠ¤í† ë¦¬ ê¸°ë¡
                if self.hybrid_enabled and self.stack_type_tracking:
                    self.stack_history.append({
                        'type': 'loss_based',
                        'task_id': self.current_task_id,
                        'reason': 'loss_plateau',
                        'step': self.steps_since_last_stack,
                        'loss_mean': mean,
                        'loss_var': var,
                        'timestamp': time.time()
                    })
                
                print(f"   - Loss mean: {mean:.6f} (threshold: {self.mean_threshold})")
                print(f"   - Loss variance: {var:.6f} (threshold: {self.variance_threshold})")
                print(f"   - Steps since last stack: {self.steps_since_last_stack}")
                print(f"   - Stacks in current task: {self.stacks_in_current_task}/{self.max_stacks_per_task}")
            
            # ìƒíƒœ ë³€ìˆ˜ ì´ˆê¸°í™”
            self.new_peak_detected = False
            self.last_loss_mean = mean
            self.last_loss_var = var
            self.steps_since_last_stack = 0
            self.loss_window.clear()

    def _manage_hybrid_stacking(self, current_loss_value):
        """
        í•˜ì´ë¸Œë¦¬ë“œ ì ì¸µ ë¡œì§: íƒœìŠ¤í¬ ê¸°ë°˜ê³¼ Loss ê¸°ë°˜ ì ì¸µì„ ëª¨ë‘ ê³ ë ¤í•©ë‹ˆë‹¤.
        """
        # Loss ê¸°ë°˜ ì ì¸µ (ê¸°ì¡´ ë¡œì§)
        if self.loss_based_stacking:
            self._manage_loss_window_and_stacking(current_loss_value)
    
    def trigger_task_based_stacking(self, task_id, reason="task_change"):
        """
        íƒœìŠ¤í¬ ê¸°ë°˜ ì ì¸µì„ íŠ¸ë¦¬ê±°í•©ë‹ˆë‹¤.
        
        Args:
            task_id: ìƒˆë¡œìš´ íƒœìŠ¤í¬ ID
            reason: ì ì¸µ ì´ìœ  ("task_change", "manual", etc.)
        """
        # hybrid_enabled ì†ì„±ì´ ì—†ê±°ë‚˜ Falseì¸ ê²½ìš° ì ì¸µí•˜ì§€ ì•ŠìŒ
        if not hasattr(self, 'hybrid_enabled') or not self.hybrid_enabled or not self.task_based_stacking:
            return False
            
        # íƒœìŠ¤í¬ê°€ ë³€ê²½ëœ ê²½ìš°
        if task_id != self.current_task_id:
            self.current_task_id = task_id
            self.stacks_in_current_task = 0
            print(f"ğŸ”„ Task changed to {task_id}. Resetting stack counter.")
        
        # ìµœëŒ€ ì ì¸µ íšŸìˆ˜ í™•ì¸
        if self.stacks_in_current_task >= self.max_stacks_per_task:
            print(f"âš ï¸  Max stacks per task ({self.max_stacks_per_task}) reached. Skipping task-based stacking.")
            return False
        
        # íƒœìŠ¤í¬ ê¸°ë°˜ ì ì¸µ ìˆ˜í–‰
        print(f"ğŸ¯ Task-based LoRA stacking triggered (Task {task_id}, Reason: {reason})")
        success = self._perform_lora_stacking("task_based", task_id, reason)
        
        if success:
            self.stacks_in_current_task += 1
            self.steps_since_last_stack = 0
            
            # ì ì¸µ íˆìŠ¤í† ë¦¬ ê¸°ë¡
            if self.stack_type_tracking:
                self.stack_history.append({
                    'type': 'task_based',
                    'task_id': task_id,
                    'reason': reason,
                    'step': self.steps_since_last_stack,
                    'timestamp': time.time()
                })
        
        return success
    
    def compute_loss_only(self, trans_obs_0, actions, e_obses):
        """
        ì˜¨ë¼ì¸ í•™ìŠµì„ ìˆ˜í–‰í•˜ì§€ ì•Šê³  í˜„ì¬ ëª¨ë¸ì˜ ì†ì‹¤ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
        """
        try:
            with torch.no_grad():
                i_z_obses_pred, _ = self.wm.rollout(obs_0=trans_obs_0, act=actions)
                trans_obs_gt = self.workspace.data_preprocessor.transform_obs(e_obses)
                trans_obs_gt = move_to_device(trans_obs_gt, self.device)
                i_z_obses_gt = self.wm.encode_obs(trans_obs_gt)

                frameskip = self.workspace.frameskip
                gt_proprio_resampled = i_z_obses_gt["proprio"][:, ::frameskip, :]
                gt_visual_resampled = i_z_obses_gt["visual"][:, ::frameskip, :, :]

                proprio_loss = self.loss_fn(i_z_obses_pred["proprio"], gt_proprio_resampled)
                visual_loss = self.loss_fn(i_z_obses_pred["visual"], gt_visual_resampled)
                total_loss = self.visual_loss_weight * visual_loss + self.proprio_loss_weight * proprio_loss

                metrics = {
                    "visual_loss": float(visual_loss.item()),
                    "proprio_loss": float(proprio_loss.item()),
                    "total_loss": float(total_loss.item()),
                }

                self.last_visual_loss = metrics["visual_loss"]
                self.last_proprio_loss = metrics["proprio_loss"]
                self.last_loss = metrics["total_loss"]

                return metrics
        except Exception as e:
            print(f"Error computing evaluation loss: {e}")
            return None
        finally:
            if 'i_z_obses_pred' in locals():
                del i_z_obses_pred
            if 'i_z_obses_gt' in locals():
                del i_z_obses_gt
    
    def _perform_lora_stacking(self, stack_type, task_id, reason):
        """
        ì‹¤ì œ LoRA ì ì¸µì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            stack_type: "task_based" ë˜ëŠ” "loss_based"
            task_id: íƒœìŠ¤í¬ ID
            reason: ì ì¸µ ì´ìœ 
        
        Returns:
            bool: ì ì¸µ ì„±ê³µ ì—¬ë¶€
        """
        try:
            print(f"Performing {stack_type} LoRA stacking...")
            
            # --- LoRA ì ì¸µ ìˆ˜í–‰ ---
            self.wm.predictor.update_and_reset_lora_parameters()
            
            # --- ì˜µí‹°ë§ˆì´ì € ì¬ì„¤ì • ---
            self._reset_optimizer_for_new_lora()
            
            # ì ì¸µ ì™„ë£Œ ë¡œê·¸
            print(f"{stack_type.title()} LoRA stacking completed successfully!")
            print(f"   - Task ID: {task_id}")
            print(f"   - Reason: {reason}")
            print(f"   - Stacks in current task: {self.stacks_in_current_task + 1}/{self.max_stacks_per_task}")
            
            # ğŸ”§ LoRA ì ì¸µ ì½œë°± í˜¸ì¶œ (ì•™ìƒë¸” ì €ì¥ì„ ìœ„í•´ í™•ì¥ëœ ì •ë³´ ì „ë‹¬)
            if self.on_lora_stack_callback:
                self.on_lora_stack_callback(
                    steps=self.steps_since_last_stack, 
                    loss=self.last_loss,
                    task_id=task_id,
                    stack_type=stack_type,
                    reason=reason
                )
            
            return True
            
        except Exception as e:
            print(f"âŒ Error during {stack_type} LoRA stacking: {e}")
            return False

    def _reset_optimizer_for_new_lora(self):
        """ì ì¸µ í›„ ìƒˆë¡œìš´ wnew íŒŒë¼ë¯¸í„°ë“¤ë§Œ í•™ìŠµí•˜ë„ë¡ ì˜µí‹°ë§ˆì´ì €ë¥¼ ì¬ì„¤ì •í•©ë‹ˆë‹¤."""
        # í˜„ì¬ í•™ìŠµ ê°€ëŠ¥í•œ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì ¸ì˜´
        params_to_train = [p for p in self.wm.parameters() if p.requires_grad]
        
        if not params_to_train:
            print("Warning: No trainable parameters found after LoRA stacking.")
            return
            
        # ìƒˆë¡œìš´ ì˜µí‹°ë§ˆì´ì € ìƒì„± (ê¸°ì¡´ ì˜µí‹°ë§ˆì´ì €ì˜ ìƒíƒœëŠ” ìœ ì§€í•˜ì§€ ì•ŠìŒ)
        old_lr = self.optimizer.param_groups[0]['lr']
        self.optimizer = torch.optim.Adam(params_to_train, lr=old_lr)
        
        print(f"Optimizer reset: {len(params_to_train)} trainable parameters, lr={old_lr}")
    
    def check_task_change(self, new_task_id):
        """
        íƒœìŠ¤í¬ ì „í™˜ì„ ê°ì§€í•˜ê³  task_changed í”Œë˜ê·¸ë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
        
        Args:
            new_task_id (int): ìƒˆë¡œìš´ íƒœìŠ¤í¬ ID
            
        Returns:
            bool: íƒœìŠ¤í¬ê°€ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
        """
        if not self.is_online_lora:
            return False
            
        if new_task_id != self.current_task_id:
            self.current_task_id = new_task_id
            self.stacks_in_current_task = 0
            self.task_changed = True
            return True
        else:
            self.task_changed = False
            return False
    
    def reset_task_changed_flag(self):
        """task_changed í”Œë˜ê·¸ë¥¼ ë¦¬ì…‹í•©ë‹ˆë‹¤."""
        self.task_changed = False